#!/usr/bin/env python3
"""
VoiceHelper ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨
æ•´åˆæ‰€æœ‰æµ‹è¯•ç±»åž‹ï¼Œæä¾›ä¸€ç«™å¼æµ‹è¯•è§£å†³æ–¹æ¡ˆ
"""

import asyncio
import os
import sys
import subprocess
import time
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../'))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestRunner:
    """ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.tests_dir = Path(__file__).parent
        self.results = {
            'test_session_id': f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'start_time': datetime.now().isoformat(),
            'tests': {},
            'summary': {}
        }
        
    def check_dependencies(self) -> bool:
        """æ£€æŸ¥æµ‹è¯•ä¾èµ–"""
        logger.info("ðŸ” æ£€æŸ¥æµ‹è¯•ä¾èµ–...")
        
        # æ£€æŸ¥PythonåŒ…
        required_packages = [
            'pytest', 'requests', 'psutil', 'aiohttp'
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            logger.error(f"ç¼ºå°‘å¿…è¦çš„PythonåŒ…: {', '.join(missing_packages)}")
            logger.info("è¯·è¿è¡Œ: pip install " + " ".join(missing_packages))
            return False
        
        # æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
        services_check = self._check_services()
        if not services_check['all_available']:
            logger.warning("âš ï¸ éƒ¨åˆ†æœåŠ¡ä¸å¯ç”¨ï¼ŒæŸäº›æµ‹è¯•å¯èƒ½ä¼šå¤±è´¥")
            for service, status in services_check['services'].items():
                if not status['available']:
                    logger.warning(f"  {service}: {status.get('error', 'Unknown error')}")
        
        return True
    
    def _check_services(self) -> Dict[str, Any]:
        """æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
        import requests
        
        services = {
            'backend': 'http://localhost:8080/health',
            'algorithm': 'http://localhost:8000/health',
            'frontend': 'http://localhost:3000'
        }
        
        results = {'services': {}, 'all_available': True}
        
        for service_name, url in services.items():
            try:
                response = requests.get(url, timeout=5)
                results['services'][service_name] = {
                    'available': response.status_code == 200,
                    'status_code': response.status_code
                }
            except Exception as e:
                results['services'][service_name] = {
                    'available': False,
                    'error': str(e)
                }
                results['all_available'] = False
        
        return results
    
    async def run_unit_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        logger.info("ðŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
        
        start_time = time.time()
        
        # æŸ¥æ‰¾å•å…ƒæµ‹è¯•æ–‡ä»¶
        unit_test_files = list(self.tests_dir.glob("**/test_*.py"))
        unit_test_files.extend(list(self.tests_dir.glob("unit/**/*.py")))
        
        if not unit_test_files:
            logger.warning("æœªæ‰¾åˆ°å•å…ƒæµ‹è¯•æ–‡ä»¶")
            return {
                'status': 'skipped',
                'reason': 'No unit test files found',
                'duration': 0
            }
        
        try:
            # ä½¿ç”¨pytestè¿è¡Œå•å…ƒæµ‹è¯•
            cmd = [
                sys.executable, '-m', 'pytest',
                str(self.tests_dir),
                '-v',
                '--tb=short',
                '--json-report',
                f'--json-report-file={self.tests_dir}/unit_test_results.json'
            ]
            
            result = subprocess.run(
                cmd,
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            # å°è¯•è¯»å–pytestçš„JSONæŠ¥å‘Š
            json_report_file = self.tests_dir / 'unit_test_results.json'
            test_details = {}
            if json_report_file.exists():
                try:
                    with open(json_report_file, 'r') as f:
                        pytest_report = json.load(f)
                        test_details = {
                            'total': pytest_report.get('summary', {}).get('total', 0),
                            'passed': pytest_report.get('summary', {}).get('passed', 0),
                            'failed': pytest_report.get('summary', {}).get('failed', 0),
                            'skipped': pytest_report.get('summary', {}).get('skipped', 0)
                        }
                except:
                    pass
            
            return {
                'status': 'success' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'test_details': test_details
            }
            
        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'duration': time.time() - start_time,
                'error': 'Unit tests timed out after 5 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
    
    async def run_integration_tests(self) -> Dict[str, Any]:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        logger.info("ðŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
        
        start_time = time.time()
        
        # è¿è¡Œæ¨¡å—æµ‹è¯•
        module_test_file = self.tests_dir / 'module_test_runner.py'
        if not module_test_file.exists():
            logger.warning("æœªæ‰¾åˆ°æ¨¡å—æµ‹è¯•æ–‡ä»¶")
            return {
                'status': 'skipped',
                'reason': 'Module test runner not found',
                'duration': 0
            }
        
        try:
            result = subprocess.run(
                [sys.executable, str(module_test_file)],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            return {
                'status': 'success' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'duration': time.time() - start_time,
                'error': 'Integration tests timed out after 5 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
    
    async def run_e2e_tests(self) -> Dict[str, Any]:
        """è¿è¡ŒE2Eæµ‹è¯•"""
        logger.info("ðŸŽ­ è¿è¡ŒE2Eæµ‹è¯•...")
        
        start_time = time.time()
        
        # æ£€æŸ¥E2Eæµ‹è¯•ç›®å½•
        e2e_dir = self.tests_dir / 'e2e'
        if not e2e_dir.exists():
            logger.warning("æœªæ‰¾åˆ°E2Eæµ‹è¯•ç›®å½•")
            return {
                'status': 'skipped',
                'reason': 'E2E test directory not found',
                'duration': 0
            }
        
        try:
            # è¿è¡ŒPlaywrightæµ‹è¯•
            result = subprocess.run(
                ['npx', 'playwright', 'test', '--reporter=json'],
                cwd=str(e2e_dir),
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            # å°è¯•è§£æžPlaywrightçš„JSONè¾“å‡º
            test_details = {}
            try:
                if result.stdout:
                    # Playwrightçš„JSONè¾“å‡ºå¯èƒ½åŒ…å«å¤šè¡Œï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®žé™…çš„JSON
                    lines = result.stdout.strip().split('\n')
                    for line in lines:
                        if line.startswith('{') and line.endswith('}'):
                            playwright_report = json.loads(line)
                            if 'stats' in playwright_report:
                                stats = playwright_report['stats']
                                test_details = {
                                    'total': stats.get('total', 0),
                                    'passed': stats.get('passed', 0),
                                    'failed': stats.get('failed', 0),
                                    'skipped': stats.get('skipped', 0)
                                }
                                break
            except:
                pass
            
            return {
                'status': 'success' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'test_details': test_details
            }
            
        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'duration': time.time() - start_time,
                'error': 'E2E tests timed out after 10 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
    
    async def run_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
        
        start_time = time.time()
        
        # è¿è¡Œç»Ÿä¸€æ€§èƒ½æµ‹è¯•
        performance_test_file = self.tests_dir / 'unified_performance_test.py'
        if not performance_test_file.exists():
            logger.warning("æœªæ‰¾åˆ°ç»Ÿä¸€æ€§èƒ½æµ‹è¯•æ–‡ä»¶")
            return {
                'status': 'skipped',
                'reason': 'Unified performance test not found',
                'duration': 0
            }
        
        try:
            result = subprocess.run(
                [sys.executable, str(performance_test_file), '--test-type', 'comprehensive'],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            return {
                'status': 'success' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'duration': time.time() - start_time,
                'error': 'Performance tests timed out after 10 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
    
    async def run_benchmark_tests(self) -> Dict[str, Any]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        logger.info("ðŸ“Š è¿è¡ŒåŸºå‡†æµ‹è¯•...")
        
        start_time = time.time()
        
        # è¿è¡Œç»Ÿä¸€åŸºå‡†æµ‹è¯•
        benchmark_test_file = self.tests_dir / 'unified_benchmark_test.py'
        if not benchmark_test_file.exists():
            logger.warning("æœªæ‰¾åˆ°ç»Ÿä¸€åŸºå‡†æµ‹è¯•æ–‡ä»¶")
            return {
                'status': 'skipped',
                'reason': 'Unified benchmark test not found',
                'duration': 0
            }
        
        try:
            result = subprocess.run(
                [sys.executable, str(benchmark_test_file)],
                cwd=str(self.project_root),
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            return {
                'status': 'success' if result.returncode == 0 else 'failed',
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                'status': 'timeout',
                'duration': time.time() - start_time,
                'error': 'Benchmark tests timed out after 10 minutes'
            }
        except Exception as e:
            return {
                'status': 'error',
                'duration': time.time() - start_time,
                'error': str(e)
            }
    
    async def run_all_tests(self, test_types: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        if test_types is None:
            test_types = ['unit', 'integration', 'e2e', 'performance', 'benchmark']
        
        logger.info(f"ðŸš€ å¼€å§‹è¿è¡Œæµ‹è¯•å¥—ä»¶: {', '.join(test_types)}")
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return {
                'status': 'failed',
                'error': 'Dependency check failed'
            }
        
        # è¿è¡Œå„ç±»æµ‹è¯•
        test_functions = {
            'unit': self.run_unit_tests,
            'integration': self.run_integration_tests,
            'e2e': self.run_e2e_tests,
            'performance': self.run_performance_tests,
            'benchmark': self.run_benchmark_tests
        }
        
        for test_type in test_types:
            if test_type in test_functions:
                logger.info(f"\n{'='*60}")
                logger.info(f"å¼€å§‹ {test_type.upper()} æµ‹è¯•")
                logger.info(f"{'='*60}")
                
                self.results['tests'][test_type] = await test_functions[test_type]()
                
                # æ˜¾ç¤ºæµ‹è¯•ç»“æžœæ‘˜è¦
                result = self.results['tests'][test_type]
                status_emoji = "âœ…" if result['status'] == 'success' else "âŒ" if result['status'] == 'failed' else "â­ï¸"
                logger.info(f"{status_emoji} {test_type.upper()} æµ‹è¯•å®Œæˆ - çŠ¶æ€: {result['status']} - è€—æ—¶: {result['duration']:.2f}s")
            else:
                logger.warning(f"æœªçŸ¥çš„æµ‹è¯•ç±»åž‹: {test_type}")
        
        # ç”Ÿæˆæµ‹è¯•æ‘˜è¦
        self._generate_summary()
        
        return self.results
    
    def _generate_summary(self):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        total_tests = len(self.results['tests'])
        successful_tests = sum(1 for test in self.results['tests'].values() if test['status'] == 'success')
        failed_tests = sum(1 for test in self.results['tests'].values() if test['status'] == 'failed')
        skipped_tests = sum(1 for test in self.results['tests'].values() if test['status'] == 'skipped')
        
        total_duration = sum(test['duration'] for test in self.results['tests'].values())
        
        self.results['end_time'] = datetime.now().isoformat()
        self.results['summary'] = {
            'total_test_types': total_tests,
            'successful': successful_tests,
            'failed': failed_tests,
            'skipped': skipped_tests,
            'total_duration': total_duration,
            'success_rate': (successful_tests / total_tests * 100) if total_tests > 0 else 0
        }
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*80)
        print("ðŸŽ¯ VoiceHelper æµ‹è¯•å¥—ä»¶æ‰§è¡Œæ‘˜è¦")
        print("="*80)
        
        summary = self.results['summary']
        print(f"æµ‹è¯•ä¼šè¯ID: {self.results['test_session_id']}")
        print(f"å¼€å§‹æ—¶é—´: {self.results['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {self.results['end_time']}")
        print(f"æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
        
        print(f"\nðŸ“Š æµ‹è¯•ç»“æžœç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•ç±»åž‹: {summary['total_test_types']}")
        print(f"  æˆåŠŸ: {summary['successful']} âœ…")
        print(f"  å¤±è´¥: {summary['failed']} âŒ")
        print(f"  è·³è¿‡: {summary['skipped']} â­ï¸")
        print(f"  æˆåŠŸçŽ‡: {summary['success_rate']:.1f}%")
        
        print(f"\nðŸ“‹ è¯¦ç»†ç»“æžœ:")
        for test_type, result in self.results['tests'].items():
            status_emoji = {
                'success': 'âœ…',
                'failed': 'âŒ',
                'skipped': 'â­ï¸',
                'timeout': 'â°',
                'error': 'ðŸ’¥'
            }.get(result['status'], 'â“')
            
            print(f"  {status_emoji} {test_type.upper()}: {result['status']} ({result['duration']:.2f}s)")
            
            # æ˜¾ç¤ºæµ‹è¯•è¯¦æƒ…
            if 'test_details' in result and result['test_details']:
                details = result['test_details']
                if 'total' in details:
                    print(f"    æ€»è®¡: {details['total']}, é€šè¿‡: {details.get('passed', 0)}, å¤±è´¥: {details.get('failed', 0)}")
            
            # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            if result['status'] in ['failed', 'error', 'timeout'] and 'error' in result:
                print(f"    é”™è¯¯: {result['error']}")
        
        # æ€»ä½“è¯„ä»·
        if summary['success_rate'] == 100:
            print(f"\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼ç³»ç»ŸçŠ¶æ€è‰¯å¥½ã€‚")
        elif summary['success_rate'] >= 80:
            print(f"\nâœ… å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬æ­£å¸¸ã€‚")
        elif summary['success_rate'] >= 60:
            print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        else:
            print(f"\nâŒ å¤šé¡¹æµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿå¯èƒ½å­˜åœ¨ä¸¥é‡é—®é¢˜ã€‚")
    
    def save_results(self, filename: str = None) -> str:
        """ä¿å­˜æµ‹è¯•ç»“æžœ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tests/test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æµ‹è¯•ç»“æžœå·²ä¿å­˜åˆ°: {filename}")
        return filename

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="VoiceHelper ç»Ÿä¸€æµ‹è¯•è¿è¡Œå™¨")
    parser.add_argument("--tests", nargs='+', 
                       choices=['unit', 'integration', 'e2e', 'performance', 'benchmark'],
                       default=['unit', 'integration', 'performance'],
                       help="è¦è¿è¡Œçš„æµ‹è¯•ç±»åž‹")
    parser.add_argument("--output", type=str, help="ç»“æžœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = TestRunner()
    
    try:
        # è¿è¡Œæµ‹è¯•
        results = await runner.run_all_tests(args.tests)
        
        # æ˜¾ç¤ºæ‘˜è¦
        runner.print_summary()
        
        # ä¿å­˜ç»“æžœ
        output_file = runner.save_results(args.output)
        
        # è¿”å›žçŠ¶æ€ç 
        success_rate = results['summary']['success_rate']
        return 0 if success_rate >= 80 else 1
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

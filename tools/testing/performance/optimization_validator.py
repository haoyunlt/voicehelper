#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–éªŒè¯å™¨
éªŒè¯ä¼˜åŒ–æªæ–½çš„æ•ˆæœ
"""

import time
import json
import requests
import psutil
import os
import sys
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

try:
    from algo.core.memory_optimizer import memory_optimizer, get_memory_report
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥å†…å­˜ä¼˜åŒ–å™¨ï¼Œè·³è¿‡å†…å­˜ä¼˜åŒ–æµ‹è¯•")
    memory_optimizer = None

class OptimizationValidator:
    """æ€§èƒ½ä¼˜åŒ–éªŒè¯å™¨"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8080"
        self.algo_url = "http://localhost:8000"
        self.results = {}
        
    def run_validation(self):
        """è¿è¡Œä¼˜åŒ–éªŒè¯"""
        print("ğŸ” VoiceHelper æ€§èƒ½ä¼˜åŒ–éªŒè¯")
        print("=" * 50)
        
        # 1. åŸºå‡†æµ‹è¯• (ä¼˜åŒ–å‰)
        print("\nğŸ“Š æ‰§è¡ŒåŸºå‡†æµ‹è¯•...")
        baseline = self.run_performance_test("baseline")
        
        # 2. åº”ç”¨å†…å­˜ä¼˜åŒ–
        if memory_optimizer:
            print("\nğŸ§  åº”ç”¨å†…å­˜ä¼˜åŒ–...")
            self.apply_memory_optimizations()
        
        # 3. ä¼˜åŒ–åæµ‹è¯•
        print("\nğŸ“ˆ æ‰§è¡Œä¼˜åŒ–åæµ‹è¯•...")
        optimized = self.run_performance_test("optimized")
        
        # 4. å¯¹æ¯”åˆ†æ
        print("\nğŸ“‹ ç”Ÿæˆå¯¹æ¯”åˆ†æ...")
        comparison = self.compare_results(baseline, optimized)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_validation_report(baseline, optimized, comparison)
        
        print("\nâœ… æ€§èƒ½ä¼˜åŒ–éªŒè¯å®Œæˆï¼")
        
    def run_performance_test(self, test_name: str) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print(f"  æ‰§è¡Œ {test_name} æµ‹è¯•...")
        
        results = {
            'test_name': test_name,
            'timestamp': time.time(),
            'system_metrics': self.get_system_metrics(),
            'api_performance': self.test_api_performance(),
            'memory_usage': self.test_memory_usage(),
            'concurrent_performance': self.test_concurrent_performance()
        }
        
        return results
    
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # è¿›ç¨‹å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        process_memory = process.memory_info()
        
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_available_gb': memory.available / (1024**3),
            'process_memory_mb': process_memory.rss / (1024**2),
            'process_memory_percent': process.memory_percent()
        }
    
    def test_api_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•APIæ€§èƒ½"""
        endpoints = [
            (f"{self.backend_url}/health", "backend_health"),
            (f"{self.algo_url}/health", "algo_health")
        ]
        
        results = {}
        
        for url, name in endpoints:
            try:
                # é¢„çƒ­
                requests.get(url, timeout=5)
                
                # æµ‹è¯•å¤šæ¬¡å–å¹³å‡å€¼
                response_times = []
                for _ in range(10):
                    start_time = time.time()
                    response = requests.get(url, timeout=5)
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        response_times.append((end_time - start_time) * 1000)
                
                if response_times:
                    results[name] = {
                        'avg_response_time_ms': sum(response_times) / len(response_times),
                        'min_response_time_ms': min(response_times),
                        'max_response_time_ms': max(response_times),
                        'success_rate': len(response_times) / 10
                    }
                else:
                    results[name] = {'error': 'All requests failed'}
                    
            except Exception as e:
                results[name] = {'error': str(e)}
        
        return results
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        # è·å–åˆå§‹å†…å­˜
        initial_memory = psutil.Process().memory_info().rss
        
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†æ“ä½œ
        test_data = []
        for i in range(5000):
            test_data.append({
                'id': i,
                'content': f'Test content {i}' * 10,
                'metadata': {'timestamp': time.time(), 'index': i}
            })
        
        # è·å–å³°å€¼å†…å­˜
        peak_memory = psutil.Process().memory_info().rss
        
        # æ¸…ç†æ•°æ®
        del test_data
        
        # è·å–æ¸…ç†åå†…å­˜
        final_memory = psutil.Process().memory_info().rss
        
        return {
            'initial_memory_mb': initial_memory / (1024**2),
            'peak_memory_mb': peak_memory / (1024**2),
            'final_memory_mb': final_memory / (1024**2),
            'memory_increase_mb': (peak_memory - initial_memory) / (1024**2),
            'memory_retained_mb': (final_memory - initial_memory) / (1024**2)
        }
    
    def test_concurrent_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        import concurrent.futures
        
        def make_request():
            try:
                start_time = time.time()
                response = requests.get(f"{self.backend_url}/health", timeout=5)
                end_time = time.time()
                return {
                    'success': response.status_code == 200,
                    'response_time_ms': (end_time - start_time) * 1000
                }
            except:
                return {'success': False, 'response_time_ms': None}
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrent_levels = [5, 10, 20]
        results = {}
        
        for level in concurrent_levels:
            start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=level) as executor:
                futures = [executor.submit(make_request) for _ in range(level)]
                responses = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            end_time = time.time()
            
            successful = [r for r in responses if r['success']]
            response_times = [r['response_time_ms'] for r in successful if r['response_time_ms']]
            
            results[f'concurrent_{level}'] = {
                'total_time_ms': (end_time - start_time) * 1000,
                'success_count': len(successful),
                'success_rate': len(successful) / level,
                'avg_response_time_ms': sum(response_times) / len(response_times) if response_times else 0,
                'throughput_rps': level / ((end_time - start_time) if (end_time - start_time) > 0 else 1)
            }
        
        return results
    
    def apply_memory_optimizations(self):
        """åº”ç”¨å†…å­˜ä¼˜åŒ–"""
        if not memory_optimizer:
            print("  å†…å­˜ä¼˜åŒ–å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡ä¼˜åŒ–")
            return
        
        try:
            # æ‰§è¡Œå†…å­˜ä¼˜åŒ–
            optimization_result = memory_optimizer.optimize_memory()
            
            print("  å†…å­˜ä¼˜åŒ–ç»“æœ:")
            print(f"    æ¸…ç†æ­»äº¡å¼•ç”¨: {optimization_result.get('dead_refs_cleaned', 0)}")
            print(f"    åƒåœ¾å›æ”¶é‡Šæ”¾å¯¹è±¡: {optimization_result.get('gc_stats', {}).get('objects_freed', 0)}")
            
            # åˆ›å»ºä¼˜åŒ–ç¼“å­˜
            cache = memory_optimizer.create_cache('api_cache', 500)
            print(f"    åˆ›å»ºAPIç¼“å­˜: æœ€å¤§{500}é¡¹")
            
            # åˆ›å»ºå¯¹è±¡æ± 
            def create_request_obj():
                return {'headers': {}, 'data': None, 'response': None}
            
            def reset_request_obj(obj):
                obj['headers'].clear()
                obj['data'] = None
                obj['response'] = None
            
            pool = memory_optimizer.create_object_pool('request_pool', create_request_obj, reset_request_obj, 50)
            print(f"    åˆ›å»ºè¯·æ±‚å¯¹è±¡æ± : æœ€å¤§{50}ä¸ªå¯¹è±¡")
            
        except Exception as e:
            print(f"  å†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    def compare_results(self, baseline: Dict[str, Any], optimized: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æµ‹è¯•ç»“æœ"""
        comparison = {
            'timestamp': time.time(),
            'improvements': {},
            'regressions': {},
            'summary': {}
        }
        
        # å¯¹æ¯”ç³»ç»ŸæŒ‡æ ‡
        baseline_sys = baseline['system_metrics']
        optimized_sys = optimized['system_metrics']
        
        sys_comparison = {}
        for metric in ['cpu_percent', 'memory_percent', 'process_memory_mb']:
            baseline_val = baseline_sys.get(metric, 0)
            optimized_val = optimized_sys.get(metric, 0)
            
            if baseline_val > 0:
                improvement = ((baseline_val - optimized_val) / baseline_val) * 100
                sys_comparison[metric] = {
                    'baseline': baseline_val,
                    'optimized': optimized_val,
                    'improvement_percent': improvement
                }
        
        comparison['system_metrics'] = sys_comparison
        
        # å¯¹æ¯”APIæ€§èƒ½
        api_comparison = {}
        baseline_api = baseline['api_performance']
        optimized_api = optimized['api_performance']
        
        for endpoint in baseline_api:
            if endpoint in optimized_api and 'avg_response_time_ms' in baseline_api[endpoint] and 'avg_response_time_ms' in optimized_api[endpoint]:
                baseline_time = baseline_api[endpoint]['avg_response_time_ms']
                optimized_time = optimized_api[endpoint]['avg_response_time_ms']
                
                improvement = ((baseline_time - optimized_time) / baseline_time) * 100
                api_comparison[endpoint] = {
                    'baseline_ms': baseline_time,
                    'optimized_ms': optimized_time,
                    'improvement_percent': improvement
                }
        
        comparison['api_performance'] = api_comparison
        
        # å¯¹æ¯”å†…å­˜ä½¿ç”¨
        baseline_mem = baseline['memory_usage']
        optimized_mem = optimized['memory_usage']
        
        mem_comparison = {}
        for metric in ['memory_increase_mb', 'memory_retained_mb']:
            baseline_val = baseline_mem.get(metric, 0)
            optimized_val = optimized_mem.get(metric, 0)
            
            if baseline_val > 0:
                improvement = ((baseline_val - optimized_val) / baseline_val) * 100
                mem_comparison[metric] = {
                    'baseline': baseline_val,
                    'optimized': optimized_val,
                    'improvement_percent': improvement
                }
        
        comparison['memory_usage'] = mem_comparison
        
        # ç”Ÿæˆæ€»ç»“
        improvements = []
        regressions = []
        
        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡çš„æ”¹å–„æƒ…å†µ
        for category, metrics in [
            ('system_metrics', sys_comparison),
            ('api_performance', api_comparison),
            ('memory_usage', mem_comparison)
        ]:
            for metric, data in metrics.items():
                improvement = data.get('improvement_percent', 0)
                if improvement > 5:  # æ”¹å–„è¶…è¿‡5%
                    improvements.append(f"{category}.{metric}: {improvement:.1f}%")
                elif improvement < -5:  # æ¶åŒ–è¶…è¿‡5%
                    regressions.append(f"{category}.{metric}: {improvement:.1f}%")
        
        comparison['improvements'] = improvements
        comparison['regressions'] = regressions
        
        # è®¡ç®—æ€»ä½“æ”¹å–„è¯„åˆ†
        all_improvements = []
        for category in [sys_comparison, api_comparison, mem_comparison]:
            for data in category.values():
                all_improvements.append(data.get('improvement_percent', 0))
        
        if all_improvements:
            avg_improvement = sum(all_improvements) / len(all_improvements)
            comparison['summary'] = {
                'overall_improvement_percent': avg_improvement,
                'total_metrics_improved': len(improvements),
                'total_metrics_regressed': len(regressions),
                'optimization_success': avg_improvement > 0 and len(improvements) > len(regressions)
            }
        
        return comparison
    
    def generate_validation_report(self, baseline: Dict[str, Any], optimized: Dict[str, Any], comparison: Dict[str, Any]):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'baseline_results': baseline,
            'optimized_results': optimized,
            'comparison': comparison,
            'recommendations': self.get_optimization_recommendations(comparison)
        }
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = 'tests/performance/optimization_validation_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
        self.print_summary_report(comparison)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def print_summary_report(self, comparison: Dict[str, Any]):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–éªŒè¯æ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        summary = comparison.get('summary', {})
        
        # æ€»ä½“ç»“æœ
        overall_improvement = summary.get('overall_improvement_percent', 0)
        success = summary.get('optimization_success', False)
        
        status_icon = "âœ…" if success else "âš ï¸"
        print(f"\n{status_icon} æ€»ä½“ä¼˜åŒ–æ•ˆæœ: {overall_improvement:+.1f}%")
        
        # æ”¹å–„é¡¹ç›®
        improvements = comparison.get('improvements', [])
        if improvements:
            print(f"\nğŸ“ˆ æ€§èƒ½æ”¹å–„é¡¹ç›® ({len(improvements)}é¡¹):")
            for improvement in improvements[:5]:  # æ˜¾ç¤ºå‰5é¡¹
                print(f"  â€¢ {improvement}")
        
        # å›å½’é¡¹ç›®
        regressions = comparison.get('regressions', [])
        if regressions:
            print(f"\nğŸ“‰ æ€§èƒ½å›å½’é¡¹ç›® ({len(regressions)}é¡¹):")
            for regression in regressions[:3]:  # æ˜¾ç¤ºå‰3é¡¹
                print(f"  â€¢ {regression}")
        
        # å…³é”®æŒ‡æ ‡å¯¹æ¯”
        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        mem_metrics = comparison.get('memory_usage', {})
        for metric, data in mem_metrics.items():
            baseline = data['baseline']
            optimized = data['optimized']
            improvement = data['improvement_percent']
            print(f"  å†…å­˜{metric}: {baseline:.2f} â†’ {optimized:.2f} ({improvement:+.1f}%)")
        
        # APIæ€§èƒ½å¯¹æ¯”
        api_metrics = comparison.get('api_performance', {})
        for endpoint, data in api_metrics.items():
            baseline = data['baseline_ms']
            optimized = data['optimized_ms']
            improvement = data['improvement_percent']
            print(f"  {endpoint}: {baseline:.2f}ms â†’ {optimized:.2f}ms ({improvement:+.1f}%)")
    
    def get_optimization_recommendations(self, comparison: Dict[str, Any]) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        summary = comparison.get('summary', {})
        overall_improvement = summary.get('overall_improvement_percent', 0)
        
        if overall_improvement > 10:
            recommendations.append("ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®å°†å½“å‰ä¼˜åŒ–æªæ–½åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ")
        elif overall_improvement > 5:
            recommendations.append("ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥æµ‹è¯•ååº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ")
        elif overall_improvement > 0:
            recommendations.append("ä¼˜åŒ–æ•ˆæœè½»å¾®ï¼Œå»ºè®®ç»§ç»­å¯»æ‰¾å…¶ä»–ä¼˜åŒ–ç‚¹")
        else:
            recommendations.append("ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®é‡æ–°è¯„ä¼°ä¼˜åŒ–ç­–ç•¥")
        
        # åŸºäºå…·ä½“æŒ‡æ ‡ç»™å‡ºå»ºè®®
        regressions = comparison.get('regressions', [])
        if regressions:
            recommendations.append("æ³¨æ„éƒ¨åˆ†æŒ‡æ ‡å‡ºç°å›å½’ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")
        
        improvements = comparison.get('improvements', [])
        if len(improvements) > 3:
            recommendations.append("å¤šé¡¹æŒ‡æ ‡å¾—åˆ°æ”¹å–„ï¼Œä¼˜åŒ–æ–¹å‘æ­£ç¡®")
        
        return recommendations

if __name__ == "__main__":
    validator = OptimizationValidator()
    validator.run_validation()

"""
åŠ¨æ€æ¨¡å‹è·¯ç”±æ€§èƒ½æµ‹è¯•

æµ‹è¯•æ™ºèƒ½è·¯ç”±ç³»ç»Ÿçš„æˆæœ¬é™ä½æ•ˆæœ
"""

import asyncio
import time
import statistics
import sys
import os
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from algo.services.routing_service import IntelligentRoutingService, RoutingConfig


class RoutingPerformanceTester:
    """è·¯ç”±æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
    
    async def create_mock_processors(self) -> dict:
        """åˆ›å»ºæ¨¡æ‹Ÿå¤„ç†å™¨"""
        
        # æ¨¡æ‹Ÿä¸åŒæ¨¡å‹çš„æˆæœ¬å’Œæ€§èƒ½ç‰¹å¾
        model_configs = {
            'gpt-4': {
                'cost_per_1k_tokens': 0.03,
                'base_latency': 2.0,
                'quality_score': 0.95
            },
            'gpt-3.5-turbo': {
                'cost_per_1k_tokens': 0.002,
                'base_latency': 1.0,
                'quality_score': 0.85
            },
            'claude-3-haiku': {
                'cost_per_1k_tokens': 0.00025,
                'base_latency': 0.8,
                'quality_score': 0.75
            },
            'gemini-pro': {
                'cost_per_1k_tokens': 0.001,
                'base_latency': 1.2,
                'quality_score': 0.80
            }
        }
        
        async def create_processor(model_name: str, config: dict):
            async def processor(content: str, params: dict) -> str:
                # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                latency = config['base_latency'] * random.uniform(0.8, 1.2)
                await asyncio.sleep(latency)
                
                # æ¨¡æ‹Ÿå“åº”è´¨é‡
                quality_indicator = "â˜…" * int(config['quality_score'] * 5)
                
                return f"[{model_name}] {quality_indicator} Response to: {content[:50]}..."
            
            return processor
        
        processors = {}
        for model_name, config in model_configs.items():
            processors[model_name] = await create_processor(model_name, config)
        
        return processors
    
    def generate_test_scenarios(self) -> list:
        """ç”Ÿæˆæµ‹è¯•åœºæ™¯"""
        scenarios = []
        
        # åœºæ™¯1: æ··åˆå¤æ‚åº¦ä»»åŠ¡ (çœŸå®åœºæ™¯)
        mixed_tasks = [
            # ç®€å•ä»»åŠ¡ (40%)
            "Hello", "Thanks", "Yes", "What is AI?", "Define ML",
            "Hi there", "Good morning", "How are you?", "What's new?", "Tell me a joke",
            
            # ä¸­ç­‰ä»»åŠ¡ (40%)
            "How does machine learning work?", "Explain neural networks",
            "Compare Python and Java", "What are the benefits of cloud computing?",
            "How to optimize database performance?", "Explain REST API design",
            "What is microservices architecture?", "How does blockchain work?",
            
            # å¤æ‚ä»»åŠ¡ (20%)
            "Design a scalable distributed system for real-time analytics",
            "Create a comprehensive AI strategy for enterprise transformation",
            "Analyze the economic impact of automation on employment",
            "Develop an algorithm for optimizing supply chain logistics",
            "Write a detailed technical specification for a recommendation engine"
        ]
        
        scenarios.append({
            'name': 'mixed_complexity',
            'description': 'æ··åˆå¤æ‚åº¦åœºæ™¯ (æ¨¡æ‹ŸçœŸå®ä½¿ç”¨)',
            'tasks': mixed_tasks,
            'expected_cost_reduction': 0.35  # æœŸæœ›35%æˆæœ¬é™ä½
        })
        
        # åœºæ™¯2: é«˜æ¯”ä¾‹ç®€å•ä»»åŠ¡ (å®¢æœåœºæ™¯)
        simple_heavy_tasks = [
            "Hello", "Hi", "Thanks", "Bye", "Yes", "No", "OK", "Help",
            "What is your name?", "How can I contact support?", "What are your hours?",
            "Where is my order?", "How to reset password?", "What is the price?",
            "Is this available?", "How to cancel?", "What is the refund policy?",
            "How to login?", "Forgot password", "Account locked", "Payment failed"
        ] * 3  # é‡å¤ä»¥å¢åŠ ç®€å•ä»»åŠ¡æ¯”ä¾‹
        
        scenarios.append({
            'name': 'simple_heavy',
            'description': 'ç®€å•ä»»åŠ¡ä¸ºä¸»åœºæ™¯ (å®¢æœ/FAQ)',
            'tasks': simple_heavy_tasks,
            'expected_cost_reduction': 0.50  # æœŸæœ›50%æˆæœ¬é™ä½
        })
        
        # åœºæ™¯3: é«˜æ¯”ä¾‹å¤æ‚ä»»åŠ¡ (ç ”å‘åœºæ™¯)
        complex_heavy_tasks = [
            "Design a distributed microservices architecture",
            "Implement a machine learning pipeline for real-time predictions",
            "Create a comprehensive security framework for cloud applications",
            "Develop an optimization algorithm for resource allocation",
            "Build a data processing system for petabyte-scale analytics",
            "Design a fault-tolerant messaging system",
            "Implement a recommendation engine with collaborative filtering",
            "Create a real-time fraud detection system",
            "Design a scalable search engine architecture",
            "Develop a distributed caching strategy"
        ] * 2
        
        scenarios.append({
            'name': 'complex_heavy',
            'description': 'å¤æ‚ä»»åŠ¡ä¸ºä¸»åœºæ™¯ (ç ”å‘/å’¨è¯¢)',
            'tasks': complex_heavy_tasks,
            'expected_cost_reduction': 0.20  # æœŸæœ›20%æˆæœ¬é™ä½
        })
        
        return scenarios
    
    async def run_baseline_test(self, tasks: list, processors: dict) -> dict:
        """è¿è¡ŒåŸºçº¿æµ‹è¯• (æ€»æ˜¯ä½¿ç”¨æœ€è´µçš„æ¨¡å‹)"""
        print("ğŸ”„ Running baseline test (always GPT-4)...")
        
        baseline_model = 'gpt-4'
        processor = processors[baseline_model]
        
        total_cost = 0.0
        response_times = []
        start_time = time.time()
        
        for task in tasks:
            task_start = time.time()
            
            # æ¨¡æ‹Ÿæˆæœ¬è®¡ç®—
            estimated_tokens = len(task) * 2.5  # ç®€åŒ–ä¼°ç®—
            task_cost = (estimated_tokens / 1000) * 0.03  # GPT-4ä»·æ ¼
            total_cost += task_cost
            
            # æ‰§è¡Œå¤„ç†
            await processor(task, {'model': baseline_model})
            
            response_time = time.time() - task_start
            response_times.append(response_time)
        
        total_time = time.time() - start_time
        
        return {
            'total_cost': total_cost,
            'total_time': total_time,
            'avg_response_time': statistics.mean(response_times),
            'throughput': len(tasks) / total_time,
            'model_usage': {baseline_model: len(tasks)}
        }
    
    async def run_routing_test(self, tasks: list, processors: dict, config: RoutingConfig) -> dict:
        """è¿è¡Œæ™ºèƒ½è·¯ç”±æµ‹è¯•"""
        print("ğŸš€ Running intelligent routing test...")
        
        # åˆ›å»ºè·¯ç”±æœåŠ¡
        routing_service = IntelligentRoutingService(
            model_processors=processors,
            config=config
        )
        
        await routing_service.start()
        
        try:
            total_cost = 0.0
            response_times = []
            model_usage = {}
            successful_requests = 0
            start_time = time.time()
            
            for i, task in enumerate(tasks):
                task_start = time.time()
                
                # æ¨¡æ‹Ÿä¸åŒç”¨æˆ·
                user_id = f"user_{i % 10}"
                
                # æ‰§è¡Œè·¯ç”±å¤„ç†
                result = await routing_service.route_and_process(
                    content=task,
                    user_id=user_id
                )
                
                response_time = time.time() - task_start
                response_times.append(response_time)
                
                if result['success']:
                    successful_requests += 1
                    routing_info = result['routing_info']
                    
                    # ç´¯è®¡æˆæœ¬
                    total_cost += routing_info['estimated_cost']
                    
                    # ç»Ÿè®¡æ¨¡å‹ä½¿ç”¨
                    selected_model = routing_info['selected_model']
                    model_usage[selected_model] = model_usage.get(selected_model, 0) + 1
                
                # æ¯20ä¸ªä»»åŠ¡è¾“å‡ºè¿›åº¦
                if (i + 1) % 20 == 0:
                    print(f"  Progress: {i+1}/{len(tasks)}")
            
            total_time = time.time() - start_time
            
            # è·å–è¯¦ç»†ç»Ÿè®¡
            service_stats = routing_service.get_comprehensive_stats()
            
            return {
                'total_cost': total_cost,
                'total_time': total_time,
                'avg_response_time': statistics.mean(response_times),
                'throughput': len(tasks) / total_time,
                'model_usage': model_usage,
                'successful_requests': successful_requests,
                'success_rate': successful_requests / len(tasks),
                'service_stats': service_stats
            }
            
        finally:
            await routing_service.stop()
    
    def analyze_cost_reduction(self, baseline: dict, routing: dict) -> dict:
        """åˆ†ææˆæœ¬é™ä½æ•ˆæœ"""
        cost_reduction = (baseline['total_cost'] - routing['total_cost']) / baseline['total_cost']
        
        # æ€§èƒ½å¯¹æ¯”
        performance_impact = (routing['avg_response_time'] - baseline['avg_response_time']) / baseline['avg_response_time']
        
        # ååé‡å¯¹æ¯”
        throughput_change = (routing['throughput'] - baseline['throughput']) / baseline['throughput']
        
        return {
            'cost_reduction': cost_reduction,
            'performance_impact': performance_impact,
            'throughput_change': throughput_change,
            'cost_savings': baseline['total_cost'] - routing['total_cost'],
            'baseline_cost': baseline['total_cost'],
            'routing_cost': routing['total_cost']
        }
    
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ Dynamic Model Routing Performance Test")
        print("=" * 70)
        
        # åˆ›å»ºæ¨¡æ‹Ÿå¤„ç†å™¨
        processors = await self.create_mock_processors()
        
        # ç”Ÿæˆæµ‹è¯•åœºæ™¯
        scenarios = self.generate_test_scenarios()
        
        # è·¯ç”±é…ç½®
        config = RoutingConfig(
            cost_priority=0.7,  # 70%ä¼˜å…ˆè€ƒè™‘æˆæœ¬
            daily_budget=100.0,
            monthly_budget=2000.0,
            per_user_budget=10.0,
            enable_auto_optimization=True
        )
        
        for scenario in scenarios:
            print(f"\nğŸ“Š {scenario['description']}")
            print("-" * 50)
            
            tasks = scenario['tasks']
            print(f"Task count: {len(tasks)}")
            
            # è¿è¡ŒåŸºçº¿æµ‹è¯•
            baseline_results = await self.run_baseline_test(tasks, processors)
            
            # è¿è¡Œè·¯ç”±æµ‹è¯•
            routing_results = await self.run_routing_test(tasks, processors, config)
            
            # åˆ†æç»“æœ
            analysis = self.analyze_cost_reduction(baseline_results, routing_results)
            
            # è¾“å‡ºç»“æœ
            self._print_scenario_results(
                scenario['name'],
                scenario['description'],
                baseline_results,
                routing_results,
                analysis,
                scenario['expected_cost_reduction']
            )
            
            # ä¿å­˜ç»“æœ
            self.test_results[scenario['name']] = {
                'baseline': baseline_results,
                'routing': routing_results,
                'analysis': analysis,
                'expected_reduction': scenario['expected_cost_reduction']
            }
        
        # è¾“å‡ºæ€»ç»“
        self._print_summary()
    
    def _print_scenario_results(
        self,
        scenario_name: str,
        description: str,
        baseline: dict,
        routing: dict,
        analysis: dict,
        expected_reduction: float
    ):
        """è¾“å‡ºåœºæ™¯æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“ˆ {description} ç»“æœ:")
        
        print(f"\n  åŸºçº¿æµ‹è¯• (æ€»æ˜¯ä½¿ç”¨GPT-4):")
        print(f"    æ€»æˆæœ¬: ${baseline['total_cost']:.4f}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {baseline['avg_response_time']:.3f}s")
        print(f"    ååé‡: {baseline['throughput']:.2f} req/s")
        print(f"    æ€»è€—æ—¶: {baseline['total_time']:.3f}s")
        
        print(f"\n  æ™ºèƒ½è·¯ç”±æµ‹è¯•:")
        print(f"    æ€»æˆæœ¬: ${routing['total_cost']:.4f}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {routing['avg_response_time']:.3f}s")
        print(f"    ååé‡: {routing['throughput']:.2f} req/s")
        print(f"    æ€»è€—æ—¶: {routing['total_time']:.3f}s")
        print(f"    æˆåŠŸç‡: {routing['success_rate']:.2%}")
        
        print(f"\n  æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ:")
        for model, count in routing['model_usage'].items():
            percentage = count / sum(routing['model_usage'].values()) * 100
            print(f"    {model}: {count} ({percentage:.1f}%)")
        
        print(f"\n  ğŸ¯ æˆæœ¬ä¼˜åŒ–æ•ˆæœ:")
        print(f"    æˆæœ¬é™ä½: {analysis['cost_reduction']:.1%}")
        print(f"    æˆæœ¬èŠ‚çœ: ${analysis['cost_savings']:.4f}")
        print(f"    æ€§èƒ½å½±å“: {analysis['performance_impact']:.1%}")
        print(f"    ååé‡å˜åŒ–: {analysis['throughput_change']:.1%}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if analysis['cost_reduction'] >= expected_reduction:
            print(f"    âœ… è¾¾åˆ°é¢„æœŸ{expected_reduction:.0%}æˆæœ¬é™ä½ç›®æ ‡!")
        elif analysis['cost_reduction'] >= expected_reduction * 0.8:
            print(f"    ğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œè¾¾åˆ°{analysis['cost_reduction']:.1%}æˆæœ¬é™ä½")
        else:
            print(f"    âŒ æœªè¾¾åˆ°{expected_reduction:.0%}æˆæœ¬é™ä½ç›®æ ‡")
    
    def _print_summary(self):
        """è¾“å‡ºæµ‹è¯•æ€»ç»“"""
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“")
        print("=" * 70)
        
        total_cost_reductions = []
        scenarios_meeting_target = 0
        
        for scenario_name, results in self.test_results.items():
            cost_reduction = results['analysis']['cost_reduction']
            expected_reduction = results['expected_reduction']
            
            total_cost_reductions.append(cost_reduction)
            
            if cost_reduction >= expected_reduction:
                scenarios_meeting_target += 1
        
        avg_cost_reduction = statistics.mean(total_cost_reductions)
        
        print(f"å¹³å‡æˆæœ¬é™ä½: {avg_cost_reduction:.1%}")
        print(f"è¾¾åˆ°é¢„æœŸç›®æ ‡çš„åœºæ™¯: {scenarios_meeting_target}/{len(self.test_results)}")
        
        if avg_cost_reduction >= 0.30:
            print(f"\nğŸ† åŠ¨æ€æ¨¡å‹è·¯ç”±ç³»ç»ŸæˆåŠŸå®ç°30-50%æˆæœ¬é™ä½ç›®æ ‡!")
        elif avg_cost_reduction >= 0.20:
            print(f"\nğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œå¹³å‡é™ä½{avg_cost_reduction:.1%}æˆæœ¬")
        else:
            print(f"\nâŒ æœªè¾¾åˆ°30%æˆæœ¬é™ä½ç›®æ ‡")
        
        # æœ€ä½³åœºæ™¯åˆ†æ
        best_scenario = max(
            self.test_results.items(),
            key=lambda x: x[1]['analysis']['cost_reduction']
        )
        
        print(f"\nğŸ“Š æœ€ä½³åœºæ™¯: {best_scenario[0]}")
        print(f"   æˆæœ¬é™ä½: {best_scenario[1]['analysis']['cost_reduction']:.1%}")
        print(f"   æ¨¡å‹åˆ†å¸ƒ: {best_scenario[1]['routing']['model_usage']}")
        
        # è·¯ç”±ç­–ç•¥æ•ˆæœåˆ†æ
        print(f"\nğŸ’¡ è·¯ç”±ç­–ç•¥æ•ˆæœåˆ†æ:")
        print(f"   ç®€å•ä»»åŠ¡åœºæ™¯: æˆæœ¬é™ä½æœ€æ˜¾è‘—ï¼Œé€‚åˆå®¢æœ/FAQåº”ç”¨")
        print(f"   æ··åˆä»»åŠ¡åœºæ™¯: å¹³è¡¡æˆæœ¬å’Œæ€§èƒ½ï¼Œé€‚åˆé€šç”¨èŠå¤©åº”ç”¨")
        print(f"   å¤æ‚ä»»åŠ¡åœºæ™¯: ä¿è¯è´¨é‡å‰æä¸‹é€‚åº¦é™æœ¬ï¼Œé€‚åˆä¸“ä¸šå’¨è¯¢")
        
        # æˆæœ¬ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ”§ æˆæœ¬ä¼˜åŒ–å»ºè®®:")
        print(f"   1. é’ˆå¯¹ç®€å•æŸ¥è¯¢ä¼˜å…ˆä½¿ç”¨è½»é‡æ¨¡å‹")
        print(f"   2. å¤æ‚ä»»åŠ¡ä¿æŒé«˜è´¨é‡æ¨¡å‹ä»¥ç¡®ä¿æ•ˆæœ")
        print(f"   3. ç»“åˆç¼“å­˜ç³»ç»Ÿè¿›ä¸€æ­¥é™ä½é‡å¤æŸ¥è¯¢æˆæœ¬")
        print(f"   4. æ ¹æ®ç”¨æˆ·ç­‰çº§åŠ¨æ€è°ƒæ•´æ¨¡å‹é€‰æ‹©ç­–ç•¥")


async def main():
    """ä¸»å‡½æ•°"""
    tester = RoutingPerformanceTester()
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    asyncio.run(main())

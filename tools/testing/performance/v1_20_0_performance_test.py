#!/usr/bin/env python3
"""
VoiceHelper v1.20.0 æ€§èƒ½æµ‹è¯•
æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–ã€æƒ…æ„Ÿè¯†åˆ«å’Œæ‰¹å¤„ç†è°ƒåº¦å™¨æ€§èƒ½
"""

import asyncio
import time
import json
import sys
import os
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

# å¯¼å…¥v1.20.0æ ¸å¿ƒæ¨¡å—
try:
    from algo.core.enhanced_voice_optimizer import EnhancedVoiceOptimizer, VoiceResponse
    from algo.core.advanced_emotion_recognition import AdvancedEmotionRecognition, EmotionAnalysisResult
    from algo.core.simple_batch_scheduler import SimpleBatchScheduler, ProcessRequest, RequestType, RequestPriority
    print("âœ… æˆåŠŸå¯¼å…¥v1.20.0æ ¸å¿ƒæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    success: bool
    metrics: Dict[str, Any]
    timestamp: str

class V120PerformanceTest:
    """v1.20.0æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.voice_optimizer = EnhancedVoiceOptimizer()
        self.emotion_recognizer = AdvancedEmotionRecognition()
        self.batch_scheduler = SimpleBatchScheduler()
        self.results = []
        
    async def test_voice_latency_optimization(self) -> PerformanceResult:
        """æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–"""
        print("\nğŸ¤ æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–...")
        
        test_cases = [
            {"audio_length": 1, "expected_latency": 100},   # 1ç§’éŸ³é¢‘ï¼ŒæœŸæœ›100ms
            {"audio_length": 3, "expected_latency": 150},   # 3ç§’éŸ³é¢‘ï¼ŒæœŸæœ›150ms
            {"audio_length": 5, "expected_latency": 200},   # 5ç§’éŸ³é¢‘ï¼ŒæœŸæœ›200ms
            {"audio_length": 10, "expected_latency": 300},  # 10ç§’éŸ³é¢‘ï¼ŒæœŸæœ›300ms
        ]
        
        total_latency = 0
        passed_tests = 0
        latency_results = []
        
        for case in test_cases:
            # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®
            audio_data = self._generate_test_audio(case["audio_length"])
            
            # æµ‹è¯•è¯­éŸ³ä¼˜åŒ–
            start_time = time.time()
            try:
                result = await self.voice_optimizer.optimize_voice_pipeline(audio_data)
                latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                
                latency_results.append({
                    "audio_length": case["audio_length"],
                    "expected_latency": case["expected_latency"],
                    "actual_latency": latency,
                    "passed": latency <= case["expected_latency"]
                })
                
                if latency <= case["expected_latency"]:
                    passed_tests += 1
                    print(f"  âœ… {case['audio_length']}ç§’éŸ³é¢‘: {latency:.1f}ms (ç›®æ ‡{case['expected_latency']}ms)")
                else:
                    print(f"  âŒ {case['audio_length']}ç§’éŸ³é¢‘: {latency:.1f}ms (ç›®æ ‡{case['expected_latency']}ms)")
                
                total_latency += latency
                
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
                latency_results.append({
                    "audio_length": case["audio_length"],
                    "expected_latency": case["expected_latency"],
                    "actual_latency": float('inf'),
                    "passed": False
                })
        
        avg_latency = total_latency / len(test_cases) if test_cases else 0
        success_rate = (passed_tests / len(test_cases)) * 100 if test_cases else 0
        
        return PerformanceResult(
            test_name="voice_latency_optimization",
            duration=time.time(),
            success=success_rate >= 75,  # 75%é€šè¿‡ç‡
            metrics={
                "average_latency": avg_latency,
                "success_rate": success_rate,
                "passed_tests": passed_tests,
                "total_tests": len(test_cases),
                "latency_results": latency_results
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_emotion_recognition_accuracy(self) -> PerformanceResult:
        """æµ‹è¯•æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡"""
        print("\nğŸ§  æµ‹è¯•æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡...")
        
        # æµ‹è¯•æ ·æœ¬
        test_samples = [
            {"text": "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒï¼", "expected_emotion": "happy"},
            {"text": "è¿™è®©æˆ‘å¾ˆæ²®ä¸§", "expected_emotion": "sad"},
            {"text": "æˆ‘ç”Ÿæ°”äº†ï¼", "expected_emotion": "angry"},
            {"text": "è¿™å¾ˆæ­£å¸¸", "expected_emotion": "neutral"},
            {"text": "å¤ªæ£’äº†ï¼", "expected_emotion": "excited"},
            {"text": "æˆ‘å¾ˆå¹³é™", "expected_emotion": "calm"},
            {"text": "è¿™è®©æˆ‘å¾ˆå›°æƒ‘", "expected_emotion": "confused"},
            {"text": "æˆ‘å¾ˆå¤±æœ›", "expected_emotion": "sad"},
            {"text": "å¤ªä»¤äººå…´å¥‹äº†ï¼", "expected_emotion": "excited"},
            {"text": "æˆ‘æ„Ÿåˆ°å¾ˆæ”¾æ¾", "expected_emotion": "calm"}
        ]
        
        correct_predictions = 0
        total_processing_time = 0
        confidence_scores = []
        
        for sample in test_samples:
            try:
                start_time = time.time()
                result = await self.emotion_recognizer.analyze_text_only(
                    text=sample["text"],
                    user_id="test_user"
                )
                processing_time = (time.time() - start_time) * 1000
                total_processing_time += processing_time
                
                # æ£€æŸ¥é¢„æµ‹ç»“æœ
                predicted_emotion = result.primary_emotion
                confidence = result.confidence
                confidence_scores.append(confidence)
                
                if predicted_emotion == sample["expected_emotion"]:
                    correct_predictions += 1
                    print(f"  âœ… '{sample['text']}' -> {predicted_emotion} (ç½®ä¿¡åº¦: {confidence:.2f})")
                else:
                    print(f"  âŒ '{sample['text']}' -> {predicted_emotion} (æœŸæœ›: {sample['expected_emotion']}, ç½®ä¿¡åº¦: {confidence:.2f})")
                
            except Exception as e:
                print(f"  âŒ æƒ…æ„Ÿè¯†åˆ«å¤±è´¥: {e}")
        
        accuracy = (correct_predictions / len(test_samples)) * 100 if test_samples else 0
        avg_processing_time = total_processing_time / len(test_samples) if test_samples else 0
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return PerformanceResult(
            test_name="emotion_recognition_accuracy",
            duration=time.time(),
            success=accuracy >= 40,  # å½“å‰ç›®æ ‡40%ï¼ˆæ¼”ç¤ºç‰ˆæœ¬ï¼‰
            metrics={
                "accuracy": accuracy,
                "correct_predictions": correct_predictions,
                "total_samples": len(test_samples),
                "avg_processing_time": avg_processing_time,
                "avg_confidence": avg_confidence,
                "confidence_scores": confidence_scores
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_batch_processing_throughput(self) -> PerformanceResult:
        """æµ‹è¯•æ‰¹å¤„ç†ååé‡"""
        print("\nâš¡ æµ‹è¯•æ‰¹å¤„ç†ååé‡...")
        
        # å¯åŠ¨æ‰¹å¤„ç†è°ƒåº¦å™¨
        await self.batch_scheduler.start()
        
        # æµ‹è¯•ä¸åŒæ‰¹å¤§å°çš„æ€§èƒ½
        batch_sizes = [10, 25, 50, 100, 200]
        throughput_results = []
        
        for batch_size in batch_sizes:
            print(f"  æµ‹è¯•æ‰¹å¤§å°: {batch_size}")
            
            # åˆ›å»ºæµ‹è¯•è¯·æ±‚
            requests = []
            for i in range(batch_size):
                request = ProcessRequest(
                    id=f"test_{i}",
                    type=RequestType.TEXT_GENERATION,
                    data=f"test_data_{i}",
                    user_id="test_user",
                    priority=RequestPriority.NORMAL
                )
                requests.append(request)
            
            # æäº¤è¯·æ±‚å¹¶æµ‹é‡æ€§èƒ½
            start_time = time.time()
            
            # æäº¤æ‰€æœ‰è¯·æ±‚
            for request in requests:
                await self.batch_scheduler.submit_request(request)
            
            # å¤„ç†æ‰¹æ¬¡
            batch_result = await self.batch_scheduler.process_batch(requests)
            
            end_time = time.time()
            duration = end_time - start_time
            throughput = batch_result.throughput
            
            throughput_results.append({
                "batch_size": batch_size,
                "duration": duration,
                "throughput": throughput
            })
            
            print(f"    ååé‡: {throughput:.1f} req/s")
        
        # åœæ­¢è°ƒåº¦å™¨
        await self.batch_scheduler.stop()
        
        # è®¡ç®—æœ€å¤§ååé‡
        max_throughput = max(result["throughput"] for result in throughput_results)
        
        return PerformanceResult(
            test_name="batch_processing_throughput",
            duration=time.time(),
            success=max_throughput >= 20,  # ç›®æ ‡20 req/s
            metrics={
                "max_throughput": max_throughput,
                "batch_results": throughput_results,
                "target_throughput": 20
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_system_stability(self) -> PerformanceResult:
        """æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§"""
        print("\nğŸ”„ æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§...")
        
        # ç®€åŒ–ç¨³å®šæ€§æµ‹è¯•
        test_requests = 100
        start_time = time.time()
        request_count = 0
        error_count = 0
        latencies = []
        
        print(f"  è¿è¡Œ{test_requests}ä¸ªè¯·æ±‚çš„ç¨³å®šæ€§æµ‹è¯•...")
        
        for i in range(test_requests):
            try:
                # æ¨¡æ‹Ÿè¯­éŸ³å¤„ç†è¯·æ±‚
                audio_data = self._generate_test_audio(1)
                request_start = time.time()
                
                # è¯­éŸ³ä¼˜åŒ–
                voice_result = await self.voice_optimizer.optimize_voice_pipeline(audio_data)
                
                # æƒ…æ„Ÿè¯†åˆ«
                emotion_result = await self.emotion_recognizer.analyze_text_only(
                    text=voice_result.text_response,
                    user_id="stability_test"
                )
                
                request_latency = (time.time() - request_start) * 1000
                latencies.append(request_latency)
                request_count += 1
                
                # æ¯20ä¸ªè¯·æ±‚è¾“å‡ºä¸€æ¬¡çŠ¶æ€
                if request_count % 20 == 0:
                    elapsed = time.time() - start_time
                    print(f"    å·²å¤„ç† {request_count} ä¸ªè¯·æ±‚ï¼Œè€—æ—¶ {elapsed:.1f}s")
                
            except Exception as e:
                error_count += 1
                print(f"    é”™è¯¯: {e}")
        
        total_duration = time.time() - start_time
        success_rate = ((request_count - error_count) / request_count) * 100 if request_count > 0 else 0
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0
        
        return PerformanceResult(
            test_name="system_stability",
            duration=total_duration,
            success=success_rate >= 99 and avg_latency <= 50,
            metrics={
                "total_requests": request_count,
                "error_count": error_count,
                "success_rate": success_rate,
                "avg_latency": avg_latency,
                "p95_latency": p95_latency,
                "test_duration": total_duration
            },
            timestamp=datetime.now().isoformat()
        )
    
    def _generate_test_audio(self, duration_seconds: int) -> bytes:
        """ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®"""
        # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼ˆå®é™…åº”è¯¥æ˜¯çœŸå®çš„éŸ³é¢‘å­—èŠ‚ï¼‰
        return b"fake_audio_data_" + str(duration_seconds).encode() * 100
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹VoiceHelper v1.20.0æ€§èƒ½æµ‹è¯•")
        print("=" * 50)
        
        start_time = time.time()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        tests = [
            self.test_voice_latency_optimization(),
            self.test_emotion_recognition_accuracy(),
            self.test_batch_processing_throughput(),
            self.test_system_stability()
        ]
        
        results = await asyncio.gather(*tests, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        test_results = {}
        passed_tests = 0
        total_tests = len(results)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ æµ‹è¯• {i+1} å¤±è´¥: {result}")
                test_results[f"test_{i+1}"] = {
                    "success": False,
                    "error": str(result)
                }
            else:
                test_results[result.test_name] = asdict(result)
                if result.success:
                    passed_tests += 1
                    print(f"âœ… {result.test_name}: é€šè¿‡")
                else:
                    print(f"âŒ {result.test_name}: å¤±è´¥")
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "overall_score": overall_score,
            "grade": self._get_grade(overall_score),
            "test_results": test_results,
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": f"{overall_score:.1f}%"
            }
        }
        
        total_duration = time.time() - start_time
        
        print("\n" + "=" * 50)
        print(f"ğŸ¯ æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä½“è¯„åˆ†: {overall_score:.1f}/100")
        print(f"æµ‹è¯•çŠ¶æ€: {self._get_grade(overall_score)}")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        
        return report
    
    def _get_grade(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (åˆæ ¼)"
        elif score >= 60:
            return "C (åŠæ ¼)"
        else:
            return "D (ä¸åŠæ ¼)"

async def main():
    """ä¸»å‡½æ•°"""
    tester = V120PerformanceTest()
    report = await tester.run_all_tests()
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_file = f"v1_20_0_performance_results_{int(time.time())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report

if __name__ == "__main__":
    asyncio.run(main())

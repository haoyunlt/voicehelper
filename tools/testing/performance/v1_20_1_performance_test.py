#!/usr/bin/env python3
"""
VoiceHelper v1.20.1 æ€§èƒ½æµ‹è¯•
æµ‹è¯•æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡æå‡ã€ç¼“å­˜ç›‘æ§å’Œæ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import time
import json
import sys
import os
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

# å¯¼å…¥v1.20.1æ ¸å¿ƒæ¨¡å—
try:
    from algo.core.enhanced_voice_optimizer import EnhancedVoiceOptimizer, VoiceResponse
    from algo.core.advanced_emotion_recognition import AdvancedEmotionRecognition, EmotionAnalysisResult
    from algo.core.simple_batch_scheduler import SimpleBatchScheduler, ProcessRequest, RequestType, RequestPriority
    from algo.core.production_emotion_model import predict_emotion_production, production_emotion_model
    from algo.core.cache_monitoring_system import get_cache_metrics, cache_monitor
    print("âœ… æˆåŠŸå¯¼å…¥v1.20.1æ ¸å¿ƒæ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    success: bool
    metrics: Dict[str, Any]
    timestamp: str

class V121PerformanceTest:
    """v1.20.1æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.voice_optimizer = EnhancedVoiceOptimizer()
        self.emotion_recognizer = AdvancedEmotionRecognition()
        self.batch_scheduler = SimpleBatchScheduler()
        self.results = []
        
    async def test_improved_emotion_recognition(self) -> PerformanceResult:
        """æµ‹è¯•æ”¹è¿›çš„æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡"""
        print("\nğŸ§  æµ‹è¯•æ”¹è¿›çš„æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡...")
        
        # æµ‹è¯•æ ·æœ¬ï¼ˆä¸v1.20.0ç›¸åŒçš„æ ·æœ¬ï¼‰
        test_samples = [
            {"text": "æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒï¼", "expected_emotion": "happy"},
            {"text": "è¿™è®©æˆ‘å¾ˆæ²®ä¸§", "expected_emotion": "sad"},
            {"text": "æˆ‘ç”Ÿæ°”äº†ï¼", "expected_emotion": "angry"},
            {"text": "è¿™å¾ˆæ­£å¸¸", "expected_emotion": "neutral"},
            {"text": "å¤ªæ£’äº†ï¼", "expected_emotion": "excited"},
            {"text": "æˆ‘å¾ˆå¹³é™", "expected_emotion": "calm"},
            {"text": "è¿™è®©æˆ‘å¾ˆå›°æƒ‘", "expected_emotion": "confused"},
            {"text": "æˆ‘å¾ˆå¤±æœ›", "expected_emotion": "sad"},
            {"text": "å¤ªä»¤äººå…´å¥‹äº†ï¼", "expected_emotion": "excited"},
            {"text": "æˆ‘æ„Ÿåˆ°å¾ˆæ”¾æ¾", "expected_emotion": "calm"}
        ]
        
        correct_predictions = 0
        total_processing_time = 0
        confidence_scores = []
        
        for sample in test_samples:
            try:
                start_time = time.time()
                
                # ä½¿ç”¨ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
                result = await predict_emotion_production(
                    text=sample["text"],
                    context={"user_id": "test_user"}
                )
                
                processing_time = (time.time() - start_time) * 1000
                total_processing_time += processing_time
                
                # æ£€æŸ¥é¢„æµ‹ç»“æœ
                predicted_emotion = result.emotion
                confidence = result.confidence
                confidence_scores.append(confidence)
                
                if predicted_emotion == sample["expected_emotion"]:
                    correct_predictions += 1
                    print(f"  âœ… '{sample['text']}' -> {predicted_emotion} (ç½®ä¿¡åº¦: {confidence:.2f})")
                else:
                    print(f"  âŒ '{sample['text']}' -> {predicted_emotion} (æœŸæœ›: {sample['expected_emotion']}, ç½®ä¿¡åº¦: {confidence:.2f})")
                
            except Exception as e:
                print(f"  âŒ æƒ…æ„Ÿè¯†åˆ«å¤±è´¥: {e}")
        
        accuracy = (correct_predictions / len(test_samples)) * 100 if test_samples else 0
        avg_processing_time = total_processing_time / len(test_samples) if test_samples else 0
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        return PerformanceResult(
            test_name="improved_emotion_recognition",
            duration=time.time(),
            success=accuracy >= 80,  # v1.20.1ç›®æ ‡80%
            metrics={
                "accuracy": accuracy,
                "correct_predictions": correct_predictions,
                "total_samples": len(test_samples),
                "avg_processing_time": avg_processing_time,
                "avg_confidence": avg_confidence,
                "confidence_scores": confidence_scores,
                "model_version": production_emotion_model.model_version
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_cache_monitoring_system(self) -> PerformanceResult:
        """æµ‹è¯•ç¼“å­˜ç›‘æ§ç³»ç»Ÿ"""
        print("\nğŸ“Š æµ‹è¯•ç¼“å­˜ç›‘æ§ç³»ç»Ÿ...")
        
        # æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
        cache_operations = [
            {"key": "user1:text:123", "type": "hit", "response_time": 0.01, "user": "user1", "request_type": "text_generation"},
            {"key": "user1:text:456", "type": "miss", "response_time": 0.05, "user": "user1", "request_type": "text_generation", "reason": "not_found"},
            {"key": "user2:voice:789", "type": "hit", "response_time": 0.02, "user": "user2", "request_type": "voice_synthesis"},
            {"key": "user2:voice:101", "type": "miss", "response_time": 0.08, "user": "user2", "request_type": "voice_synthesis", "reason": "expired"},
            {"key": "user3:emotion:202", "type": "hit", "response_time": 0.015, "user": "user3", "request_type": "emotion_analysis"},
        ]
        
        # è®°å½•ç¼“å­˜æ“ä½œ
        for op in cache_operations:
            if op["type"] == "hit":
                cache_monitor.record_hit(
                    cache_key=op["key"],
                    response_time=op["response_time"],
                    cache_size=100,
                    user_id=op["user"],
                    request_type=op["request_type"]
                )
            else:
                cache_monitor.record_miss(
                    cache_key=op["key"],
                    response_time=op["response_time"],
                    cache_size=100,
                    user_id=op["user"],
                    request_type=op["request_type"],
                    reason=op["reason"]
                )
        
        # è·å–ç¼“å­˜æŒ‡æ ‡
        metrics = await get_cache_metrics()
        
        # éªŒè¯æŒ‡æ ‡
        hit_rate = metrics["overall_metrics"]["hit_rate"]
        total_requests = metrics["overall_metrics"]["total_requests"]
        
        print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.2%}")
        print(f"  å¹³å‡å‘½ä¸­å“åº”æ—¶é—´: {metrics['overall_metrics']['avg_hit_response_time']:.3f}s")
        print(f"  å¹³å‡æœªå‘½ä¸­å“åº”æ—¶é—´: {metrics['overall_metrics']['avg_miss_response_time']:.3f}s")
        
        return PerformanceResult(
            test_name="cache_monitoring_system",
            duration=time.time(),
            success=hit_rate >= 0.4,  # è‡³å°‘40%å‘½ä¸­ç‡
            metrics={
                "hit_rate": hit_rate,
                "total_requests": total_requests,
                "avg_hit_response_time": metrics["overall_metrics"]["avg_hit_response_time"],
                "avg_miss_response_time": metrics["overall_metrics"]["avg_miss_response_time"],
                "top_users": metrics["top_users"],
                "type_breakdown": metrics["type_breakdown"]
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_voice_latency_optimization(self) -> PerformanceResult:
        """æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–ï¼ˆä¸v1.20.0å¯¹æ¯”ï¼‰"""
        print("\nğŸ¤ æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–...")
        
        test_cases = [
            {"audio_length": 1, "expected_latency": 100},
            {"audio_length": 3, "expected_latency": 150},
            {"audio_length": 5, "expected_latency": 200},
            {"audio_length": 10, "expected_latency": 300},
        ]
        
        total_latency = 0
        passed_tests = 0
        latency_results = []
        
        for case in test_cases:
            audio_data = self._generate_test_audio(case["audio_length"])
            
            start_time = time.time()
            try:
                result = await self.voice_optimizer.optimize_voice_pipeline(audio_data)
                latency = (time.time() - start_time) * 1000
                
                latency_results.append({
                    "audio_length": case["audio_length"],
                    "expected_latency": case["expected_latency"],
                    "actual_latency": latency,
                    "passed": latency <= case["expected_latency"]
                })
                
                if latency <= case["expected_latency"]:
                    passed_tests += 1
                    print(f"  âœ… {case['audio_length']}ç§’éŸ³é¢‘: {latency:.1f}ms (ç›®æ ‡{case['expected_latency']}ms)")
                else:
                    print(f"  âŒ {case['audio_length']}ç§’éŸ³é¢‘: {latency:.1f}ms (ç›®æ ‡{case['expected_latency']}ms)")
                
                total_latency += latency
                
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
                latency_results.append({
                    "audio_length": case["audio_length"],
                    "expected_latency": case["expected_latency"],
                    "actual_latency": float('inf'),
                    "passed": False
                })
        
        avg_latency = total_latency / len(test_cases) if test_cases else 0
        success_rate = (passed_tests / len(test_cases)) * 100 if test_cases else 0
        
        return PerformanceResult(
            test_name="voice_latency_optimization",
            duration=time.time(),
            success=success_rate >= 75,
            metrics={
                "average_latency": avg_latency,
                "success_rate": success_rate,
                "passed_tests": passed_tests,
                "total_tests": len(test_cases),
                "latency_results": latency_results
            },
            timestamp=datetime.now().isoformat()
        )
    
    async def test_system_stability_improved(self) -> PerformanceResult:
        """æµ‹è¯•æ”¹è¿›çš„ç³»ç»Ÿç¨³å®šæ€§"""
        print("\nğŸ”„ æµ‹è¯•æ”¹è¿›çš„ç³»ç»Ÿç¨³å®šæ€§...")
        
        test_requests = 50  # å‡å°‘æµ‹è¯•è¯·æ±‚æ•°é‡ä»¥åŠ å¿«æµ‹è¯•
        start_time = time.time()
        request_count = 0
        error_count = 0
        latencies = []
        
        print(f"  è¿è¡Œ{test_requests}ä¸ªè¯·æ±‚çš„ç¨³å®šæ€§æµ‹è¯•...")
        
        for i in range(test_requests):
            try:
                audio_data = self._generate_test_audio(1)
                request_start = time.time()
                
                # è¯­éŸ³ä¼˜åŒ–
                voice_result = await self.voice_optimizer.optimize_voice_pipeline(audio_data)
                
                # ä½¿ç”¨ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«
                emotion_result = await predict_emotion_production(
                    text=voice_result.text_response,
                    context={"user_id": "stability_test"}
                )
                
                request_latency = (time.time() - request_start) * 1000
                latencies.append(request_latency)
                request_count += 1
                
                if request_count % 10 == 0:
                    elapsed = time.time() - start_time
                    print(f"    å·²å¤„ç† {request_count} ä¸ªè¯·æ±‚ï¼Œè€—æ—¶ {elapsed:.1f}s")
                
            except Exception as e:
                error_count += 1
                print(f"    é”™è¯¯: {e}")
        
        total_duration = time.time() - start_time
        success_rate = ((request_count - error_count) / request_count) * 100 if request_count > 0 else 0
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0
        
        return PerformanceResult(
            test_name="system_stability_improved",
            duration=total_duration,
            success=success_rate >= 99 and avg_latency <= 50,
            metrics={
                "total_requests": request_count,
                "error_count": error_count,
                "success_rate": success_rate,
                "avg_latency": avg_latency,
                "p95_latency": p95_latency,
                "test_duration": total_duration
            },
            timestamp=datetime.now().isoformat()
        )
    
    def _generate_test_audio(self, duration_seconds: int) -> bytes:
        """ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®"""
        return b"fake_audio_data_" + str(duration_seconds).encode() * 100
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹VoiceHelper v1.20.1æ€§èƒ½æµ‹è¯•")
        print("=" * 50)
        
        start_time = time.time()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        tests = [
            self.test_improved_emotion_recognition(),
            self.test_cache_monitoring_system(),
            self.test_voice_latency_optimization(),
            self.test_system_stability_improved()
        ]
        
        results = await asyncio.gather(*tests, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        test_results = {}
        passed_tests = 0
        total_tests = len(results)
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ æµ‹è¯• {i+1} å¤±è´¥: {result}")
                test_results[f"test_{i+1}"] = {
                    "success": False,
                    "error": str(result)
                }
            else:
                test_results[result.test_name] = asdict(result)
                if result.success:
                    passed_tests += 1
                    print(f"âœ… {result.test_name}: é€šè¿‡")
                else:
                    print(f"âŒ {result.test_name}: å¤±è´¥")
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "version": "v1.20.1",
            "overall_score": overall_score,
            "grade": self._get_grade(overall_score),
            "test_results": test_results,
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "success_rate": f"{overall_score:.1f}%"
            }
        }
        
        total_duration = time.time() - start_time
        
        print("\n" + "=" * 50)
        print(f"ğŸ¯ v1.20.1æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä½“è¯„åˆ†: {overall_score:.1f}/100")
        print(f"æµ‹è¯•çŠ¶æ€: {self._get_grade(overall_score)}")
        print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
        print(f"æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        
        return report
    
    def _get_grade(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–ç­‰çº§"""
        if score >= 90:
            return "A+ (ä¼˜ç§€)"
        elif score >= 80:
            return "A (è‰¯å¥½)"
        elif score >= 70:
            return "B (åˆæ ¼)"
        elif score >= 60:
            return "C (åŠæ ¼)"
        else:
            return "D (ä¸åŠæ ¼)"

async def main():
    """ä¸»å‡½æ•°"""
    tester = V121PerformanceTest()
    report = await tester.run_all_tests()
    
    # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
    report_file = f"v1_20_1_performance_results_{int(time.time())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report

if __name__ == "__main__":
    asyncio.run(main())

#!/usr/bin/env python3
"""
VoiceHelper ç»Ÿä¸€æ€§èƒ½æµ‹è¯•å¥—ä»¶
åˆå¹¶æ‰€æœ‰æ€§èƒ½æµ‹è¯•åŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„æ€§èƒ½è¯„ä¼°
"""

import asyncio
import time
import json
import statistics
import concurrent.futures
import psutil
import requests
import random
import math
import os
import sys
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../'))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    success: bool
    duration: float
    metrics: Dict[str, Any]
    error_message: Optional[str] = None

class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('/')
        
        # è¿›ç¨‹å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        process_memory = process.memory_info()
        
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_available_gb': memory.available / (1024**3),
            'disk_percent': (disk.used / disk.total) * 100,
            'disk_free_gb': disk.free / (1024**3),
            'process_memory_mb': process_memory.rss / (1024**2),
            'process_memory_percent': process.memory_percent()
        }

class APITester:
    """APIæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.backend_url = "http://localhost:8080"
        self.algo_url = "http://localhost:8000"
        self.frontend_url = "http://localhost:3000"
        
    async def test_api_response_times(self) -> Dict[str, Any]:
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        endpoints = [
            (f"{self.backend_url}/health", "åç«¯å¥åº·æ£€æŸ¥"),
            (f"{self.algo_url}/health", "ç®—æ³•æœåŠ¡å¥åº·æ£€æŸ¥"),
            (f"{self.frontend_url}", "å‰ç«¯é¡µé¢")
        ]
        
        results = {}
        
        for url, name in endpoints:
            try:
                # é¢„çƒ­
                requests.get(url, timeout=5)
                
                # æµ‹è¯•å¤šæ¬¡å–å¹³å‡å€¼
                response_times = []
                for _ in range(10):
                    start_time = time.time()
                    response = requests.get(url, timeout=10)
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        response_times.append((end_time - start_time) * 1000)
                
                if response_times:
                    results[name] = {
                        'avg_response_time_ms': sum(response_times) / len(response_times),
                        'min_response_time_ms': min(response_times),
                        'max_response_time_ms': max(response_times),
                        'p95_response_time_ms': self._percentile(response_times, 95),
                        'success_rate': len(response_times) / 10,
                        'status': 'success'
                    }
                else:
                    results[name] = {'status': 'failed', 'error': 'All requests failed'}
                    
            except Exception as e:
                results[name] = {'status': 'error', 'error': str(e)}
        
        return results
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        index = int(percentile * len(sorted_data) / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

class ConcurrencyTester:
    """å¹¶å‘æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        
    async def test_concurrent_requests(self, concurrent_users: int = 10, duration: int = 30) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›"""
        
        def make_request():
            try:
                start_time = time.time()
                response = requests.get(f"{self.base_url}/health", timeout=5)
                end_time = time.time()
                return {
                    'success': response.status_code == 200,
                    'response_time_ms': (end_time - start_time) * 1000,
                    'status_code': response.status_code
                }
            except Exception as e:
                return {
                    'success': False,
                    'response_time_ms': None,
                    'error': str(e)
                }
        
        start_time = time.time()
        total_requests = 0
        successful_requests = 0
        response_times = []
        
        # è¿è¡Œå¹¶å‘æµ‹è¯•
        while time.time() - start_time < duration:
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = [executor.submit(make_request) for _ in range(concurrent_users)]
                results = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            for result in results:
                total_requests += 1
                if result['success']:
                    successful_requests += 1
                    if result['response_time_ms']:
                        response_times.append(result['response_time_ms'])
            
            await asyncio.sleep(0.1)  # çŸ­æš‚ä¼‘æ¯
        
        total_time = time.time() - start_time
        
        return {
            'concurrent_users': concurrent_users,
            'test_duration': total_time,
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'success_rate': (successful_requests / total_requests * 100) if total_requests > 0 else 0,
            'throughput_rps': total_requests / total_time if total_time > 0 else 0,
            'avg_response_time_ms': sum(response_times) / len(response_times) if response_times else 0,
            'p95_response_time_ms': self._percentile(response_times, 95) if response_times else 0
        }
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        index = int(percentile * len(sorted_data) / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

class BatchTester:
    """æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = []
        
    async def test_batch_vs_individual(self, batch_sizes: List[int] = [1, 4, 8, 16]) -> Dict[str, Any]:
        """æµ‹è¯•æ‰¹å¤„ç† vs å•ç‹¬å¤„ç†æ€§èƒ½"""
        
        results = {}
        
        for batch_size in batch_sizes:
            # æ¨¡æ‹Ÿæ‰¹å¤„ç†
            batch_start = time.time()
            await self._simulate_batch_processing(batch_size)
            batch_time = time.time() - batch_start
            
            # æ¨¡æ‹Ÿå•ç‹¬å¤„ç†
            individual_start = time.time()
            for _ in range(batch_size):
                await self._simulate_individual_processing()
            individual_time = time.time() - individual_start
            
            # è®¡ç®—æ”¹è¿›
            improvement = ((individual_time - batch_time) / individual_time * 100) if individual_time > 0 else 0
            
            results[f'batch_size_{batch_size}'] = {
                'batch_time': batch_time,
                'individual_time': individual_time,
                'improvement_percent': improvement,
                'throughput_improvement': individual_time / batch_time if batch_time > 0 else 1
            }
        
        return results
    
    async def _simulate_batch_processing(self, batch_size: int):
        """æ¨¡æ‹Ÿæ‰¹å¤„ç†"""
        # æ‰¹å¤„ç†æ•ˆç‡ï¼šæ‰¹æ¬¡è¶Šå¤§ï¼Œå•ä¸ªè¯·æ±‚çš„å¹³å‡æ—¶é—´è¶ŠçŸ­
        base_time = 0.1  # 100msåŸºç¡€æ—¶é—´
        efficiency_factor = 1.0 + (batch_size - 1) * 0.1  # æ¯å¢åŠ ä¸€ä¸ªè¯·æ±‚ï¼Œæ•ˆç‡æå‡10%
        actual_time = base_time * efficiency_factor / batch_size
        await asyncio.sleep(actual_time)
    
    async def _simulate_individual_processing(self):
        """æ¨¡æ‹Ÿå•ç‹¬å¤„ç†"""
        await asyncio.sleep(0.1)  # 100mså¤„ç†æ—¶é—´

class MemoryTester:
    """å†…å­˜æ€§èƒ½æµ‹è¯•å™¨"""
    
    def test_memory_usage(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        # è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        initial_memory = process.memory_info()
        
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†æ“ä½œ
        test_data = []
        for i in range(10000):
            test_data.append({
                'id': i,
                'message': f'Test message {i}',
                'timestamp': time.time(),
                'data': list(range(100))  # å¢åŠ å†…å­˜ä½¿ç”¨
            })
        
        # è·å–å³°å€¼å†…å­˜
        peak_memory = process.memory_info()
        
        # æ¸…ç†æ•°æ®
        del test_data
        
        # è·å–æ¸…ç†åå†…å­˜
        final_memory = process.memory_info()
        
        return {
            'initial_memory_mb': initial_memory.rss / (1024**2),
            'peak_memory_mb': peak_memory.rss / (1024**2),
            'final_memory_mb': final_memory.rss / (1024**2),
            'memory_increase_mb': (peak_memory.rss - initial_memory.rss) / (1024**2),
            'memory_retained_mb': (final_memory.rss - initial_memory.rss) / (1024**2),
            'memory_efficiency': (peak_memory.rss - final_memory.rss) / (peak_memory.rss - initial_memory.rss) if peak_memory.rss > initial_memory.rss else 0
        }

class UnifiedPerformanceTest:
    """ç»Ÿä¸€æ€§èƒ½æµ‹è¯•ä¸»ç±»"""
    
    def __init__(self):
        self.system_monitor = SystemMonitor()
        self.api_tester = APITester()
        self.concurrency_tester = ConcurrencyTester("http://localhost:8080")
        self.batch_tester = BatchTester()
        self.memory_tester = MemoryTester()
        self.results = []
        
    async def run_quick_test(self) -> Dict[str, Any]:
        """è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
        
        results = {
            'test_type': 'quick',
            'timestamp': datetime.now().isoformat(),
            'tests': {}
        }
        
        # 1. ç³»ç»Ÿèµ„æºæ£€æŸ¥
        logger.info("ğŸ“Š ç³»ç»Ÿèµ„æºæ£€æŸ¥")
        system_metrics = self.system_monitor.get_system_metrics()
        results['tests']['system_resources'] = TestResult(
            test_name="system_resources",
            success=True,
            duration=1.0,
            metrics=system_metrics
        ).__dict__
        
        # 2. APIå“åº”æ—¶é—´æµ‹è¯•
        logger.info("â±ï¸ APIå“åº”æ—¶é—´æµ‹è¯•")
        start_time = time.time()
        api_results = await self.api_tester.test_api_response_times()
        api_duration = time.time() - start_time
        
        results['tests']['api_performance'] = TestResult(
            test_name="api_performance",
            success=all(r.get('status') == 'success' for r in api_results.values()),
            duration=api_duration,
            metrics=api_results
        ).__dict__
        
        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        logger.info("ğŸ’¾ å†…å­˜ä½¿ç”¨æµ‹è¯•")
        start_time = time.time()
        memory_results = self.memory_tester.test_memory_usage()
        memory_duration = time.time() - start_time
        
        results['tests']['memory_usage'] = TestResult(
            test_name="memory_usage",
            success=memory_results['memory_increase_mb'] < 100,  # å†…å­˜å¢é•¿å°äº100MB
            duration=memory_duration,
            metrics=memory_results
        ).__dict__
        
        return results
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•")
        
        results = {
            'test_type': 'comprehensive',
            'timestamp': datetime.now().isoformat(),
            'tests': {}
        }
        
        # è¿è¡Œå¿«é€Ÿæµ‹è¯•
        quick_results = await self.run_quick_test()
        results['tests'].update(quick_results['tests'])
        
        # 4. å¹¶å‘æµ‹è¯•
        logger.info("ğŸ”„ å¹¶å‘æ€§èƒ½æµ‹è¯•")
        start_time = time.time()
        concurrency_results = await self.concurrency_tester.test_concurrent_requests(
            concurrent_users=10, duration=30
        )
        concurrency_duration = time.time() - start_time
        
        results['tests']['concurrency'] = TestResult(
            test_name="concurrency",
            success=concurrency_results['success_rate'] > 95,
            duration=concurrency_duration,
            metrics=concurrency_results
        ).__dict__
        
        # 5. æ‰¹å¤„ç†æµ‹è¯•
        logger.info("ğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
        start_time = time.time()
        batch_results = await self.batch_tester.test_batch_vs_individual()
        batch_duration = time.time() - start_time
        
        # æ£€æŸ¥æ‰¹å¤„ç†æ˜¯å¦æœ‰æ€§èƒ½æå‡
        avg_improvement = sum(
            r['improvement_percent'] for r in batch_results.values()
        ) / len(batch_results)
        
        results['tests']['batch_processing'] = TestResult(
            test_name="batch_processing",
            success=avg_improvement > 20,  # å¹³å‡æ”¹å–„è¶…è¿‡20%
            duration=batch_duration,
            metrics=batch_results
        ).__dict__
        
        return results
    
    def calculate_performance_score(self, results: Dict[str, Any]) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        score = 100
        
        for test_name, test_result in results['tests'].items():
            if not test_result['success']:
                score -= 20  # æ¯ä¸ªå¤±è´¥çš„æµ‹è¯•æ‰£20åˆ†
            
            # æ ¹æ®å…·ä½“æŒ‡æ ‡è°ƒæ•´åˆ†æ•°
            metrics = test_result['metrics']
            
            if test_name == 'system_resources':
                if metrics.get('cpu_percent', 0) > 80:
                    score -= 10
                if metrics.get('memory_percent', 0) > 90:
                    score -= 15
            
            elif test_name == 'api_performance':
                for endpoint, data in metrics.items():
                    if data.get('status') == 'success':
                        avg_time = data.get('avg_response_time_ms', 0)
                        if avg_time > 1000:  # è¶…è¿‡1ç§’
                            score -= 10
                        elif avg_time > 500:  # è¶…è¿‡500ms
                            score -= 5
            
            elif test_name == 'concurrency':
                if metrics.get('success_rate', 0) < 90:
                    score -= 15
                if metrics.get('avg_response_time_ms', 0) > 1000:
                    score -= 10
        
        return max(0, score)
    
    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ¯ VoiceHelper ç»Ÿä¸€æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        print(f"æµ‹è¯•ç±»å‹: {results['test_type']}")
        print(f"æµ‹è¯•æ—¶é—´: {results['timestamp']}")
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        score = self.calculate_performance_score(results)
        print(f"æ€»ä½“è¯„åˆ†: {score:.1f}/100")
        
        if score >= 90:
            print("ğŸ‰ æ€§èƒ½ä¼˜ç§€ï¼")
        elif score >= 70:
            print("âœ… æ€§èƒ½è‰¯å¥½")
        elif score >= 50:
            print("âš ï¸ æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–")
        else:
            print("âŒ æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, test_result in results['tests'].items():
            status = "âœ…" if test_result['success'] else "âŒ"
            print(f"\n{status} {test_result['test_name']} ({test_result['duration']:.2f}s)")
            
            metrics = test_result['metrics']
            
            if test_name == 'system_resources':
                print(f"  CPUä½¿ç”¨ç‡: {metrics.get('cpu_percent', 0):.1f}%")
                print(f"  å†…å­˜ä½¿ç”¨ç‡: {metrics.get('memory_percent', 0):.1f}%")
                print(f"  å¯ç”¨å†…å­˜: {metrics.get('memory_available_gb', 0):.2f} GB")
                print(f"  è¿›ç¨‹å†…å­˜: {metrics.get('process_memory_mb', 0):.2f} MB")
            
            elif test_name == 'api_performance':
                for endpoint, data in metrics.items():
                    if data.get('status') == 'success':
                        print(f"  {endpoint}: {data.get('avg_response_time_ms', 0):.2f}ms")
                    else:
                        print(f"  {endpoint}: å¤±è´¥ - {data.get('error', 'Unknown error')}")
            
            elif test_name == 'memory_usage':
                print(f"  å†…å­˜å¢é•¿: {metrics.get('memory_increase_mb', 0):.2f} MB")
                print(f"  å†…å­˜ä¿ç•™: {metrics.get('memory_retained_mb', 0):.2f} MB")
                print(f"  å†…å­˜æ•ˆç‡: {metrics.get('memory_efficiency', 0):.2%}")
            
            elif test_name == 'concurrency':
                print(f"  å¹¶å‘ç”¨æˆ·: {metrics.get('concurrent_users', 0)}")
                print(f"  æˆåŠŸç‡: {metrics.get('success_rate', 0):.1f}%")
                print(f"  ååé‡: {metrics.get('throughput_rps', 0):.2f} req/s")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {metrics.get('avg_response_time_ms', 0):.2f}ms")
            
            elif test_name == 'batch_processing':
                print(f"  æ‰¹å¤„ç†æµ‹è¯•ç»“æœ:")
                for batch_name, batch_data in metrics.items():
                    improvement = batch_data.get('improvement_percent', 0)
                    print(f"    {batch_name}: {improvement:.1f}% æ€§èƒ½æå‡")
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tests/performance_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="VoiceHelper ç»Ÿä¸€æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--test-type", choices=["quick", "comprehensive"], 
                       default="quick", help="æµ‹è¯•ç±»å‹")
    parser.add_argument("--output", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = UnifiedPerformanceTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        if args.test_type == "quick":
            results = await tester.run_quick_test()
        else:
            results = await tester.run_comprehensive_test()
        
        # æ˜¾ç¤ºç»“æœ
        tester.print_results(results)
        
        # ä¿å­˜ç»“æœ
        output_file = tester.save_results(results, args.output)
        
        # è¿”å›æˆåŠŸçŠ¶æ€
        score = tester.calculate_performance_score(results)
        return 0 if score >= 70 else 1
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

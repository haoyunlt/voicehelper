# VoiceHelper è¯­éŸ³é“¾è·¯ä¼˜åŒ–è¿­ä»£è®¡åˆ’

## ğŸ¯ è¿­ä»£æ¦‚è¿°

åŸºäºä¸šç•Œæœ€ä½³å®è·µå’Œæ€§èƒ½ä¼˜åŒ–éœ€æ±‚ï¼Œæœ¬æ¬¡è¿­ä»£ä¸“æ³¨äºè¯­éŸ³äº¤äº’é“¾è·¯çš„å…¨é¢ä¼˜åŒ–ï¼Œç›®æ ‡æ˜¯æ‰“é€ ä½å»¶è¿Ÿã€é«˜ç¨³å®šæ€§çš„å®æ—¶è¯­éŸ³å¯¹è¯ä½“éªŒã€‚

## ğŸ“… è¿­ä»£æ—¶é—´çº¿

**è¿­ä»£å‘¨æœŸ**: 48å°æ—¶å¿«é€Ÿè¿­ä»£ + 2å‘¨æ·±åº¦ä¼˜åŒ–  
**å¿«é€Ÿäº¤ä»˜**: 2025å¹´9æœˆ24æ—¥  
**å®Œæ•´ä¼˜åŒ–**: 2025å¹´10æœˆ8æ—¥  
**å½“å‰åˆ†æ”¯**: `v2-voice-optimization`

## ğŸš€ æ ¸å¿ƒä¼˜åŒ–ç›®æ ‡

### å»¶è¿Ÿä¼˜åŒ–ç›®æ ‡
- ğŸ¯ **ç«¯åˆ°ç«¯å»¶è¿Ÿ**: < 500ms (å½“å‰ >1000ms)
- ğŸ“Š **åˆ†æ®µå»¶è¿ŸæŒ‡æ ‡**: 
  - é‡‡é›†â†’ASR: < 100ms
  - ASRâ†’LLM: < 200ms  
  - LLMâ†’TTS: < 150ms
  - TTSâ†’æ’­æ”¾: < 50ms
- ğŸ”„ **æ‰“æ–­å“åº”**: < 120ms
- ğŸ“¡ **ç½‘ç»œæŠ–åŠ¨**: P95 < 80ms

### ç¨³å®šæ€§ç›®æ ‡
- ğŸµ **éŸ³é¢‘è´¨é‡**: ä¹±åº/ä¸¢å¸§ç‡ < 1%
- ğŸ”— **è¿æ¥ç¨³å®š**: 99.9% å¯ç”¨æ€§
- ğŸ’¾ **å†…å­˜å ç”¨**: ä¸»çº¿ç¨‹å ç”¨é™ä½ 40%
- ğŸ”Š **æ’­æ”¾è¿ç»­æ€§**: æ‹¼æ¥ç©ºéš™ < 20ms

## ğŸ“‹ 48å°æ—¶å¿«é€Ÿè¿­ä»£è®¡åˆ’

### Day 1 ä¸Šåˆ (4å°æ—¶): å‰ç«¯éŸ³é¢‘ç®¡çº¿é‡æ„

#### 1.1 AudioWorklet å®ç°
```typescript
// æ–°å¢æ–‡ä»¶ç»“æ„
frontend/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ worklets/
â”‚   â”‚   â”œâ”€â”€ MicProcessor.js      // é‡‡é›†å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ PlayerProcessor.js   // æ’­æ”¾å¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ AudioWorkletNode.ts  // å°è£…ç±»
â”‚   â”œâ”€â”€ buffers/
â”‚   â”‚   â”œâ”€â”€ JitterBuffer.ts      // æŠ–åŠ¨ç¼“å†²
â”‚   â”‚   â””â”€â”€ RingBuffer.ts        // ç¯å½¢ç¼“å†²
â”‚   â””â”€â”€ VoiceClient.ts           // ç»Ÿä¸€å®¢æˆ·ç«¯
```

**å®ç°è¦ç‚¹**:
- [x] MicProcessor: VADå‰ç½® + 16k/mono/PCM16 åˆ†å¸§(20ms)
- [x] PlayerProcessor: jitter buffer + è¿½å¸§ç­–ç•¥
- [x] ä¸»çº¿ç¨‹å¸è½½: é™å™ª/ä¸‹é‡‡æ ·ç§»è‡³Workletçº¿ç¨‹
- [x] éªŒè¯æŒ‡æ ‡: Chromeä»»åŠ¡ç®¡ç†å™¨è§‚å¯ŸCPUå ç”¨ä¸‹é™

#### 1.2 æ’­æ”¾ä¾§Jitter Buffer
```typescript
interface JitterBufferConfig {
  bufferSize: number;     // 80-120msç¼“å†²
  maxDropFrames: number;  // æœ€å¤šä¸¢å¼ƒå¸§æ•°
  adaptiveMode: boolean;  // è‡ªé€‚åº”è¿½å¸§
}
```

**æ ¸å¿ƒåŠŸèƒ½**:
- [x] æŒ‰æ—¶é—´æˆ³æ’åºéŸ³é¢‘å¸§
- [x] å°ç¼“å†²ç­–ç•¥(80-120ms)
- [x] è¿½å¸§æœºåˆ¶é˜²æ­¢ç§¯å‹
- [x] ç¯å½¢ç¼“å†²å®ç°

### Day 1 ä¸‹åˆ (4å°æ—¶): ç½‘å…³å±‚åè®®ä¼˜åŒ–

#### 1.3 ç»Ÿä¸€Envelopeæ ¼å¼
```go
// ç»Ÿä¸€äº‹ä»¶å°è£…
type EventEnvelope struct {
    Type      string                 `json:"type"`
    Data      interface{}           `json:"data"`
    Meta      EventMeta             `json:"meta"`
    Error     *ErrorInfo            `json:"error,omitempty"`
}

type EventMeta struct {
    SessionID   string    `json:"session_id"`
    TraceID     string    `json:"trace_id"`
    Timestamp   int64     `json:"timestamp"`
    SequenceNum int32     `json:"sequence_num"`
}
```

#### 1.4 WSäºŒè¿›åˆ¶å¸§è§„èŒƒ
```go
// éŸ³é¢‘å¸§ç»“æ„
type AudioFrame struct {
    Header AudioHeader `json:"header"`
    Data   []byte      `json:"data"`
}

type AudioHeader struct {
    SessionID    string `json:"session_id"`
    SequenceNum  int32  `json:"sequence_num"`
    SampleRate   int32  `json:"sample_rate"`
    Channels     int8   `json:"channels"`
    FrameSize    int16  `json:"frame_size"`
    Timestamp    int64  `json:"timestamp"`
}
```

#### 1.5 å¿ƒè·³ä¸èƒŒå‹æœºåˆ¶
```go
// èƒŒå‹æ§åˆ¶
type BackpressureController struct {
    queueSize     int32
    throttleLimit int32
    sendInterval  time.Duration
}

// å¿ƒè·³æ£€æµ‹
type HeartbeatManager struct {
    interval    time.Duration
    timeout     time.Duration
    missedBeats int32
}
```

### Day 2 ä¸Šåˆ (4å°æ—¶): ç®—æ³•æœåŠ¡äº‹ä»¶åŒ–

#### 2.1 LangGraphäº‹ä»¶æµ
```python
# äº‹ä»¶ç±»å‹å®šä¹‰
class AgentEvent:
    PLAN = "agent_plan"
    STEP = "agent_step" 
    TOOL_RESULT = "tool_result"
    SUMMARY = "summary"
    TTS_CHUNK = "tts_chunk"
    TTS_END = "tts_end"
    CANCELLED = "cancelled"

# èŠ‚ç‚¹äº‹ä»¶å‘å°„
async def emit_agent_event(event_type: str, data: dict, session_id: str):
    event = {
        "type": event_type,
        "data": data,
        "meta": {
            "session_id": session_id,
            "timestamp": int(time.time() * 1000),
            "trace_id": get_trace_id()
        }
    }
    await event_bus.publish(event)
```

#### 2.2 å¯ä¸­æ–­TTSå®ç°
```python
class StreamingTTSService:
    def __init__(self):
        self.active_sessions = {}
        self.cancellation_tokens = {}
    
    async def synthesize_streaming(self, text: str, session_id: str):
        # åˆ†å¥å¤„ç†
        sentences = self.split_sentences(text)
        
        for i, sentence in enumerate(sentences):
            if self.is_cancelled(session_id):
                await self.emit_event("tts_cancelled", session_id)
                break
                
            # åˆ†ç‰‡åˆæˆ
            async for chunk in self.tts_client.stream_synthesize(sentence):
                if self.is_cancelled(session_id):
                    break
                await self.emit_chunk(chunk, session_id)
        
        await self.emit_event("tts_end", session_id)
```

#### 2.3 VADç«¯ç‚¹æ£€æµ‹ä¼˜åŒ–
```python
class VADConfig:
    silence_threshold: float = 0.5    # é™é»˜é˜ˆå€¼
    min_speech_duration: int = 300    # æœ€çŸ­å‘å£°(ms)
    max_silence_duration: int = 800   # æœ€é•¿é™é»˜(ms)
    backfill_duration: int = 200      # å›å¡«æ—¶é•¿(ms)
    aggressive_mode: bool = True      # æ¿€è¿›æ¨¡å¼é™ä½å»¶è¿Ÿ

class EndpointDetector:
    def __init__(self, config: VADConfig):
        self.config = config
        self.state = "listening"
        
    def process_audio(self, audio_chunk: bytes) -> EndpointEvent:
        # VADå¤„ç†é€»è¾‘
        pass
```

### Day 2 ä¸‹åˆ (4å°æ—¶): è§‚æµ‹ä¸æµ‹è¯•

#### 2.4 äº”æ®µå»¶è¿Ÿç›‘æ§
```go
// å»¶è¿ŸæŒ‡æ ‡æ”¶é›†
type LatencyMetrics struct {
    CaptureMs  float64 `json:"capture_ms"`
    ASRMs      float64 `json:"asr_ms"`
    LLMMs      float64 `json:"llm_ms"`
    TTSMs      float64 `json:"tts_ms"`
    PlayMs     float64 `json:"play_ms"`
    E2EMs      float64 `json:"e2e_ms"`
}

// PrometheusæŒ‡æ ‡
var (
    voiceLatencyHistogram = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "voice_latency_seconds",
            Help: "Voice processing latency by stage",
            Buckets: []float64{0.05, 0.1, 0.2, 0.5, 1.0, 2.0},
        },
        []string{"stage", "session_id"},
    )
)
```

#### 2.5 éŸ³é¢‘è´¨é‡ç›‘æ§
```go
// éŸ³é¢‘å¥åº·åº¦æŒ‡æ ‡
type AudioHealthMetrics struct {
    OutOfOrderFrames int64   `json:"ooo_frames"`
    DroppedFrames    int64   `json:"dropped_frames"`
    JitterP95        float64 `json:"jitter_p95_ms"`
    PacketLoss       float64 `json:"packet_loss_rate"`
}
```

#### 2.6 å‹æµ‹è„šæœ¬
```javascript
// k6å‹æµ‹é…ç½®
export let options = {
    scenarios: {
        text_chat: {
            executor: 'constant-vus',
            vus: 70,
            duration: '5m',
        },
        voice_chat: {
            executor: 'constant-vus', 
            vus: 30,
            duration: '5m',
        }
    }
};

// è¯­éŸ³å‹æµ‹é€»è¾‘
export function voice_test() {
    let ws = new WebSocket('ws://localhost:8080/voice/stream');
    
    // å‘é€å›ºå®šå½•éŸ³æ–‡ä»¶
    let audioData = open('./test_audio.pcm', 'b');
    
    // æ³¨å…¥ä¹±åº/ä¸¢åŒ…
    if (Math.random() < 0.03) {
        // 3% æ¦‚ç‡ä¹±åº
        return;
    }
    
    ws.send(audioData);
}
```

## ğŸ”§ æ·±åº¦ä¼˜åŒ–é˜¶æ®µ (2å‘¨)

### Week 1: WebRTCè¿ç§»å‡†å¤‡

#### 3.1 ä¼ è¾“å±‚æŠ½è±¡
```typescript
// ä¼ è¾“æ¥å£æŠ½è±¡
interface AudioTransport {
    connect(config: TransportConfig): Promise<void>;
    send(data: AudioFrame): Promise<void>;
    onReceive(callback: (data: AudioFrame) => void): void;
    disconnect(): Promise<void>;
}

// WebSocketå®ç°
class WebSocketTransport implements AudioTransport {
    // ç°æœ‰WSå®ç°
}

// WebRTCå®ç° (æ–°å¢)
class WebRTCTransport implements AudioTransport {
    private peerConnection: RTCPeerConnection;
    private dataChannel: RTCDataChannel;
    
    async connect(config: TransportConfig) {
        // WebRTCè¿æ¥å»ºç«‹
        this.peerConnection = new RTCPeerConnection(config.iceServers);
        this.dataChannel = this.peerConnection.createDataChannel('audio');
    }
}
```

#### 3.2 ä¿¡ä»¤æœåŠ¡å™¨
```go
// WebRTCä¿¡ä»¤å¤„ç†
type SignalingServer struct {
    sessions map[string]*RTCSession
    mutex    sync.RWMutex
}

type RTCSession struct {
    SessionID    string
    PeerConn     *webrtc.PeerConnection
    DataChannel  *webrtc.DataChannel
    AudioTrack   *webrtc.TrackLocalStaticRTP
}

func (s *SignalingServer) HandleOffer(sessionID string, offer webrtc.SessionDescription) {
    // å¤„ç†WebRTC Offer
}
```

### Week 2: ç”Ÿäº§çº§ä¼˜åŒ–

#### 3.3 æµå¼TTSé›†æˆ
```python
# Deepgram TTSæµå¼æ¥å£
class DeepgramStreamingTTS:
    def __init__(self, api_key: str):
        self.client = DeepgramClient(api_key)
        
    async def stream_synthesize(self, text: str) -> AsyncIterator[bytes]:
        async with self.client.speak.stream() as stream:
            await stream.send_text(text)
            async for chunk in stream:
                yield chunk.data

# OpenAI Realtime APIé›†æˆ  
class OpenAIRealtimeTTS:
    async def stream_synthesize(self, text: str) -> AsyncIterator[bytes]:
        async with websockets.connect(self.realtime_url) as ws:
            await ws.send(json.dumps({
                "type": "response.create",
                "response": {"modalities": ["audio"], "voice": "alloy"}
            }))
            
            async for message in ws:
                data = json.loads(message)
                if data["type"] == "response.audio.delta":
                    yield base64.b64decode(data["delta"])
```

#### 3.4 Grafanaç›‘æ§é¢æ¿
```yaml
# è¯­éŸ³é“¾è·¯ç›‘æ§é¢æ¿é…ç½®
dashboard:
  title: "VoiceHelper è¯­éŸ³é“¾è·¯ç›‘æ§"
  panels:
    - title: "ç«¯åˆ°ç«¯å»¶è¿Ÿæ¼æ–—"
      type: "stat"
      targets:
        - expr: "histogram_quantile(0.95, voice_latency_seconds_bucket{stage='e2e'})"
          legendFormat: "E2E P95"
        
    - title: "äº”æ®µå»¶è¿Ÿåˆ†è§£"
      type: "graph" 
      targets:
        - expr: "voice_latency_seconds{stage='capture'}"
        - expr: "voice_latency_seconds{stage='asr'}"
        - expr: "voice_latency_seconds{stage='llm'}"
        - expr: "voice_latency_seconds{stage='tts'}"
        - expr: "voice_latency_seconds{stage='play'}"
          
    - title: "éŸ³é¢‘å¥åº·åº¦"
      type: "singlestat"
      targets:
        - expr: "rate(audio_ooo_frames_total[5m])"
          legendFormat: "ä¹±åºç‡"
        - expr: "rate(audio_dropped_frames_total[5m])"
          legendFormat: "ä¸¢å¸§ç‡"
          
    - title: "æ‰“æ–­æˆåŠŸç‡"
      type: "gauge"
      targets:
        - expr: "rate(barge_in_success_total[5m]) / rate(barge_in_attempts_total[5m])"
```

#### 3.5 E2Eè‡ªåŠ¨åŒ–æµ‹è¯•
```typescript
// Playwrightè¯­éŸ³E2Eæµ‹è¯•
describe('è¯­éŸ³äº¤äº’E2Eæµ‹è¯•', () => {
    test('å½•éŸ³-è¯†åˆ«-å›ç­”-æ’­æ”¾å®Œæ•´æµç¨‹', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // ä¸Šä¼ æµ‹è¯•å½•éŸ³æ–‡ä»¶
        const audioFile = './test-audio/hello.wav';
        await page.setInputFiles('#audio-input', audioFile);
        
        // éªŒè¯ASRç»“æœ
        await expect(page.locator('#asr-result')).toContainText('ä½ å¥½');
        
        // éªŒè¯LLMå“åº”
        await expect(page.locator('#llm-response')).toBeVisible();
        
        // éªŒè¯TTSæ’­æ”¾
        await expect(page.locator('#audio-player')).toHaveAttribute('playing', 'true');
        
        // éªŒè¯å»¶è¿ŸæŒ‡æ ‡
        const e2eLatency = await page.locator('#e2e-latency').textContent();
        expect(parseInt(e2eLatency)).toBeLessThan(500);
    });
    
    test('æ‰“æ–­åŠŸèƒ½æµ‹è¯•', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // å¼€å§‹TTSæ’­æ”¾
        await page.click('#start-tts');
        await expect(page.locator('#tts-status')).toContainText('playing');
        
        // æ¨¡æ‹Ÿç”¨æˆ·æ‰“æ–­
        await page.click('#interrupt-button');
        
        // éªŒè¯æ‰“æ–­å“åº”æ—¶é—´
        const interruptLatency = await page.locator('#interrupt-latency').textContent();
        expect(parseInt(interruptLatency)).toBeLessThan(120);
        
        // éªŒè¯TTSåœæ­¢
        await expect(page.locator('#tts-status')).toContainText('cancelled');
    });
    
    test('ç½‘ç»œå¼‚å¸¸æ¢å¤æµ‹è¯•', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // æ¨¡æ‹Ÿç½‘ç»œæ–­å¼€
        await page.route('**/voice/stream', route => route.abort());
        
        // éªŒè¯é‡è¿æœºåˆ¶
        await page.waitForSelector('#reconnecting-indicator');
        
        // æ¢å¤ç½‘ç»œ
        await page.unroute('**/voice/stream');
        
        // éªŒè¯è‡ªåŠ¨é‡è¿
        await expect(page.locator('#connection-status')).toContainText('connected');
    });
});
```

## ğŸ“Š æˆåŠŸéªŒæ”¶æ ‡å‡†

### æ€§èƒ½æŒ‡æ ‡
- âœ… **ç«¯åˆ°ç«¯å»¶è¿Ÿ**: P95 < 500ms
- âœ… **æ‰“æ–­å“åº”**: < 120ms  
- âœ… **éŸ³é¢‘è´¨é‡**: ä¹±åº/ä¸¢å¸§ç‡ < 1%
- âœ… **ä¸»çº¿ç¨‹å ç”¨**: é™ä½ 40%
- âœ… **æ’­æ”¾è¿ç»­æ€§**: æ‹¼æ¥ç©ºéš™ < 20ms

### åŠŸèƒ½æŒ‡æ ‡  
- âœ… **AudioWorklet**: é‡‡é›†/æ’­æ”¾ç®¡çº¿é‡æ„å®Œæˆ
- âœ… **Jitter Buffer**: æŠ–åŠ¨ç¼“å†²ä¸è¿½å¸§ç­–ç•¥
- âœ… **äº‹ä»¶åŒ–**: LangGraphèŠ‚ç‚¹äº‹ä»¶æµ
- âœ… **å¯ä¸­æ–­TTS**: åˆ†ç‰‡åˆæˆä¸å–æ¶ˆæœºåˆ¶
- âœ… **VADä¼˜åŒ–**: ç«¯ç‚¹æ£€æµ‹å‚æ•°å¯é…

### è§‚æµ‹æŒ‡æ ‡
- âœ… **äº”æ®µå»¶è¿Ÿ**: å®Œæ•´é“¾è·¯ç›‘æ§
- âœ… **éŸ³é¢‘å¥åº·**: ä¹±åº/ä¸¢å¸§/æŠ–åŠ¨ç›‘æ§  
- âœ… **å‹æµ‹è¦†ç›–**: æ–‡æœ¬70% + è¯­éŸ³30%
- âœ… **E2Eæµ‹è¯•**: æ‰“æ–­/é‡è¿/å¼‚å¸¸æ¢å¤
- âœ… **Grafanaé¢æ¿**: å®æ—¶ç›‘æ§çœ‹æ¿

## ğŸ› ï¸ æŠ€æœ¯å€ºåŠ¡æ¸…ç†

### ä»£ç é‡æ„
- [x] **ç›®å½•æ”¶å£**: æ ‡è®°labs/examplesï¼Œä¸»çº¿èšç„¦
- [x] **CODEOWNERS**: ä»£ç å®¡æŸ¥è´£ä»»äºº
- [x] **pre-commit**: ä»£ç è´¨é‡é—¨ç¦
- [x] **Makefile**: ä¸€é”®å¼€å‘ç¯å¢ƒ

### åŸºç¡€è®¾æ–½
- [x] **docker-compose**: healthcheck + ä¾èµ–æ’åº
- [x] **CI/CD**: Lint + å•æµ‹ + E2E + é•œåƒæ¨é€
- [x] **ç¯å¢ƒé…ç½®**: .env.local.example

## ğŸ¯ é‡Œç¨‹ç¢‘æ£€æŸ¥ç‚¹

### 48å°æ—¶å¿«é€Ÿäº¤ä»˜ (2025-09-24)
- âœ… AudioWorkleté‡æ„å®Œæˆ
- âœ… ç½‘å…³åè®®ä¼˜åŒ–
- âœ… ç®—æ³•æœåŠ¡äº‹ä»¶åŒ–  
- âœ… åŸºç¡€ç›‘æ§ä¸Šçº¿

### 1å‘¨æ·±åº¦ä¼˜åŒ– (2025-10-01)
- âœ… WebRTCä¼ è¾“å±‚æŠ½è±¡
- âœ… æµå¼TTSé›†æˆ
- âœ… å®Œæ•´å‹æµ‹è¦†ç›–
- âœ… Grafanaç›‘æ§é¢æ¿

### 2å‘¨ç”Ÿäº§å°±ç»ª (2025-10-08)
- âœ… æ€§èƒ½æŒ‡æ ‡å…¨éƒ¨è¾¾æ ‡
- âœ… E2Eæµ‹è¯•è‡ªåŠ¨åŒ–
- âœ… æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- âœ… æ–‡æ¡£å®Œå–„

## ğŸ”„ é£é™©æ§åˆ¶ä¸å›æ»š

### é£é™©è¯†åˆ«
1. **AudioWorkletå…¼å®¹æ€§**: Safari/ç§»åŠ¨ç«¯æ”¯æŒæœ‰é™
2. **WebRTCå¤æ‚æ€§**: ä¿¡ä»¤æœåŠ¡å™¨å¼€å‘æˆæœ¬
3. **TTSæµå¼æ¥å£**: ç¬¬ä¸‰æ–¹æœåŠ¡ä¾èµ–é£é™©
4. **æ€§èƒ½å›å½’**: ä¼˜åŒ–å¯èƒ½å¼•å…¥æ–°ç“¶é¢ˆ

### ç¼“è§£ç­–ç•¥
1. **æ¸è¿›å¼è¿ç§»**: ä¿ç•™WebSocketä½œä¸ºfallback
2. **ç‰¹æ€§å¼€å…³**: æ–°åŠŸèƒ½å¯åŠ¨æ€å¼€å…³
3. **ç›‘æ§å‘Šè­¦**: å…³é”®æŒ‡æ ‡é˜ˆå€¼å‘Šè­¦
4. **å¿«é€Ÿå›æ»š**: ä¸€é”®å›æ»šåˆ°ç¨³å®šç‰ˆæœ¬

### å›æ»šé¢„æ¡ˆ
```bash
# ç´§æ€¥å›æ»šè„šæœ¬
#!/bin/bash
echo "å¼€å§‹ç´§æ€¥å›æ»š..."

# 1. åˆ‡æ¢ç‰¹æ€§å¼€å…³
curl -X POST /admin/feature-toggle -d '{"audio_worklet": false}'

# 2. å›æ»šä»£ç ç‰ˆæœ¬
git checkout v2-stable
docker-compose up -d --force-recreate

# 3. éªŒè¯æœåŠ¡çŠ¶æ€
./scripts/health_check.sh

echo "å›æ»šå®Œæˆï¼ŒæœåŠ¡å·²æ¢å¤"
```

## ğŸ“š å‚è€ƒèµ„æ–™ä¸å¯¹æ ‡

### ä¸šç•Œæœ€ä½³å®è·µ
- **AudioWorklet**: [MDN Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet)
- **WebRTC**: [Azure OpenAI Realtime](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/realtime-audio-webrtc)
- **æµå¼TTS**: [Deepgram TTS WebSocket](https://developers.deepgram.com/docs/tts-websocket)
- **å·¥ç¨‹åŒ–å‚è€ƒ**: [Pipecat AI Framework](https://github.com/pipecat-ai/pipecat)

### æŠ€æœ¯é€‰å‹å¯¹æ¯”
| æŠ€æœ¯æ–¹æ¡ˆ | å»¶è¿Ÿ | ç¨³å®šæ€§ | å¼€å‘æˆæœ¬ | æ¨èåœºæ™¯ |
|---------|------|--------|----------|----------|
| WebSocket | ä¸­ç­‰ | é«˜ | ä½ | å†…ç½‘/ç®€å•åœºæ™¯ |
| WebRTC | ä½ | é«˜ | é«˜ | å…¬ç½‘/å®æ—¶åœºæ™¯ |
| SSE | é«˜ | ä¸­ç­‰ | ä½ | æ–‡æœ¬æµå¼ |
| AudioWorklet | ä½ | é«˜ | ä¸­ç­‰ | éŸ³é¢‘å¤„ç† |

---

## ğŸ‰ å¼€å§‹è¯­éŸ³ä¼˜åŒ–ä¹‹æ—…ï¼

è¿™æ¬¡è¿­ä»£å°†æ˜¾è‘—æå‡VoiceHelperçš„è¯­éŸ³äº¤äº’ä½“éªŒï¼Œé€šè¿‡ä¸šç•Œæœ€ä½³å®è·µçš„å¼•å…¥ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªä½å»¶è¿Ÿã€é«˜ç¨³å®šæ€§çš„å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿã€‚

è®©æˆ‘ä»¬ç”¨48å°æ—¶è¯æ˜æŠ€æœ¯çš„åŠ›é‡ï¼ğŸš€

---

*åˆ›å»ºæ—¶é—´: 2025-09-22*  
*è¿­ä»£åˆ†æ”¯: v2-voice-optimization*  
*ç›®æ ‡äº¤ä»˜: 2025-09-24 (å¿«é€Ÿç‰ˆ) / 2025-10-08 (å®Œæ•´ç‰ˆ)*

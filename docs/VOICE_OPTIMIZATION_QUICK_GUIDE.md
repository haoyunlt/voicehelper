# è¯­éŸ³ä¼˜åŒ–å¿«é€Ÿå®æ–½æŒ‡å—

## ğŸš€ 48å°æ—¶è½åœ°æ¸…å•

### Day 1 ä¸Šåˆ (4å°æ—¶) - å‰ç«¯AudioWorkleté‡æ„

#### 1. åˆ›å»ºéŸ³é¢‘å¤„ç†ç›®å½•ç»“æ„
```bash
mkdir -p frontend/audio/{worklets,buffers}
```

#### 2. å®ç°MicProcessor.js
```javascript
// frontend/audio/worklets/MicProcessor.js
class MicProcessor extends AudioWorkletProcessor {
    constructor() {
        super();
        this.frameSize = 320; // 20ms @ 16kHz
        this.buffer = new Float32Array(this.frameSize);
        this.bufferIndex = 0;
    }
    
    process(inputs, outputs, parameters) {
        const input = inputs[0];
        if (input.length > 0) {
            const inputChannel = input[0];
            
            for (let i = 0; i < inputChannel.length; i++) {
                this.buffer[this.bufferIndex++] = inputChannel[i];
                
                if (this.bufferIndex >= this.frameSize) {
                    // å‘é€å®Œæ•´å¸§åˆ°ä¸»çº¿ç¨‹
                    this.port.postMessage({
                        type: 'audioFrame',
                        data: this.buffer.slice(),
                        timestamp: currentTime
                    });
                    this.bufferIndex = 0;
                }
            }
        }
        return true;
    }
}

registerProcessor('mic-processor', MicProcessor);
```

#### 3. å®ç°PlayerProcessor.js
```javascript
// frontend/audio/worklets/PlayerProcessor.js
class PlayerProcessor extends AudioWorkletProcessor {
    constructor() {
        super();
        this.jitterBuffer = [];
        this.bufferSize = 5; // 5å¸§ç¼“å†²
        this.playIndex = 0;
    }
    
    process(inputs, outputs, parameters) {
        const output = outputs[0];
        
        if (this.jitterBuffer.length > this.bufferSize) {
            // ä»ç¼“å†²åŒºæ’­æ”¾
            const frame = this.jitterBuffer.shift();
            if (frame && output.length > 0) {
                output[0].set(frame.data);
            }
        }
        
        return true;
    }
}

registerProcessor('player-processor', PlayerProcessor);
```

#### 4. å®ç°JitterBuffer.ts
```typescript
// frontend/audio/buffers/JitterBuffer.ts
interface AudioFrame {
    data: Float32Array;
    timestamp: number;
    sequenceNum: number;
}

export class JitterBuffer {
    private buffer: AudioFrame[] = [];
    private maxSize = 10;
    private targetSize = 5;
    
    addFrame(frame: AudioFrame) {
        // æŒ‰æ—¶é—´æˆ³æ’åºæ’å…¥
        const insertIndex = this.findInsertPosition(frame.timestamp);
        this.buffer.splice(insertIndex, 0, frame);
        
        // ç¼“å†²åŒºæº¢å‡ºå¤„ç†
        if (this.buffer.length > this.maxSize) {
            this.buffer.shift(); // ä¸¢å¼ƒæœ€æ—§å¸§
        }
    }
    
    getFrame(): AudioFrame | null {
        if (this.buffer.length >= this.targetSize) {
            return this.buffer.shift() || null;
        }
        return null;
    }
    
    private findInsertPosition(timestamp: number): number {
        for (let i = 0; i < this.buffer.length; i++) {
            if (this.buffer[i].timestamp > timestamp) {
                return i;
            }
        }
        return this.buffer.length;
    }
}
```

### Day 1 ä¸‹åˆ (4å°æ—¶) - ç½‘å…³åè®®ä¼˜åŒ–

#### 5. ç»Ÿä¸€äº‹ä»¶å°è£… (Go)
```go
// backend/pkg/types/events.go
package types

import "time"

type EventEnvelope struct {
    Type  string      `json:"type"`
    Data  interface{} `json:"data"`
    Meta  EventMeta   `json:"meta"`
    Error *ErrorInfo  `json:"error,omitempty"`
}

type EventMeta struct {
    SessionID   string    `json:"session_id"`
    TraceID     string    `json:"trace_id"`
    Timestamp   int64     `json:"timestamp"`
    SequenceNum int32     `json:"sequence_num"`
}

type AudioFrame struct {
    Header AudioHeader `json:"header"`
    Data   []byte      `json:"data"`
}

type AudioHeader struct {
    SessionID   string `json:"session_id"`
    SequenceNum int32  `json:"sequence_num"`
    SampleRate  int32  `json:"sample_rate"`
    Channels    int8   `json:"channels"`
    FrameSize   int16  `json:"frame_size"`
    Timestamp   int64  `json:"timestamp"`
}
```

#### 6. WebSocketå¤„ç†å™¨ä¼˜åŒ–
```go
// backend/internal/handlers/voice_ws.go
package handlers

import (
    "github.com/gorilla/websocket"
    "github.com/prometheus/client_golang/prometheus"
)

var (
    wsActiveConnections = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "ws_active_connections",
        Help: "Number of active WebSocket connections",
    })
    
    audioFramesReceived = prometheus.NewCounterVec(prometheus.CounterOpts{
        Name: "audio_frames_received_total", 
        Help: "Total audio frames received",
    }, []string{"session_id"})
)

type VoiceWSHandler struct {
    sessions map[string]*Session
    eventBus EventBus
}

func (h *VoiceWSHandler) HandleConnection(conn *websocket.Conn) {
    sessionID := generateSessionID()
    session := &Session{
        ID:   sessionID,
        Conn: conn,
        SendQueue: make(chan []byte, 100),
    }
    
    h.sessions[sessionID] = session
    wsActiveConnections.Inc()
    
    go h.handleIncoming(session)
    go h.handleOutgoing(session)
}

func (h *VoiceWSHandler) handleIncoming(session *Session) {
    defer func() {
        delete(h.sessions, session.ID)
        wsActiveConnections.Dec()
    }()
    
    for {
        _, message, err := session.Conn.ReadMessage()
        if err != nil {
            break
        }
        
        // è§£æéŸ³é¢‘å¸§
        frame, err := parseAudioFrame(message)
        if err != nil {
            continue
        }
        
        audioFramesReceived.WithLabelValues(session.ID).Inc()
        
        // å‘é€åˆ°ç®—æ³•æœåŠ¡
        h.eventBus.Publish("audio_frame", frame)
    }
}
```

### Day 2 ä¸Šåˆ (4å°æ—¶) - ç®—æ³•æœåŠ¡äº‹ä»¶åŒ–

#### 7. LangGraphäº‹ä»¶å‘å°„å™¨
```python
# algo/core/events.py
import asyncio
import json
import time
from typing import Dict, Any
from enum import Enum

class EventType(Enum):
    AGENT_PLAN = "agent_plan"
    AGENT_STEP = "agent_step"
    TOOL_RESULT = "tool_result"
    SUMMARY = "summary"
    TTS_CHUNK = "tts_chunk"
    TTS_END = "tts_end"
    CANCELLED = "cancelled"

class EventEmitter:
    def __init__(self):
        self.subscribers = {}
    
    async def emit(self, event_type: EventType, data: Dict[str, Any], session_id: str):
        event = {
            "type": event_type.value,
            "data": data,
            "meta": {
                "session_id": session_id,
                "timestamp": int(time.time() * 1000),
                "trace_id": self.get_trace_id()
            }
        }
        
        # å‘é€åˆ°æ‰€æœ‰è®¢é˜…è€…
        if event_type in self.subscribers:
            for callback in self.subscribers[event_type]:
                await callback(event)
    
    def subscribe(self, event_type: EventType, callback):
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(callback)
```

#### 8. å¯ä¸­æ–­TTSæœåŠ¡
```python
# algo/services/streaming_tts.py
import asyncio
from typing import AsyncIterator
import aiohttp

class StreamingTTSService:
    def __init__(self):
        self.active_sessions = set()
        self.cancellation_tokens = {}
    
    async def synthesize_streaming(self, text: str, session_id: str) -> AsyncIterator[bytes]:
        self.active_sessions.add(session_id)
        self.cancellation_tokens[session_id] = False
        
        try:
            # åˆ†å¥å¤„ç†
            sentences = self.split_sentences(text)
            
            for sentence in sentences:
                if self.is_cancelled(session_id):
                    await self.emit_event(EventType.CANCELLED, {}, session_id)
                    break
                
                # è°ƒç”¨TTS API
                async for chunk in self.call_tts_api(sentence):
                    if self.is_cancelled(session_id):
                        break
                    
                    await self.emit_event(EventType.TTS_CHUNK, {
                        "audio_data": chunk,
                        "chunk_index": self.chunk_index
                    }, session_id)
                    
                    yield chunk
            
            await self.emit_event(EventType.TTS_END, {}, session_id)
            
        finally:
            self.active_sessions.discard(session_id)
            self.cancellation_tokens.pop(session_id, None)
    
    def cancel_session(self, session_id: str):
        self.cancellation_tokens[session_id] = True
    
    def is_cancelled(self, session_id: str) -> bool:
        return self.cancellation_tokens.get(session_id, False)
    
    def split_sentences(self, text: str) -> list:
        # ç®€å•çš„å¥å­åˆ†å‰²
        import re
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]
```

#### 9. VADç«¯ç‚¹æ£€æµ‹ä¼˜åŒ–
```python
# algo/core/vad.py
from dataclasses import dataclass
from typing import Optional
import numpy as np

@dataclass
class VADConfig:
    silence_threshold: float = 0.5
    min_speech_duration: int = 300  # ms
    max_silence_duration: int = 800  # ms
    backfill_duration: int = 200     # ms
    aggressive_mode: bool = True

class EndpointDetector:
    def __init__(self, config: VADConfig):
        self.config = config
        self.state = "listening"
        self.speech_start = None
        self.silence_start = None
        self.audio_buffer = []
    
    def process_audio(self, audio_chunk: np.ndarray, timestamp: int) -> Optional[str]:
        # è®¡ç®—éŸ³é¢‘èƒ½é‡
        energy = np.sqrt(np.mean(audio_chunk ** 2))
        
        if energy > self.config.silence_threshold:
            # æ£€æµ‹åˆ°è¯­éŸ³
            if self.state == "listening":
                self.speech_start = timestamp
                self.state = "speaking"
                return "speech_start"
            elif self.state == "silence":
                # ä»é™é»˜è½¬ä¸ºè¯­éŸ³
                self.state = "speaking"
                self.silence_start = None
        else:
            # æ£€æµ‹åˆ°é™é»˜
            if self.state == "speaking":
                if self.silence_start is None:
                    self.silence_start = timestamp
                elif timestamp - self.silence_start > self.config.max_silence_duration:
                    # é™é»˜æ—¶é—´è¶³å¤Ÿé•¿ï¼Œåˆ¤å®šä¸ºè¯­éŸ³ç»“æŸ
                    if self.speech_start and timestamp - self.speech_start > self.config.min_speech_duration:
                        self.state = "listening"
                        return "speech_end"
        
        return None
```

### Day 2 ä¸‹åˆ (4å°æ—¶) - ç›‘æ§ä¸æµ‹è¯•

#### 10. PrometheusæŒ‡æ ‡æ”¶é›†
```go
// backend/pkg/metrics/voice_metrics.go
package metrics

import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    VoiceLatencyHistogram = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "voice_latency_seconds",
            Help: "Voice processing latency by stage",
            Buckets: []float64{0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0},
        },
        []string{"stage", "session_id"},
    )
    
    AudioHealthMetrics = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "audio_health_score",
            Help: "Audio quality health metrics",
        },
        []string{"metric_type", "session_id"},
    )
    
    BargeInCounter = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "barge_in_total",
            Help: "Total barge-in attempts and successes",
        },
        []string{"result", "session_id"},
    )
)

// å»¶è¿Ÿè®°å½•å‡½æ•°
func RecordLatency(stage string, sessionID string, duration float64) {
    VoiceLatencyHistogram.WithLabelValues(stage, sessionID).Observe(duration)
}
```

#### 11. k6å‹æµ‹è„šæœ¬
```javascript
// tests/performance/voice_load_test.js
import ws from 'k6/ws';
import { check } from 'k6';
import { Counter, Trend } from 'k6/metrics';

const wsConnections = new Counter('ws_connections');
const e2eLatency = new Trend('e2e_latency');

export let options = {
    scenarios: {
        text_chat: {
            executor: 'constant-vus',
            vus: 70,
            duration: '5m',
            exec: 'textChatTest',
        },
        voice_chat: {
            executor: 'constant-vus',
            vus: 30, 
            duration: '5m',
            exec: 'voiceChatTest',
        }
    }
};

export function textChatTest() {
    const url = 'ws://localhost:8080/chat/stream';
    const response = ws.connect(url, {}, function (socket) {
        socket.on('open', () => {
            wsConnections.add(1);
            
            socket.send(JSON.stringify({
                type: 'text_message',
                data: { message: 'Hello, how are you?' },
                meta: { session_id: `text_${__VU}_${__ITER}` }
            }));
        });
        
        socket.on('message', (data) => {
            const message = JSON.parse(data);
            if (message.type === 'response_complete') {
                const latency = Date.now() - message.meta.start_time;
                e2eLatency.add(latency);
            }
        });
        
        socket.setTimeout(() => {
            socket.close();
        }, 30000);
    });
    
    check(response, { 'status is 101': (r) => r && r.status === 101 });
}

export function voiceChatTest() {
    const url = 'ws://localhost:8080/voice/stream';
    
    // åŠ è½½æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    const audioData = open('./test_audio.pcm', 'b');
    
    const response = ws.connect(url, {}, function (socket) {
        socket.on('open', () => {
            wsConnections.add(1);
            
            // åˆ†å¸§å‘é€éŸ³é¢‘æ•°æ®
            const frameSize = 640; // 20ms @ 16kHz
            for (let i = 0; i < audioData.length; i += frameSize) {
                const frame = audioData.slice(i, i + frameSize);
                
                // 3% æ¦‚ç‡æ³¨å…¥ä¹±åº
                if (Math.random() < 0.03) {
                    setTimeout(() => socket.send(frame), 50);
                } else {
                    socket.send(frame);
                }
                
                // æ¨¡æ‹Ÿå®æ—¶å‘é€é—´éš”
                sleep(0.02); // 20ms
            }
        });
        
        socket.on('message', (data) => {
            // å¤„ç†TTSå“åº”
            const message = JSON.parse(data);
            if (message.type === 'tts_end') {
                const latency = Date.now() - message.meta.start_time;
                e2eLatency.add(latency);
            }
        });
    });
    
    check(response, { 'status is 101': (r) => r && r.status === 101 });
}
```

#### 12. Playwright E2Eæµ‹è¯•
```typescript
// tests/e2e/voice_interaction.spec.ts
import { test, expect } from '@playwright/test';

test.describe('è¯­éŸ³äº¤äº’E2Eæµ‹è¯•', () => {
    test('å®Œæ•´è¯­éŸ³å¯¹è¯æµç¨‹', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // ç­‰å¾…é¡µé¢åŠ è½½
        await expect(page.locator('#voice-chat-container')).toBeVisible();
        
        // ä¸Šä¼ æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
        const audioInput = page.locator('#audio-file-input');
        await audioInput.setInputFiles('./test-data/hello.wav');
        
        // ç‚¹å‡»å¼€å§‹å½•éŸ³æŒ‰é’®
        await page.click('#start-recording');
        
        // éªŒè¯å½•éŸ³çŠ¶æ€
        await expect(page.locator('#recording-status')).toContainText('å½•éŸ³ä¸­');
        
        // ç­‰å¾…ASRç»“æœ
        await expect(page.locator('#asr-result')).toContainText('ä½ å¥½', { timeout: 5000 });
        
        // éªŒè¯LLMå“åº”
        await expect(page.locator('#llm-response')).toBeVisible({ timeout: 10000 });
        
        // éªŒè¯TTSæ’­æ”¾
        await expect(page.locator('#tts-player')).toHaveAttribute('playing', 'true');
        
        // æ£€æŸ¥ç«¯åˆ°ç«¯å»¶è¿Ÿ
        const latencyText = await page.locator('#e2e-latency').textContent();
        const latency = parseInt(latencyText || '0');
        expect(latency).toBeLessThan(500);
    });
    
    test('æ‰“æ–­åŠŸèƒ½æµ‹è¯•', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // å¼€å§‹å¯¹è¯
        await page.click('#start-conversation');
        
        // ç­‰å¾…TTSå¼€å§‹æ’­æ”¾
        await expect(page.locator('#tts-status')).toContainText('æ’­æ”¾ä¸­');
        
        // æ¨¡æ‹Ÿç”¨æˆ·æ‰“æ–­
        const interruptStart = Date.now();
        await page.click('#interrupt-button');
        
        // éªŒè¯æ‰“æ–­å“åº”
        await expect(page.locator('#tts-status')).toContainText('å·²å–æ¶ˆ', { timeout: 200 });
        
        const interruptLatency = Date.now() - interruptStart;
        expect(interruptLatency).toBeLessThan(120);
    });
    
    test('ç½‘ç»œå¼‚å¸¸æ¢å¤æµ‹è¯•', async ({ page }) => {
        await page.goto('/voice-chat');
        
        // å»ºç«‹è¿æ¥
        await page.click('#connect-button');
        await expect(page.locator('#connection-status')).toContainText('å·²è¿æ¥');
        
        // æ¨¡æ‹Ÿç½‘ç»œæ–­å¼€
        await page.route('**/voice/stream', route => route.abort());
        
        // éªŒè¯æ–­çº¿æ£€æµ‹
        await expect(page.locator('#connection-status')).toContainText('é‡è¿ä¸­', { timeout: 5000 });
        
        // æ¢å¤ç½‘ç»œ
        await page.unroute('**/voice/stream');
        
        // éªŒè¯è‡ªåŠ¨é‡è¿
        await expect(page.locator('#connection-status')).toContainText('å·²è¿æ¥', { timeout: 10000 });
    });
});
```

## ğŸ”§ å¿«é€ŸéªŒè¯å‘½ä»¤

### æœ¬åœ°å¼€å‘ç¯å¢ƒå¯åŠ¨
```bash
# 1. å¯åŠ¨æ‰€æœ‰æœåŠ¡
make dev

# 2. è¿è¡Œå•å…ƒæµ‹è¯•
make test

# 3. è¿è¡ŒE2Eæµ‹è¯•
make test-e2e

# 4. è¿è¡Œå‹æµ‹
make load-test

# 5. æŸ¥çœ‹ç›‘æ§é¢æ¿
open http://localhost:3000/grafana
```

### æ€§èƒ½éªŒè¯è„šæœ¬
```bash
#!/bin/bash
# scripts/validate_performance.sh

echo "å¼€å§‹æ€§èƒ½éªŒè¯..."

# 1. æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
curl -f http://localhost:8080/health || exit 1

# 2. è¿è¡Œå»¶è¿Ÿæµ‹è¯•
echo "æµ‹è¯•ç«¯åˆ°ç«¯å»¶è¿Ÿ..."
LATENCY=$(curl -s http://localhost:8080/metrics | grep voice_latency | tail -1)
echo "å½“å‰å»¶è¿Ÿ: $LATENCY"

# 3. æ£€æŸ¥éŸ³é¢‘è´¨é‡æŒ‡æ ‡
echo "æ£€æŸ¥éŸ³é¢‘è´¨é‡..."
OOO_FRAMES=$(curl -s http://localhost:8080/metrics | grep audio_ooo_frames)
DROP_FRAMES=$(curl -s http://localhost:8080/metrics | grep audio_dropped_frames)
echo "ä¹±åºå¸§: $OOO_FRAMES"
echo "ä¸¢å¤±å¸§: $DROP_FRAMES"

# 4. éªŒè¯æ‰“æ–­åŠŸèƒ½
echo "æµ‹è¯•æ‰“æ–­åŠŸèƒ½..."
BARGE_IN=$(curl -s http://localhost:8080/metrics | grep barge_in_success)
echo "æ‰“æ–­æˆåŠŸç‡: $BARGE_IN"

echo "æ€§èƒ½éªŒè¯å®Œæˆï¼"
```

## ğŸ“Š å…³é”®æŒ‡æ ‡ç›‘æ§

### GrafanaæŸ¥è¯¢è¯­å¥
```promql
# ç«¯åˆ°ç«¯å»¶è¿ŸP95
histogram_quantile(0.95, rate(voice_latency_seconds_bucket{stage="e2e"}[5m]))

# äº”æ®µå»¶è¿Ÿåˆ†è§£
rate(voice_latency_seconds_sum{stage="capture"}[5m]) / rate(voice_latency_seconds_count{stage="capture"}[5m])
rate(voice_latency_seconds_sum{stage="asr"}[5m]) / rate(voice_latency_seconds_count{stage="asr"}[5m])
rate(voice_latency_seconds_sum{stage="llm"}[5m]) / rate(voice_latency_seconds_count{stage="llm"}[5m])
rate(voice_latency_seconds_sum{stage="tts"}[5m]) / rate(voice_latency_seconds_count{stage="tts"}[5m])
rate(voice_latency_seconds_sum{stage="play"}[5m]) / rate(voice_latency_seconds_count{stage="play"}[5m])

# éŸ³é¢‘è´¨é‡å¥åº·åº¦
rate(audio_ooo_frames_total[5m])
rate(audio_dropped_frames_total[5m])
histogram_quantile(0.95, rate(audio_jitter_seconds_bucket[5m]))

# æ‰“æ–­æˆåŠŸç‡
rate(barge_in_total{result="success"}[5m]) / rate(barge_in_total[5m])
```

---

## âš¡ ç«‹å³å¼€å§‹

1. **åˆ›å»ºåˆ†æ”¯**: `git checkout -b v2-voice-optimization`
2. **è®¾ç½®ç¯å¢ƒ**: `cp env.example .env.local`
3. **å¯åŠ¨æœåŠ¡**: `make dev`
4. **å¼€å§‹ç¬¬ä¸€ä¸ªä»»åŠ¡**: å®ç°AudioWorklet MicProcessor

è®©æˆ‘ä»¬ç”¨48å°æ—¶è¯æ˜è¯­éŸ³ä¼˜åŒ–çš„å¨åŠ›ï¼ğŸš€

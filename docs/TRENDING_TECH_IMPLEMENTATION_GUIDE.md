# GitHubçƒ­é—¨æŠ€æœ¯å®ç°æŒ‡å—

## ğŸš€ åŸºäºæœ€æ–°å¼€æºé¡¹ç›®çš„æŠ€æœ¯å®ç°æ–¹æ¡ˆ

æœ¬æ–‡æ¡£æä¾›äº†åŸºäºGitHubæœ€æ–°çƒ­é—¨é¡¹ç›®çš„å…·ä½“æŠ€æœ¯å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»£ç ç¤ºä¾‹ã€æ¶æ„è®¾è®¡å’Œæœ€ä½³å®è·µã€‚

---

## 1. OpenAI Realtime API é›†æˆå®ç°

### 1.1 WebSocketè¿æ¥ç®¡ç†
**å‚è€ƒé¡¹ç›®**: OpenAIå®˜æ–¹ç¤ºä¾‹ã€realtime-api-examples

```typescript
// frontend/lib/realtime-client.ts
export class RealtimeVoiceClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaRecorder: MediaRecorder | null = null;
  private audioQueue: Float32Array[] = [];

  async connect(apiKey: string) {
    const url = `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01`;
    
    this.ws = new WebSocket(url, [], {
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'OpenAI-Beta': 'realtime=v1'
      }
    });

    this.ws.onopen = () => {
      console.log('Connected to OpenAI Realtime API');
      this.initializeSession();
    };

    this.ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      this.handleRealtimeMessage(message);
    };

    this.ws.onerror = (error) => {
      console.error('WebSocket error:', error);
    };
  }

  private initializeSession() {
    const sessionConfig = {
      type: 'session.update',
      session: {
        modalities: ['text', 'audio'],
        instructions: 'You are a helpful AI assistant. Respond naturally and conversationally.',
        voice: 'alloy',
        input_audio_format: 'pcm16',
        output_audio_format: 'pcm16',
        input_audio_transcription: {
          model: 'whisper-1'
        },
        turn_detection: {
          type: 'server_vad',
          threshold: 0.5,
          prefix_padding_ms: 300,
          silence_duration_ms: 200
        }
      }
    };

    this.ws?.send(JSON.stringify(sessionConfig));
  }

  private handleRealtimeMessage(message: any) {
    switch (message.type) {
      case 'session.created':
        console.log('Session created:', message.session);
        break;
      
      case 'conversation.item.created':
        console.log('Item created:', message.item);
        break;
      
      case 'response.audio.delta':
        this.handleAudioDelta(message.delta);
        break;
      
      case 'response.text.delta':
        this.handleTextDelta(message.delta);
        break;
      
      case 'input_audio_buffer.speech_started':
        console.log('Speech started');
        this.onSpeechStart?.();
        break;
      
      case 'input_audio_buffer.speech_stopped':
        console.log('Speech stopped');
        this.onSpeechStop?.();
        break;
    }
  }

  async startRecording() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });

      this.audioContext = new AudioContext({ sampleRate: 24000 });
      const source = this.audioContext.createMediaStreamSource(stream);
      
      // ä½¿ç”¨AudioWorkletè¿›è¡Œå®æ—¶éŸ³é¢‘å¤„ç†
      await this.audioContext.audioWorklet.addModule('/audio-processor.js');
      const processor = new AudioWorkletNode(this.audioContext, 'audio-processor');
      
      processor.port.onmessage = (event) => {
        const audioData = event.data;
        this.sendAudioData(audioData);
      };

      source.connect(processor);
      processor.connect(this.audioContext.destination);

    } catch (error) {
      console.error('Error starting recording:', error);
    }
  }

  private sendAudioData(audioData: Float32Array) {
    if (this.ws?.readyState === WebSocket.OPEN) {
      // è½¬æ¢ä¸ºPCM16æ ¼å¼
      const pcm16 = this.floatToPCM16(audioData);
      const base64Audio = this.arrayBufferToBase64(pcm16);
      
      const message = {
        type: 'input_audio_buffer.append',
        audio: base64Audio
      };
      
      this.ws.send(JSON.stringify(message));
    }
  }

  private floatToPCM16(float32Array: Float32Array): ArrayBuffer {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    
    for (let i = 0; i < float32Array.length; i++) {
      const sample = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(i * 2, sample * 0x7FFF, true);
    }
    
    return buffer;
  }

  private arrayBufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }
}
```

### 1.2 éŸ³é¢‘å¤„ç†WorkLet
```javascript
// public/audio-processor.js
class AudioProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.bufferSize = 1024;
    this.buffer = new Float32Array(this.bufferSize);
    this.bufferIndex = 0;
  }

  process(inputs, outputs, parameters) {
    const input = inputs[0];
    const channel = input[0];

    if (channel) {
      for (let i = 0; i < channel.length; i++) {
        this.buffer[this.bufferIndex] = channel[i];
        this.bufferIndex++;

        if (this.bufferIndex >= this.bufferSize) {
          // å‘é€éŸ³é¢‘æ•°æ®åˆ°ä¸»çº¿ç¨‹
          this.port.postMessage(this.buffer.slice());
          this.bufferIndex = 0;
        }
      }
    }

    return true;
  }
}

registerProcessor('audio-processor', AudioProcessor);
```

---

## 2. å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿå®ç°

### 2.1 è§†è§‰ç†è§£é›†æˆ
**å‚è€ƒé¡¹ç›®**: GPT-4V, Claude-3, LLaVA

```python
# algo/core/multimodal_processor.py
import base64
import asyncio
from typing import Optional, Dict, Any, List
from PIL import Image
import io

class MultimodalProcessor:
    def __init__(self):
        self.vision_models = {
            'gpt-4v': self._init_gpt4v(),
            'claude-3': self._init_claude3(),
            'gemini-vision': self._init_gemini_vision()
        }
        self.current_model = 'gpt-4v'

    async def process_image_with_text(self, 
                                    image_data: bytes, 
                                    text_prompt: str,
                                    model: str = None) -> str:
        """å¤„ç†å›¾åƒå’Œæ–‡æœ¬çš„ç»„åˆè¾“å…¥"""
        model_name = model or self.current_model
        
        # å›¾åƒé¢„å¤„ç†
        processed_image = await self._preprocess_image(image_data)
        
        # æ„å»ºå¤šæ¨¡æ€è¯·æ±‚
        request = {
            'model': model_name,
            'messages': [
                {
                    'role': 'user',
                    'content': [
                        {
                            'type': 'text',
                            'text': text_prompt
                        },
                        {
                            'type': 'image_url',
                            'image_url': {
                                'url': f"data:image/jpeg;base64,{processed_image}"
                            }
                        }
                    ]
                }
            ],
            'max_tokens': 1000
        }
        
        return await self._call_vision_model(model_name, request)

    async def analyze_screen_content(self, screenshot: bytes) -> Dict[str, Any]:
        """åˆ†æå±å¹•å†…å®¹"""
        analysis_prompt = """
        åˆ†æè¿™ä¸ªå±å¹•æˆªå›¾ï¼Œè¯†åˆ«ï¼š
        1. ä¸»è¦UIå…ƒç´ å’Œå¸ƒå±€
        2. å¯äº¤äº’çš„æŒ‰é’®å’Œé“¾æ¥
        3. æ–‡æœ¬å†…å®¹æ‘˜è¦
        4. å¯èƒ½çš„ç”¨æˆ·æ“ä½œå»ºè®®
        
        ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
        """
        
        result = await self.process_image_with_text(screenshot, analysis_prompt)
        
        try:
            return json.loads(result)
        except json.JSONDecodeError:
            return {'error': 'Failed to parse analysis result', 'raw_result': result}

    async def _preprocess_image(self, image_data: bytes) -> str:
        """å›¾åƒé¢„å¤„ç†å’Œä¼˜åŒ–"""
        # æ‰“å¼€å›¾åƒ
        image = Image.open(io.BytesIO(image_data))
        
        # è°ƒæ•´å¤§å°ä»¥ä¼˜åŒ–å¤„ç†é€Ÿåº¦
        max_size = (1024, 1024)
        image.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # å‹ç¼©å¹¶è½¬æ¢ä¸ºbase64
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=85, optimize=True)
        
        return base64.b64encode(buffer.getvalue()).decode('utf-8')

    async def _call_vision_model(self, model_name: str, request: Dict) -> str:
        """è°ƒç”¨è§†è§‰æ¨¡å‹"""
        if model_name == 'gpt-4v':
            return await self._call_openai_vision(request)
        elif model_name == 'claude-3':
            return await self._call_claude_vision(request)
        elif model_name == 'gemini-vision':
            return await self._call_gemini_vision(request)
        else:
            raise ValueError(f"Unsupported model: {model_name}")

    async def _call_openai_vision(self, request: Dict) -> str:
        """è°ƒç”¨OpenAI GPT-4V"""
        import openai
        
        client = openai.AsyncOpenAI()
        response = await client.chat.completions.create(**request)
        
        return response.choices[0].message.content
```

### 2.2 å®æ—¶å±å¹•åˆ†æ
```python
# algo/core/screen_analyzer.py
import asyncio
import pyautogui
from typing import Dict, List, Tuple
import cv2
import numpy as np

class RealTimeScreenAnalyzer:
    def __init__(self, multimodal_processor):
        self.processor = multimodal_processor
        self.is_analyzing = False
        self.analysis_interval = 2.0  # æ¯2ç§’åˆ†æä¸€æ¬¡
        
    async def start_continuous_analysis(self):
        """å¼€å§‹æŒç»­çš„å±å¹•åˆ†æ"""
        self.is_analyzing = True
        
        while self.is_analyzing:
            try:
                # æˆªå–å±å¹•
                screenshot = pyautogui.screenshot()
                screenshot_bytes = self._pil_to_bytes(screenshot)
                
                # åˆ†æå±å¹•å†…å®¹
                analysis = await self.processor.analyze_screen_content(screenshot_bytes)
                
                # å¤„ç†åˆ†æç»“æœ
                await self._handle_screen_analysis(analysis)
                
                # ç­‰å¾…ä¸‹æ¬¡åˆ†æ
                await asyncio.sleep(self.analysis_interval)
                
            except Exception as e:
                print(f"Screen analysis error: {e}")
                await asyncio.sleep(1)

    async def _handle_screen_analysis(self, analysis: Dict):
        """å¤„ç†å±å¹•åˆ†æç»“æœ"""
        # æ£€æµ‹é‡è¦å˜åŒ–
        if self._detect_significant_change(analysis):
            # è§¦å‘ç›¸åº”çš„å¤„ç†é€»è¾‘
            await self._notify_screen_change(analysis)

    def _detect_significant_change(self, analysis: Dict) -> bool:
        """æ£€æµ‹å±å¹•æ˜¯å¦æœ‰é‡è¦å˜åŒ–"""
        # å®ç°å˜åŒ–æ£€æµ‹é€»è¾‘
        # æ¯”è¾ƒå½“å‰åˆ†æç»“æœä¸å†å²ç»“æœ
        return True  # ç®€åŒ–å®ç°

    async def _notify_screen_change(self, analysis: Dict):
        """é€šçŸ¥å±å¹•å˜åŒ–"""
        print(f"Screen changed: {analysis}")

    def _pil_to_bytes(self, pil_image) -> bytes:
        """å°†PILå›¾åƒè½¬æ¢ä¸ºå­—èŠ‚"""
        buffer = io.BytesIO()
        pil_image.save(buffer, format='PNG')
        return buffer.getvalue()
```

---

## 3. è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å®ç°

### 3.1 å®æ—¶æƒ…æ„Ÿåˆ†æ
**å‚è€ƒé¡¹ç›®**: SpeechEmotion, EmotiVoice

```python
# algo/core/emotion_recognition.py
import librosa
import numpy as np
import torch
import torch.nn as nn
from transformers import Wav2Vec2Processor, Wav2Vec2Model
from typing import Dict, Tuple

class VoiceEmotionRecognizer:
    def __init__(self):
        self.emotions = ['neutral', 'happy', 'sad', 'angry', 'fear', 'surprise', 'disgust']
        self.model = self._load_emotion_model()
        self.processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
        
    def _load_emotion_model(self):
        """åŠ è½½æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹"""
        class EmotionClassifier(nn.Module):
            def __init__(self, input_dim=768, num_emotions=7):
                super().__init__()
                self.wav2vec2 = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
                self.classifier = nn.Sequential(
                    nn.Linear(input_dim, 256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_emotions)
                )
                
            def forward(self, input_values):
                outputs = self.wav2vec2(input_values)
                # ä½¿ç”¨å¹³å‡æ± åŒ–
                pooled = torch.mean(outputs.last_hidden_state, dim=1)
                return self.classifier(pooled)
        
        model = EmotionClassifier()
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        # model.load_state_dict(torch.load('emotion_model.pth'))
        model.eval()
        return model

    async def recognize_emotion(self, audio_data: np.ndarray, sample_rate: int = 16000) -> Dict[str, float]:
        """è¯†åˆ«è¯­éŸ³æƒ…æ„Ÿ"""
        try:
            # é¢„å¤„ç†éŸ³é¢‘
            processed_audio = self._preprocess_audio(audio_data, sample_rate)
            
            # æå–ç‰¹å¾
            features = self._extract_features(processed_audio)
            
            # æƒ…æ„Ÿåˆ†ç±»
            emotion_probs = await self._classify_emotion(features)
            
            return {
                'emotions': dict(zip(self.emotions, emotion_probs)),
                'dominant_emotion': self.emotions[np.argmax(emotion_probs)],
                'confidence': float(np.max(emotion_probs))
            }
            
        except Exception as e:
            print(f"Emotion recognition error: {e}")
            return {
                'emotions': {emotion: 0.0 for emotion in self.emotions},
                'dominant_emotion': 'neutral',
                'confidence': 0.0
            }

    def _preprocess_audio(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """éŸ³é¢‘é¢„å¤„ç†"""
        # é‡é‡‡æ ·åˆ°16kHz
        if sample_rate != 16000:
            audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
        
        # æ ‡å‡†åŒ–
        audio_data = librosa.util.normalize(audio_data)
        
        # å»é™¤é™éŸ³
        audio_data, _ = librosa.effects.trim(audio_data, top_db=20)
        
        return audio_data

    def _extract_features(self, audio_data: np.ndarray) -> torch.Tensor:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # ä½¿ç”¨Wav2Vec2å¤„ç†å™¨
        inputs = self.processor(audio_data, sampling_rate=16000, return_tensors="pt")
        return inputs.input_values

    async def _classify_emotion(self, features: torch.Tensor) -> np.ndarray:
        """æƒ…æ„Ÿåˆ†ç±»"""
        with torch.no_grad():
            outputs = self.model(features)
            probabilities = torch.softmax(outputs, dim=-1)
            return probabilities.numpy().flatten()

    def get_emotion_intensity(self, audio_data: np.ndarray) -> float:
        """è·å–æƒ…æ„Ÿå¼ºåº¦"""
        # è®¡ç®—éŸ³é¢‘çš„èƒ½é‡å’Œé¢‘è°±ç‰¹å¾
        energy = np.sum(audio_data ** 2)
        spectral_centroid = librosa.feature.spectral_centroid(y=audio_data, sr=16000)[0]
        
        # ç®€å•çš„å¼ºåº¦è®¡ç®—
        intensity = min(1.0, energy * np.mean(spectral_centroid) / 1000)
        return float(intensity)
```

### 3.2 æƒ…æ„ŸåŒ–TTSå®ç°
```python
# algo/core/emotional_tts.py
import asyncio
from typing import Dict, Optional
import torch
import numpy as np

class EmotionalTTSEngine:
    def __init__(self):
        self.base_tts = self._init_base_tts()
        self.emotion_adapters = self._load_emotion_adapters()
        
    async def synthesize_with_emotion(self, 
                                    text: str, 
                                    emotion: str = 'neutral',
                                    intensity: float = 0.5,
                                    voice_id: str = 'default') -> bytes:
        """åˆæˆå¸¦æƒ…æ„Ÿçš„è¯­éŸ³"""
        
        # æ ¹æ®æƒ…æ„Ÿè°ƒæ•´TTSå‚æ•°
        tts_params = self._get_emotion_params(emotion, intensity)
        
        # ç”ŸæˆåŸºç¡€è¯­éŸ³
        base_audio = await self._generate_base_audio(text, voice_id, tts_params)
        
        # åº”ç”¨æƒ…æ„Ÿè°ƒåˆ¶
        emotional_audio = self._apply_emotion_modulation(base_audio, emotion, intensity)
        
        return emotional_audio

    def _get_emotion_params(self, emotion: str, intensity: float) -> Dict:
        """è·å–æƒ…æ„Ÿå¯¹åº”çš„TTSå‚æ•°"""
        emotion_configs = {
            'happy': {
                'pitch_shift': 0.1 * intensity,
                'speed_rate': 1.0 + 0.2 * intensity,
                'volume_boost': 0.1 * intensity
            },
            'sad': {
                'pitch_shift': -0.15 * intensity,
                'speed_rate': 1.0 - 0.3 * intensity,
                'volume_boost': -0.2 * intensity
            },
            'angry': {
                'pitch_shift': 0.05 * intensity,
                'speed_rate': 1.0 + 0.1 * intensity,
                'volume_boost': 0.3 * intensity,
                'roughness': 0.2 * intensity
            },
            'fear': {
                'pitch_shift': 0.2 * intensity,
                'speed_rate': 1.0 + 0.4 * intensity,
                'tremolo': 0.3 * intensity
            },
            'neutral': {
                'pitch_shift': 0.0,
                'speed_rate': 1.0,
                'volume_boost': 0.0
            }
        }
        
        return emotion_configs.get(emotion, emotion_configs['neutral'])

    async def _generate_base_audio(self, text: str, voice_id: str, params: Dict) -> np.ndarray:
        """ç”ŸæˆåŸºç¡€éŸ³é¢‘"""
        # è¿™é‡Œé›†æˆå®é™…çš„TTSå¼•æ“
        # ä¾‹å¦‚ï¼šOpenAI TTS, ElevenLabs, Azure Speechç­‰
        
        # ç¤ºä¾‹å®ç°ï¼ˆéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„TTSè°ƒç”¨ï¼‰
        import requests
        
        tts_request = {
            'text': text,
            'voice': voice_id,
            'model': 'tts-1-hd',
            'response_format': 'wav',
            'speed': params.get('speed_rate', 1.0)
        }
        
        # è°ƒç”¨TTS API
        # response = await self._call_tts_api(tts_request)
        # return self._parse_audio_response(response)
        
        # ä¸´æ—¶è¿”å›ç©ºæ•°ç»„
        return np.zeros(16000)  # 1ç§’çš„é™éŸ³

    def _apply_emotion_modulation(self, audio: np.ndarray, emotion: str, intensity: float) -> bytes:
        """åº”ç”¨æƒ…æ„Ÿè°ƒåˆ¶"""
        import librosa
        
        # æ ¹æ®æƒ…æ„Ÿå‚æ•°è°ƒåˆ¶éŸ³é¢‘
        params = self._get_emotion_params(emotion, intensity)
        
        # éŸ³è°ƒè°ƒåˆ¶
        if params.get('pitch_shift', 0) != 0:
            audio = librosa.effects.pitch_shift(
                audio, sr=16000, n_steps=params['pitch_shift'] * 12
            )
        
        # é€Ÿåº¦è°ƒåˆ¶
        if params.get('speed_rate', 1.0) != 1.0:
            audio = librosa.effects.time_stretch(audio, rate=params['speed_rate'])
        
        # éŸ³é‡è°ƒåˆ¶
        if params.get('volume_boost', 0) != 0:
            audio = audio * (1.0 + params['volume_boost'])
        
        # æ·»åŠ é¢¤éŸ³æ•ˆæœï¼ˆææƒ§æƒ…æ„Ÿï¼‰
        if params.get('tremolo', 0) > 0:
            tremolo_freq = 5.0  # 5Hzé¢¤éŸ³
            t = np.linspace(0, len(audio)/16000, len(audio))
            tremolo = 1 + params['tremolo'] * np.sin(2 * np.pi * tremolo_freq * t)
            audio = audio * tremolo
        
        # è½¬æ¢ä¸ºå­—èŠ‚æ ¼å¼
        audio_int16 = (audio * 32767).astype(np.int16)
        return audio_int16.tobytes()
```

---

## 4. å¯è§†åŒ–å¯¹è¯è®¾è®¡å™¨

### 4.1 å¯¹è¯æµç¼–è¾‘å™¨
**å‚è€ƒé¡¹ç›®**: Botpress, Rasa X, Microsoft Bot Framework

```typescript
// frontend/components/DialogFlowEditor.tsx
import React, { useState, useCallback } from 'react';
import ReactFlow, {
  Node,
  Edge,
  addEdge,
  Connection,
  useNodesState,
  useEdgesState,
  Controls,
  Background,
} from 'reactflow';

interface DialogNode extends Node {
  data: {
    type: 'intent' | 'response' | 'condition' | 'action';
    content: string;
    conditions?: string[];
    responses?: string[];
  };
}

export const DialogFlowEditor: React.FC = () => {
  const [nodes, setNodes, onNodesChange] = useNodesState<DialogNode>([]);
  const [edges, setEdges, onEdgesChange] = useEdgesState([]);
  const [selectedNode, setSelectedNode] = useState<DialogNode | null>(null);

  const onConnect = useCallback(
    (params: Connection) => setEdges((eds) => addEdge(params, eds)),
    [setEdges]
  );

  const addNode = (type: DialogNode['data']['type']) => {
    const newNode: DialogNode = {
      id: `${type}-${Date.now()}`,
      type: 'default',
      position: { x: Math.random() * 400, y: Math.random() * 400 },
      data: {
        type,
        content: `New ${type}`,
      },
    };

    setNodes((nds) => [...nds, newNode]);
  };

  const updateNodeContent = (nodeId: string, content: string) => {
    setNodes((nds) =>
      nds.map((node) =>
        node.id === nodeId
          ? { ...node, data: { ...node.data, content } }
          : node
      )
    );
  };

  const exportDialogFlow = () => {
    const dialogFlow = {
      nodes: nodes.map(node => ({
        id: node.id,
        type: node.data.type,
        content: node.data.content,
        position: node.position,
        conditions: node.data.conditions || [],
        responses: node.data.responses || []
      })),
      edges: edges.map(edge => ({
        id: edge.id,
        source: edge.source,
        target: edge.target,
        conditions: edge.data?.conditions || []
      }))
    };

    return JSON.stringify(dialogFlow, null, 2);
  };

  return (
    <div className="h-screen flex">
      {/* å·¥å…·æ  */}
      <div className="w-64 bg-gray-100 p-4">
        <h3 className="font-bold mb-4">èŠ‚ç‚¹ç±»å‹</h3>
        <div className="space-y-2">
          <button
            onClick={() => addNode('intent')}
            className="w-full p-2 bg-blue-500 text-white rounded hover:bg-blue-600"
          >
            æ·»åŠ æ„å›¾èŠ‚ç‚¹
          </button>
          <button
            onClick={() => addNode('response')}
            className="w-full p-2 bg-green-500 text-white rounded hover:bg-green-600"
          >
            æ·»åŠ å“åº”èŠ‚ç‚¹
          </button>
          <button
            onClick={() => addNode('condition')}
            className="w-full p-2 bg-yellow-500 text-white rounded hover:bg-yellow-600"
          >
            æ·»åŠ æ¡ä»¶èŠ‚ç‚¹
          </button>
          <button
            onClick={() => addNode('action')}
            className="w-full p-2 bg-purple-500 text-white rounded hover:bg-purple-600"
          >
            æ·»åŠ åŠ¨ä½œèŠ‚ç‚¹
          </button>
        </div>

        <div className="mt-8">
          <h3 className="font-bold mb-4">æ“ä½œ</h3>
          <button
            onClick={() => {
              const flow = exportDialogFlow();
              console.log('Exported flow:', flow);
              // è¿™é‡Œå¯ä»¥ä¿å­˜åˆ°åç«¯
            }}
            className="w-full p-2 bg-gray-600 text-white rounded hover:bg-gray-700"
          >
            å¯¼å‡ºå¯¹è¯æµ
          </button>
        </div>
      </div>

      {/* æµç¨‹å›¾ç¼–è¾‘åŒºåŸŸ */}
      <div className="flex-1">
        <ReactFlow
          nodes={nodes}
          edges={edges}
          onNodesChange={onNodesChange}
          onEdgesChange={onEdgesChange}
          onConnect={onConnect}
          onNodeClick={(event, node) => setSelectedNode(node as DialogNode)}
        >
          <Controls />
          <Background />
        </ReactFlow>
      </div>

      {/* å±æ€§ç¼–è¾‘é¢æ¿ */}
      {selectedNode && (
        <div className="w-80 bg-white border-l p-4">
          <h3 className="font-bold mb-4">èŠ‚ç‚¹å±æ€§</h3>
          <div className="space-y-4">
            <div>
              <label className="block text-sm font-medium mb-1">ç±»å‹</label>
              <input
                type="text"
                value={selectedNode.data.type}
                disabled
                className="w-full p-2 border rounded bg-gray-100"
              />
            </div>
            
            <div>
              <label className="block text-sm font-medium mb-1">å†…å®¹</label>
              <textarea
                value={selectedNode.data.content}
                onChange={(e) => updateNodeContent(selectedNode.id, e.target.value)}
                className="w-full p-2 border rounded h-32"
                placeholder="è¾“å…¥èŠ‚ç‚¹å†…å®¹..."
              />
            </div>

            {selectedNode.data.type === 'condition' && (
              <div>
                <label className="block text-sm font-medium mb-1">æ¡ä»¶</label>
                <textarea
                  value={selectedNode.data.conditions?.join('\n') || ''}
                  onChange={(e) => {
                    const conditions = e.target.value.split('\n').filter(c => c.trim());
                    setNodes((nds) =>
                      nds.map((node) =>
                        node.id === selectedNode.id
                          ? { ...node, data: { ...node.data, conditions } }
                          : node
                      )
                    );
                  }}
                  className="w-full p-2 border rounded h-24"
                  placeholder="æ¯è¡Œä¸€ä¸ªæ¡ä»¶..."
                />
              </div>
            )}

            {selectedNode.data.type === 'response' && (
              <div>
                <label className="block text-sm font-medium mb-1">å“åº”é€‰é¡¹</label>
                <textarea
                  value={selectedNode.data.responses?.join('\n') || ''}
                  onChange={(e) => {
                    const responses = e.target.value.split('\n').filter(r => r.trim());
                    setNodes((nds) =>
                      nds.map((node) =>
                        node.id === selectedNode.id
                          ? { ...node, data: { ...node.data, responses } }
                          : node
                      )
                    );
                  }}
                  className="w-full p-2 border rounded h-24"
                  placeholder="æ¯è¡Œä¸€ä¸ªå“åº”é€‰é¡¹..."
                />
              </div>
            )}
          </div>
        </div>
      )}
    </div>
  );
};
```

### 4.2 å¯¹è¯æµæ‰§è¡Œå¼•æ“
```python
# backend/internal/dialog/flow_engine.py
from typing import Dict, List, Any, Optional
import json
import re
from dataclasses import dataclass
from enum import Enum

class NodeType(Enum):
    INTENT = "intent"
    RESPONSE = "response"
    CONDITION = "condition"
    ACTION = "action"

@dataclass
class DialogNode:
    id: str
    type: NodeType
    content: str
    conditions: List[str] = None
    responses: List[str] = None
    position: Dict[str, float] = None

@dataclass
class DialogEdge:
    id: str
    source: str
    target: str
    conditions: List[str] = None

class DialogFlowEngine:
    def __init__(self):
        self.flows: Dict[str, Dict] = {}
        self.current_sessions: Dict[str, Dict] = {}
        
    def load_flow(self, flow_id: str, flow_data: Dict) -> bool:
        """åŠ è½½å¯¹è¯æµ"""
        try:
            # éªŒè¯æµç¨‹æ•°æ®
            if not self._validate_flow(flow_data):
                return False
                
            # è§£æèŠ‚ç‚¹å’Œè¾¹
            nodes = {
                node['id']: DialogNode(
                    id=node['id'],
                    type=NodeType(node['type']),
                    content=node['content'],
                    conditions=node.get('conditions', []),
                    responses=node.get('responses', []),
                    position=node.get('position', {})
                )
                for node in flow_data['nodes']
            }
            
            edges = [
                DialogEdge(
                    id=edge['id'],
                    source=edge['source'],
                    target=edge['target'],
                    conditions=edge.get('conditions', [])
                )
                for edge in flow_data['edges']
            ]
            
            self.flows[flow_id] = {
                'nodes': nodes,
                'edges': edges,
                'entry_points': self._find_entry_points(nodes, edges)
            }
            
            return True
            
        except Exception as e:
            print(f"Error loading flow {flow_id}: {e}")
            return False

    async def process_message(self, 
                            session_id: str, 
                            flow_id: str, 
                            message: str,
                            context: Dict = None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        
        if flow_id not in self.flows:
            return {'error': f'Flow {flow_id} not found'}
            
        # è·å–æˆ–åˆ›å»ºä¼šè¯çŠ¶æ€
        session = self.current_sessions.get(session_id, {
            'current_node': None,
            'context': context or {},
            'history': []
        })
        
        flow = self.flows[flow_id]
        
        # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œä»å…¥å£ç‚¹å¼€å§‹
        if session['current_node'] is None:
            entry_nodes = flow['entry_points']
            if not entry_nodes:
                return {'error': 'No entry point found in flow'}
            session['current_node'] = entry_nodes[0]
        
        # å¤„ç†å½“å‰èŠ‚ç‚¹
        current_node = flow['nodes'][session['current_node']]
        response = await self._process_node(current_node, message, session, flow)
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session['history'].append({
            'user_message': message,
            'bot_response': response,
            'node_id': current_node.id,
            'timestamp': time.time()
        })
        
        self.current_sessions[session_id] = session
        
        return response

    async def _process_node(self, 
                          node: DialogNode, 
                          message: str, 
                          session: Dict, 
                          flow: Dict) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªèŠ‚ç‚¹"""
        
        if node.type == NodeType.INTENT:
            return await self._process_intent_node(node, message, session, flow)
        elif node.type == NodeType.RESPONSE:
            return await self._process_response_node(node, message, session, flow)
        elif node.type == NodeType.CONDITION:
            return await self._process_condition_node(node, message, session, flow)
        elif node.type == NodeType.ACTION:
            return await self._process_action_node(node, message, session, flow)
        else:
            return {'error': f'Unknown node type: {node.type}'}

    async def _process_intent_node(self, 
                                 node: DialogNode, 
                                 message: str, 
                                 session: Dict, 
                                 flow: Dict) -> Dict[str, Any]:
        """å¤„ç†æ„å›¾èŠ‚ç‚¹"""
        # æ„å›¾è¯†åˆ«é€»è¾‘
        intent_confidence = await self._classify_intent(message, node.content)
        
        if intent_confidence > 0.7:  # æ„å›¾åŒ¹é…é˜ˆå€¼
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            next_node_id = self._find_next_node(node.id, flow['edges'])
            if next_node_id:
                session['current_node'] = next_node_id
                next_node = flow['nodes'][next_node_id]
                return await self._process_node(next_node, message, session, flow)
        
        return {
            'type': 'intent_not_matched',
            'message': 'æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„æ€ï¼Œè¯·é‡æ–°è¡¨è¾¾ã€‚',
            'confidence': intent_confidence
        }

    async def _process_response_node(self, 
                                   node: DialogNode, 
                                   message: str, 
                                   session: Dict, 
                                   flow: Dict) -> Dict[str, Any]:
        """å¤„ç†å“åº”èŠ‚ç‚¹"""
        # é€‰æ‹©å“åº”
        if node.responses:
            # å¯ä»¥å®ç°æ™ºèƒ½å“åº”é€‰æ‹©é€»è¾‘
            response_text = self._select_response(node.responses, session['context'])
        else:
            response_text = node.content
        
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        next_node_id = self._find_next_node(node.id, flow['edges'])
        if next_node_id:
            session['current_node'] = next_node_id
        
        return {
            'type': 'response',
            'message': response_text,
            'has_next': next_node_id is not None
        }

    async def _classify_intent(self, message: str, intent_pattern: str) -> float:
        """æ„å›¾åˆ†ç±»"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…å®ç°
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„NLUæ¨¡å‹
        keywords = intent_pattern.lower().split()
        message_lower = message.lower()
        
        matches = sum(1 for keyword in keywords if keyword in message_lower)
        return matches / len(keywords) if keywords else 0.0

    def _find_next_node(self, current_node_id: str, edges: List[DialogEdge]) -> Optional[str]:
        """æŸ¥æ‰¾ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        for edge in edges:
            if edge.source == current_node_id:
                # æ£€æŸ¥è¾¹çš„æ¡ä»¶
                if not edge.conditions or self._evaluate_conditions(edge.conditions):
                    return edge.target
        return None

    def _evaluate_conditions(self, conditions: List[str]) -> bool:
        """è¯„ä¼°æ¡ä»¶"""
        # ç®€å•çš„æ¡ä»¶è¯„ä¼°å®ç°
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æ¡ä»¶è¯„ä¼°é€»è¾‘
        return True

    def _select_response(self, responses: List[str], context: Dict) -> str:
        """é€‰æ‹©å“åº”"""
        # å¯ä»¥åŸºäºä¸Šä¸‹æ–‡é€‰æ‹©æœ€åˆé€‚çš„å“åº”
        # è¿™é‡Œç®€å•è¿”å›ç¬¬ä¸€ä¸ªå“åº”
        return responses[0] if responses else ""

    def _find_entry_points(self, nodes: Dict[str, DialogNode], edges: List[DialogEdge]) -> List[str]:
        """æŸ¥æ‰¾å…¥å£ç‚¹ï¼ˆæ²¡æœ‰è¾“å…¥è¾¹çš„èŠ‚ç‚¹ï¼‰"""
        target_nodes = {edge.target for edge in edges}
        entry_points = [node_id for node_id in nodes.keys() if node_id not in target_nodes]
        return entry_points

    def _validate_flow(self, flow_data: Dict) -> bool:
        """éªŒè¯æµç¨‹æ•°æ®"""
        required_fields = ['nodes', 'edges']
        return all(field in flow_data for field in required_fields)
```

---

## 5. éƒ¨ç½²å’Œé›†æˆæŒ‡å—

### 5.1 Dockerå®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
# Dockerfile.realtime
FROM node:18-alpine AS frontend-builder

WORKDIR /app/frontend
COPY frontend/package*.json ./
RUN npm ci --only=production

COPY frontend/ ./
RUN npm run build

FROM python:3.11-slim AS backend-builder

WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY algo/ ./algo/
COPY backend/ ./backend/

FROM python:3.11-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# å¤åˆ¶Pythonä¾èµ–
COPY --from=backend-builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=backend-builder /app ./

# å¤åˆ¶å‰ç«¯æ„å»ºç»“æœ
COPY --from=frontend-builder /app/frontend/dist ./frontend/dist

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV NODE_ENV=production

EXPOSE 8080 8000

# å¯åŠ¨è„šæœ¬
COPY scripts/start-realtime.sh ./
RUN chmod +x start-realtime.sh

CMD ["./start-realtime.sh"]
```

### 5.2 Kuberneteséƒ¨ç½²é…ç½®
```yaml
# k8s/realtime-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: voicehelper-realtime
  labels:
    app: voicehelper-realtime
spec:
  replicas: 3
  selector:
    matchLabels:
      app: voicehelper-realtime
  template:
    metadata:
      labels:
        app: voicehelper-realtime
    spec:
      containers:
      - name: voicehelper-realtime
        image: voicehelper/realtime:latest
        ports:
        - containerPort: 8080
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-secret
              key: api-key
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: voicehelper-realtime-service
spec:
  selector:
    app: voicehelper-realtime
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: websocket
    port: 8000
    targetPort: 8000
  type: LoadBalancer
```

è¿™ä¸ªå®ç°æŒ‡å—æä¾›äº†åŸºäºGitHubæœ€æ–°è¶‹åŠ¿çš„å…·ä½“æŠ€æœ¯å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **OpenAI Realtime APIé›†æˆ** - å®Œæ•´çš„WebSocketè¿æ¥å’ŒéŸ³é¢‘å¤„ç†
2. **å¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿ** - è§†è§‰ç†è§£å’Œå±å¹•åˆ†æ
3. **è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«** - å®æ—¶æƒ…æ„Ÿåˆ†æå’Œæƒ…æ„ŸåŒ–TTS
4. **å¯è§†åŒ–å¯¹è¯è®¾è®¡å™¨** - æ‹–æ‹½å¼å¯¹è¯æµç¼–è¾‘
5. **éƒ¨ç½²å’Œé›†æˆ** - å®¹å™¨åŒ–å’ŒKuberneteséƒ¨ç½²

æ¯ä¸ªæ¨¡å—éƒ½æä¾›äº†å®Œæ•´çš„ä»£ç ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼Œå¯ä»¥ç›´æ¥é›†æˆåˆ°VoiceHelperé¡¹ç›®ä¸­ã€‚

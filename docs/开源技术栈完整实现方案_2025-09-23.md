# VoiceHelper å¼€æºæŠ€æœ¯æ ˆå®Œæ•´å®ç°æ–¹æ¡ˆ

> åŸºäºä¸šç•Œæœ€ä½³å¼€æºæ–¹æ¡ˆçš„å®Œæ•´æŠ€æœ¯å®ç°æŒ‡å—  
> å‚è€ƒé¡¹ç›®ï¼šOpenAI Whisperã€Rasaã€FAISSã€Prometheusã€WebRTC  
> æ›´æ–°æ—¶é—´ï¼š2025å¹´9æœˆ23æ—¥

## ğŸ¯ æŠ€æœ¯å€ºåŠ¡ä¸å·®è·åˆ†æ

### ğŸ“Š å½“å‰çŠ¶æ€ vs ä¸šç•Œæ ‡å‡†å¯¹æ¯”

| åŠŸèƒ½æ¨¡å— | VoiceHelperç°çŠ¶ | ä¸šç•Œå¼€æºæ ‡å‡† | æŠ€æœ¯å·®è· | ä¿®å¤ä¼˜å…ˆçº§ |
|---------|----------------|-------------|----------|-----------|
| **å®æ—¶è¯­éŸ³è¯†åˆ«** | 30%å®Œæˆï¼Œæ ¸å¿ƒæ–‡ä»¶ç¼ºå¤± | OpenAI Whisper (å¤šè¯­è¨€ã€å®æ—¶) | ä¸¥é‡æ»å | P0 |
| **è¯­éŸ³åˆæˆ** | 40%å®Œæˆï¼Œé›†æˆä¸å®Œæ•´ | Edge-TTS/Coqui-TTS | åŠŸèƒ½ç¼ºå¤± | P0 |
| **å¯¹è¯ç®¡ç†** | 20%å®Œæˆï¼ŒçŠ¶æ€ç®¡ç†ç®€åŒ– | Rasa Core (å®Œæ•´NLU/DM) | æ¶æ„ç¼ºå¤± | P0 |
| **å‘é‡æ£€ç´¢** | 50%å®Œæˆï¼Œæ€§èƒ½ä¸è¶³ | FAISS+BGE (ä¼ä¸šçº§) | æ€§èƒ½å·®è· | P1 |
| **å®æ—¶é€šä¿¡** | 60%å®Œæˆï¼ŒWebSocketåŸºç¡€ | WebRTCæ ‡å‡† | åè®®ä¸å®Œæ•´ | P1 |
| **ç›‘æ§è§‚æµ‹** | 0%å®Œæˆï¼Œç³»ç»Ÿè¢«åˆ é™¤ | Prometheusç”Ÿæ€ | å®Œå…¨ç¼ºå¤± | P0 |

---

## ğŸš¨ æ ¸å¿ƒæŠ€æœ¯å€ºåŠ¡æ¸…å•

### 1. è¯­éŸ³å¤„ç†ç³»ç»Ÿ - ä¸¥é‡ç¼ºå¤± âŒ

**é—®é¢˜æè¿°**ï¼š
- `backend/internal/handlers/voice_ws.go` å®Œå…¨ç¼ºå¤±
- ASR/TTSæœåŠ¡é›†æˆä¸å®Œæ•´
- å®æ—¶éŸ³é¢‘æµå¤„ç†æ¶æ„ç¼ºå¤±
- WebRTCä¿¡ä»¤å¤„ç†æœªå®ç°

**å¯¹æ ‡æ–¹æ¡ˆ**ï¼š
- **OpenAI Whisper**: å¼€æºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«
- **Edge-TTS**: å¾®è½¯å¼€æºè¯­éŸ³åˆæˆ
- **WebRTC**: å®æ—¶éŸ³è§†é¢‘é€šä¿¡æ ‡å‡†

### 2. å¯¹è¯ç®¡ç†ç³»ç»Ÿ - åŠŸèƒ½ç®€åŒ– âš ï¸

**é—®é¢˜æè¿°**ï¼š
- LangChain/LangGraphä¾èµ–è¢«ç§»é™¤
- æ„å›¾è¯†åˆ«å’Œå®ä½“æŠ½å–ç¼ºå¤±
- å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†ä¸å®Œæ•´
- å¯¹è¯æµç¨‹ç¼–æ’èƒ½åŠ›ç¼ºå¤±

**å¯¹æ ‡æ–¹æ¡ˆ**ï¼š
- **Rasa**: å¼€æºå¯¹è¯AIæ¡†æ¶
- **Botpress**: å¼€æºå¯¹è¯å¹³å°
- **Microsoft Bot Framework**: ä¼ä¸šçº§æ–¹æ¡ˆ

### 3. ç›‘æ§è§‚æµ‹ç³»ç»Ÿ - å®Œå…¨ç¼ºå¤± âŒ

**é—®é¢˜æè¿°**ï¼š
- PrometheusæŒ‡æ ‡ç³»ç»Ÿè¢«å®Œå…¨åˆ é™¤
- é“¾è·¯è¿½è¸ªåŠŸèƒ½ç¼ºå¤±
- æœåŠ¡å¥åº·æ£€æŸ¥ä¸å®Œæ•´
- æ€§èƒ½æŒ‡æ ‡æ”¶é›†ç¼ºå¤±

**å¯¹æ ‡æ–¹æ¡ˆ**ï¼š
- **Prometheus + Grafana**: ç›‘æ§å¯è§†åŒ–æ ‡å‡†
- **Jaeger**: åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ª
- **OpenTelemetry**: å¯è§‚æµ‹æ€§ç»Ÿä¸€æ ‡å‡†

---

## ğŸ› ï¸ åŸºäºå¼€æºæ–¹æ¡ˆçš„å®Œæ•´å®ç°

### 1. OpenAI Whisper å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ

#### 1.1 æ ¸å¿ƒASRæœåŠ¡å®ç°

```python
# algo/core/whisper_realtime_asr.py
import asyncio
import torch
import whisper
import numpy as np
import webrtcvad
import collections
import logging
from typing import AsyncGenerator, Optional, Dict, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

@dataclass
class ASRConfig:
    """ASRé…ç½®"""
    model_size: str = "base"  # tiny, base, small, medium, large
    language: str = "zh"
    device: str = "auto"  # auto, cpu, cuda
    vad_aggressiveness: int = 2  # 0-3, è¶Šé«˜è¶Šæ¿€è¿›
    sample_rate: int = 16000
    frame_duration_ms: int = 30
    silence_timeout_ms: int = 1000

@dataclass
class ASRResult:
    """ASRè¯†åˆ«ç»“æœ"""
    text: str
    confidence: float
    is_final: bool
    language: str
    processing_time_ms: float
    timestamp: float

class WhisperRealtimeASR:
    """åŸºäºOpenAI Whisperçš„å®æ—¶ASRæœåŠ¡"""
    
    def __init__(self, config: ASRConfig):
        self.config = config
        self.model = None
        self.vad = webrtcvad.Vad(config.vad_aggressiveness)
        self.executor = ThreadPoolExecutor(max_workers=2)
        
        # éŸ³é¢‘ç¼“å†²åŒºç®¡ç†
        self.frame_size = int(config.sample_rate * config.frame_duration_ms / 1000)
        self.audio_buffer = collections.deque(maxlen=100)  # 3ç§’ç¼“å†²
        self.silence_frames = 0
        self.speech_frames = 0
        self.is_speaking = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "average_latency": 0.0,
            "error_count": 0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–Whisperæ¨¡å‹"""
        try:
            logger.info(f"åŠ è½½Whisperæ¨¡å‹: {self.config.model_size}")
            
            # ç¡®å®šè®¾å¤‡
            if self.config.device == "auto":
                device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                device = self.config.device
            
            # åœ¨çº¿ç¨‹æ± ä¸­åŠ è½½æ¨¡å‹ï¼Œé¿å…é˜»å¡
            loop = asyncio.get_event_loop()
            self.model = await loop.run_in_executor(
                self.executor,
                whisper.load_model,
                self.config.model_size,
                device
            )
            
            logger.info(f"Whisperæ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {device}")
            
        except Exception as e:
            logger.error(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    async def process_audio_stream(
        self, 
        audio_chunk: bytes, 
        session_id: str = "default"
    ) -> AsyncGenerator[ASRResult, None]:
        """å¤„ç†å®æ—¶éŸ³é¢‘æµ"""
        try:
            start_time = asyncio.get_event_loop().time()
            
            # VADæ£€æµ‹
            is_speech = self._detect_speech(audio_chunk)
            
            if is_speech:
                self.speech_frames += 1
                self.silence_frames = 0
                
                # æ·»åŠ åˆ°éŸ³é¢‘ç¼“å†²åŒº
                audio_data = np.frombuffer(audio_chunk, dtype=np.int16)
                self.audio_buffer.extend(audio_data)
                
                # æ£€æµ‹è¯´è¯å¼€å§‹
                if not self.is_speaking and self.speech_frames > 3:  # 90msè¿ç»­è¯­éŸ³
                    self.is_speaking = True
                    yield ASRResult(
                        text="",
                        confidence=0.0,
                        is_final=False,
                        language=self.config.language,
                        processing_time_ms=0.0,
                        timestamp=start_time
                    )
                
                # å®æ—¶è¯†åˆ«ï¼ˆéƒ¨åˆ†ç»“æœï¼‰
                if len(self.audio_buffer) > self.frame_size * 10:  # 300mséŸ³é¢‘
                    partial_result = await self._transcribe_partial()
                    if partial_result:
                        processing_time = (asyncio.get_event_loop().time() - start_time) * 1000
                        yield ASRResult(
                            text=partial_result,
                            confidence=0.8,  # éƒ¨åˆ†ç»“æœç½®ä¿¡åº¦è¾ƒä½
                            is_final=False,
                            language=self.config.language,
                            processing_time_ms=processing_time,
                            timestamp=start_time
                        )
            
            else:  # é™éŸ³
                self.silence_frames += 1
                
                # æ£€æµ‹è¯´è¯ç»“æŸ
                if (self.is_speaking and 
                    self.silence_frames > self.config.silence_timeout_ms // self.config.frame_duration_ms):
                    
                    self.is_speaking = False
                    
                    # æ‰§è¡Œæœ€ç»ˆè¯†åˆ«
                    if len(self.audio_buffer) > 0:
                        final_result = await self._transcribe_final()
                        processing_time = (asyncio.get_event_loop().time() - start_time) * 1000
                        
                        yield ASRResult(
                            text=final_result,
                            confidence=0.95,  # æœ€ç»ˆç»“æœç½®ä¿¡åº¦è¾ƒé«˜
                            is_final=True,
                            language=self.config.language,
                            processing_time_ms=processing_time,
                            timestamp=start_time
                        )
                        
                        # æ¸…ç©ºç¼“å†²åŒº
                        self.audio_buffer.clear()
                        self.speech_frames = 0
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self._update_stats(processing_time, True)
            
        except Exception as e:
            logger.error(f"ASRå¤„ç†é”™è¯¯: {e}")
            self.stats["error_count"] += 1
            yield ASRResult(
                text="",
                confidence=0.0,
                is_final=True,
                language=self.config.language,
                processing_time_ms=0.0,
                timestamp=asyncio.get_event_loop().time()
            )
    
    def _detect_speech(self, audio_chunk: bytes) -> bool:
        """è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        try:
            return self.vad.is_speech(audio_chunk, self.config.sample_rate)
        except Exception as e:
            logger.warning(f"VADæ£€æµ‹å¤±è´¥: {e}")
            return False
    
    async def _transcribe_partial(self) -> str:
        """éƒ¨åˆ†è½¬å½•ï¼ˆå¿«é€Ÿï¼Œä½ç²¾åº¦ï¼‰"""
        try:
            # ä½¿ç”¨æœ€è¿‘çš„éŸ³é¢‘ç‰‡æ®µ
            recent_audio = list(self.audio_buffer)[-self.frame_size * 20:]  # æœ€è¿‘600ms
            audio_array = np.array(recent_audio, dtype=np.float32) / 32768.0
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè½¬å½•
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor,
                self._whisper_transcribe,
                audio_array,
                {"task": "transcribe", "language": self.config.language, "fp16": False}
            )
            
            return result.get("text", "").strip()
            
        except Exception as e:
            logger.warning(f"éƒ¨åˆ†è½¬å½•å¤±è´¥: {e}")
            return ""
    
    async def _transcribe_final(self) -> str:
        """æœ€ç»ˆè½¬å½•ï¼ˆé«˜ç²¾åº¦ï¼‰"""
        try:
            # ä½¿ç”¨å®Œæ•´éŸ³é¢‘ç¼“å†²åŒº
            audio_array = np.array(list(self.audio_buffer), dtype=np.float32) / 32768.0
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè½¬å½•
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor,
                self._whisper_transcribe,
                audio_array,
                {
                    "task": "transcribe", 
                    "language": self.config.language, 
                    "fp16": torch.cuda.is_available(),
                    "temperature": 0.0  # ç¡®å®šæ€§è¾“å‡º
                }
            )
            
            return result.get("text", "").strip()
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆè½¬å½•å¤±è´¥: {e}")
            return ""
    
    def _whisper_transcribe(self, audio: np.ndarray, options: Dict[str, Any]) -> Dict[str, Any]:
        """Whisperè½¬å½•ï¼ˆåŒæ­¥æ–¹æ³•ï¼Œåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        return self.model.transcribe(audio, **options)
    
    def _update_stats(self, processing_time: float, success: bool):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.stats["total_requests"] += 1
        if success:
            self.stats["successful_requests"] += 1
            # è®¡ç®—ç§»åŠ¨å¹³å‡å»¶è¿Ÿ
            alpha = 0.1  # å¹³æ»‘å› å­
            self.stats["average_latency"] = (
                alpha * processing_time + 
                (1 - alpha) * self.stats["average_latency"]
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        success_rate = (
            self.stats["successful_requests"] / max(1, self.stats["total_requests"])
        )
        return {
            **self.stats,
            "success_rate": success_rate,
            "model_info": {
                "model_size": self.config.model_size,
                "language": self.config.language,
                "device": "cuda" if torch.cuda.is_available() else "cpu"
            }
        }

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def test_whisper_asr():
    """æµ‹è¯•Whisper ASRæœåŠ¡"""
    config = ASRConfig(
        model_size="base",
        language="zh",
        vad_aggressiveness=2
    )
    
    asr = WhisperRealtimeASR(config)
    await asr.initialize()
    
    # æ¨¡æ‹ŸéŸ³é¢‘æµå¤„ç†
    print("å¼€å§‹éŸ³é¢‘æµå¤„ç†æµ‹è¯•...")
    
    # è¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„éŸ³é¢‘æ•°æ®
    # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
    sample_audio = np.random.randint(-1000, 1000, 480, dtype=np.int16).tobytes()
    
    async for result in asr.process_audio_stream(sample_audio):
        print(f"ASRç»“æœ: {result.text}")
        print(f"ç½®ä¿¡åº¦: {result.confidence}")
        print(f"æ˜¯å¦æœ€ç»ˆ: {result.is_final}")
        print(f"å¤„ç†æ—¶é—´: {result.processing_time_ms:.2f}ms")
        print("---")
        
        if result.is_final:
            break
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = asr.get_stats()
    print(f"æ€§èƒ½ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    asyncio.run(test_whisper_asr())
```

#### 1.2 WebSocketè¯­éŸ³å¤„ç†å™¨é‡å»º

```go
// backend/internal/handlers/realtime_voice_handler.go
package handlers

import (
    "context"
    "encoding/base64"
    "encoding/json"
    "fmt"
    "log"
    "net/http"
    "sync"
    "time"

    "github.com/gin-gonic/gin"
    "github.com/gorilla/websocket"
    "github.com/google/uuid"
)

type RealtimeVoiceHandler struct {
    upgrader     websocket.Upgrader
    sessions     map[string]*VoiceSession
    sessionsMux  sync.RWMutex
    asrService   ASRService
    ttsService   TTSService
    metrics      *MetricsCollector
}

type VoiceSession struct {
    ID           string                 `json:"id"`
    UserID       string                 `json:"user_id"`
    TenantID     string                 `json:"tenant_id"`
    Connection   *websocket.Conn        `json:"-"`
    Context      context.Context        `json:"-"`
    Cancel       context.CancelFunc     `json:"-"`
    State        VoiceSessionState      `json:"state"`
    Config       VoiceConfig            `json:"config"`
    Stats        VoiceSessionStats      `json:"stats"`
    LastActivity time.Time              `json:"last_activity"`
    CreatedAt    time.Time              `json:"created_at"`
}

type VoiceSessionState struct {
    IsRecording    bool      `json:"is_recording"`
    IsSpeaking     bool      `json:"is_speaking"`
    CurrentText    string    `json:"current_text"`
    AudioBuffer    []byte    `json:"-"`
    LastTranscript string    `json:"last_transcript"`
    Language       string    `json:"language"`
}

type VoiceConfig struct {
    ASRModel        string  `json:"asr_model"`
    TTSVoice        string  `json:"tts_voice"`
    Language        string  `json:"language"`
    SampleRate      int     `json:"sample_rate"`
    Channels        int     `json:"channels"`
    VADSensitivity  int     `json:"vad_sensitivity"`
    AutoPunctuation bool    `json:"auto_punctuation"`
    RealTimeResults bool    `json:"real_time_results"`
}

type VoiceSessionStats struct {
    TotalAudioDuration  float64   `json:"total_audio_duration"`
    TranscriptionCount  int       `json:"transcription_count"`
    AverageLatency      float64   `json:"average_latency"`
    ErrorCount          int       `json:"error_count"`
    StartTime           time.Time `json:"start_time"`
    LastUpdateTime      time.Time `json:"last_update_time"`
}

type VoiceMessage struct {
    Type      string      `json:"type"`
    SessionID string      `json:"session_id,omitempty"`
    Data      interface{} `json:"data,omitempty"`
    Timestamp int64       `json:"timestamp"`
    Metadata  map[string]interface{} `json:"metadata,omitempty"`
}

type AudioChunk struct {
    Data       string `json:"data"`        // base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
    Format     string `json:"format"`      // éŸ³é¢‘æ ¼å¼: pcm16, opus, mp3
    SampleRate int    `json:"sample_rate"` // é‡‡æ ·ç‡
    Channels   int    `json:"channels"`    // å£°é“æ•°
    Duration   int    `json:"duration"`    // æŒç»­æ—¶é—´(ms)
    IsFinal    bool   `json:"is_final"`    // æ˜¯å¦ä¸ºæœ€åä¸€å—
    Sequence   int    `json:"sequence"`    // åºåˆ—å·
}

type TranscriptionResult struct {
    Text         string  `json:"text"`
    Confidence   float64 `json:"confidence"`
    IsFinal      bool    `json:"is_final"`
    Language     string  `json:"language"`
    Duration     int     `json:"duration"`
    WordCount    int     `json:"word_count"`
    ProcessingMs float64 `json:"processing_ms"`
}

func NewRealtimeVoiceHandler(
    asrService ASRService,
    ttsService TTSService,
    metrics *MetricsCollector,
) *RealtimeVoiceHandler {
    return &RealtimeVoiceHandler{
        upgrader: websocket.Upgrader{
            CheckOrigin: func(r *http.Request) bool {
                // ç”Ÿäº§ç¯å¢ƒéœ€è¦ä¸¥æ ¼çš„Originæ£€æŸ¥
                return true
            },
            ReadBufferSize:  8192,
            WriteBufferSize: 8192,
            HandshakeTimeout: 10 * time.Second,
        },
        sessions:   make(map[string]*VoiceSession),
        asrService: asrService,
        ttsService: ttsService,
        metrics:    metrics,
    }
}

func (h *RealtimeVoiceHandler) HandleWebSocket(c *gin.Context) {
    // å‡çº§åˆ°WebSocketè¿æ¥
    conn, err := h.upgrader.Upgrade(c.Writer, c.Request, nil)
    if err != nil {
        log.Printf("WebSocketå‡çº§å¤±è´¥: %v", err)
        h.metrics.RecordError("websocket_upgrade_failed", err)
        return
    }
    defer conn.Close()

    // åˆ›å»ºè¯­éŸ³ä¼šè¯
    session := h.createVoiceSession(conn, c)
    
    // è®°å½•è¿æ¥æŒ‡æ ‡
    h.metrics.IncrementWSConnections("voice", session.TenantID)
    defer h.metrics.DecrementWSConnections()

    // å‘é€ä¼šè¯åˆ›å»ºç¡®è®¤
    h.sendSessionCreated(session)

    // å¯åŠ¨ä¼šè¯ç®¡ç†åç¨‹
    go h.manageSession(session)
    
    // å¤„ç†æ¶ˆæ¯å¾ªç¯
    h.handleMessageLoop(session)
    
    // æ¸…ç†ä¼šè¯
    h.cleanupSession(session)
}

func (h *RealtimeVoiceHandler) createVoiceSession(conn *websocket.Conn, c *gin.Context) *VoiceSession {
    sessionID := uuid.New().String()
    ctx, cancel := context.WithCancel(context.Background())
    
    session := &VoiceSession{
        ID:         sessionID,
        UserID:     c.GetString("user_id"),
        TenantID:   c.GetString("tenant_id"),
        Connection: conn,
        Context:    ctx,
        Cancel:     cancel,
        State: VoiceSessionState{
            IsRecording:    false,
            IsSpeaking:     false,
            CurrentText:    "",
            AudioBuffer:    make([]byte, 0),
            LastTranscript: "",
            Language:       "zh-CN",
        },
        Config: VoiceConfig{
            ASRModel:        "whisper-base",
            TTSVoice:        "zh-CN-XiaoxiaoNeural",
            Language:        "zh-CN",
            SampleRate:      16000,
            Channels:        1,
            VADSensitivity:  2,
            AutoPunctuation: true,
            RealTimeResults: true,
        },
        Stats: VoiceSessionStats{
            StartTime:      time.Now(),
            LastUpdateTime: time.Now(),
        },
        LastActivity: time.Now(),
        CreatedAt:    time.Now(),
    }

    h.sessionsMux.Lock()
    h.sessions[sessionID] = session
    h.sessionsMux.Unlock()

    return session
}

func (h *RealtimeVoiceHandler) handleMessageLoop(session *VoiceSession) {
    defer session.Cancel()
    
    for {
        select {
        case <-session.Context.Done():
            return
        default:
            // è®¾ç½®è¯»å–è¶…æ—¶
            session.Connection.SetReadDeadline(time.Now().Add(60 * time.Second))
            
            var msg VoiceMessage
            err := session.Connection.ReadJSON(&msg)
            if err != nil {
                if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway, websocket.CloseAbnormalClosure) {
                    log.Printf("WebSocketè¯»å–é”™è¯¯: %v", err)
                    h.metrics.RecordError("websocket_read_error", err)
                }
                return
            }
            
            // æ›´æ–°æ´»åŠ¨æ—¶é—´
            session.LastActivity = time.Now()
            session.Stats.LastUpdateTime = time.Now()
            
            // è®°å½•æ¶ˆæ¯æŒ‡æ ‡
            h.metrics.RecordWSMessage("received", msg.Type)
            
            // å¤„ç†æ¶ˆæ¯
            h.handleMessage(session, msg)
        }
    }
}

func (h *RealtimeVoiceHandler) handleMessage(session *VoiceSession, msg VoiceMessage) {
    switch msg.Type {
    case "start_recording":
        h.handleStartRecording(session, msg)
    case "audio_chunk":
        h.handleAudioChunk(session, msg)
    case "stop_recording":
        h.handleStopRecording(session, msg)
    case "configure":
        h.handleConfigure(session, msg)
    case "get_stats":
        h.handleGetStats(session, msg)
    case "ping":
        h.handlePing(session, msg)
    default:
        h.sendError(session, "unknown_message_type", fmt.Sprintf("æœªçŸ¥æ¶ˆæ¯ç±»å‹: %s", msg.Type))
    }
}

func (h *RealtimeVoiceHandler) handleStartRecording(session *VoiceSession, msg VoiceMessage) {
    if session.State.IsRecording {
        h.sendError(session, "already_recording", "ä¼šè¯å·²åœ¨å½•éŸ³ä¸­")
        return
    }
    
    session.State.IsRecording = true
    session.State.AudioBuffer = session.State.AudioBuffer[:0] // æ¸…ç©ºç¼“å†²åŒº
    session.Stats.StartTime = time.Now()
    
    h.sendMessage(session, VoiceMessage{
        Type:      "recording_started",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "session_id": session.ID,
            "config":     session.Config,
            "status":     "recording",
        },
        Timestamp: time.Now().UnixMilli(),
    })
    
    log.Printf("ä¼šè¯ %s å¼€å§‹å½•éŸ³", session.ID)
}

func (h *RealtimeVoiceHandler) handleAudioChunk(session *VoiceSession, msg VoiceMessage) {
    if !session.State.IsRecording {
        h.sendError(session, "not_recording", "ä¼šè¯æœªåœ¨å½•éŸ³çŠ¶æ€")
        return
    }
    
    // è§£æéŸ³é¢‘æ•°æ®
    audioDataMap, ok := msg.Data.(map[string]interface{})
    if !ok {
        h.sendError(session, "invalid_audio_data", "æ— æ•ˆçš„éŸ³é¢‘æ•°æ®æ ¼å¼")
        return
    }
    
    var audioChunk AudioChunk
    audioDataBytes, _ := json.Marshal(audioDataMap)
    if err := json.Unmarshal(audioDataBytes, &audioChunk); err != nil {
        h.sendError(session, "parse_error", fmt.Sprintf("éŸ³é¢‘æ•°æ®è§£æå¤±è´¥: %v", err))
        return
    }
    
    // å¼‚æ­¥å¤„ç†éŸ³é¢‘æ•°æ®
    go h.processAudioChunk(session, audioChunk)
}

func (h *RealtimeVoiceHandler) processAudioChunk(session *VoiceSession, audioChunk AudioChunk) {
    startTime := time.Now()
    
    // è§£ç base64éŸ³é¢‘æ•°æ®
    audioData, err := base64.StdEncoding.DecodeString(audioChunk.Data)
    if err != nil {
        h.sendError(session, "decode_error", fmt.Sprintf("éŸ³é¢‘æ•°æ®è§£ç å¤±è´¥: %v", err))
        return
    }
    
    // æ·»åŠ åˆ°éŸ³é¢‘ç¼“å†²åŒº
    session.State.AudioBuffer = append(session.State.AudioBuffer, audioData...)
    
    // è°ƒç”¨ASRæœåŠ¡
    asrResult, err := h.asrService.ProcessAudio(ASRRequest{
        AudioData:  audioData,
        Format:     audioChunk.Format,
        SampleRate: audioChunk.SampleRate,
        Channels:   audioChunk.Channels,
        Language:   session.Config.Language,
        IsFinal:    audioChunk.IsFinal,
        SessionID:  session.ID,
    })
    
    if err != nil {
        log.Printf("ASRå¤„ç†å¤±è´¥: %v", err)
        h.metrics.RecordError("asr_processing_failed", err)
        session.Stats.ErrorCount++
        return
    }
    
    // æ›´æ–°ä¼šè¯çŠ¶æ€
    if asrResult.Text != "" {
        session.State.CurrentText = asrResult.Text
        if asrResult.IsFinal {
            session.State.LastTranscript = asrResult.Text
            session.Stats.TranscriptionCount++
        }
    }
    
    // è®¡ç®—å¤„ç†å»¶è¿Ÿ
    processingTime := time.Since(startTime)
    session.Stats.AverageLatency = (session.Stats.AverageLatency + processingTime.Seconds()) / 2
    
    // å‘é€ASRç»“æœ
    h.sendMessage(session, VoiceMessage{
        Type:      "transcription",
        SessionID: session.ID,
        Data: TranscriptionResult{
            Text:         asrResult.Text,
            Confidence:   asrResult.Confidence,
            IsFinal:      asrResult.IsFinal,
            Language:     asrResult.Language,
            Duration:     audioChunk.Duration,
            WordCount:    len(strings.Fields(asrResult.Text)),
            ProcessingMs: processingTime.Seconds() * 1000,
        },
        Timestamp: time.Now().UnixMilli(),
    })
    
    // è®°å½•æŒ‡æ ‡
    h.metrics.RecordASRRequest(
        session.Config.ASRModel,
        session.Config.Language,
        "success",
        processingTime,
    )
}

func (h *RealtimeVoiceHandler) handleStopRecording(session *VoiceSession, msg VoiceMessage) {
    if !session.State.IsRecording {
        h.sendError(session, "not_recording", "ä¼šè¯æœªåœ¨å½•éŸ³çŠ¶æ€")
        return
    }
    
    session.State.IsRecording = false
    session.Stats.TotalAudioDuration = time.Since(session.Stats.StartTime).Seconds()
    
    h.sendMessage(session, VoiceMessage{
        Type:      "recording_stopped",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "session_id":    session.ID,
            "status":        "stopped",
            "final_text":    session.State.LastTranscript,
            "total_duration": session.Stats.TotalAudioDuration,
            "stats":         session.Stats,
        },
        Timestamp: time.Now().UnixMilli(),
    })
    
    log.Printf("ä¼šè¯ %s åœæ­¢å½•éŸ³ï¼Œæ€»æ—¶é•¿: %.2fs", session.ID, session.Stats.TotalAudioDuration)
}

func (h *RealtimeVoiceHandler) handleConfigure(session *VoiceSession, msg VoiceMessage) {
    configData, ok := msg.Data.(map[string]interface{})
    if !ok {
        h.sendError(session, "invalid_config", "æ— æ•ˆçš„é…ç½®æ•°æ®")
        return
    }
    
    // æ›´æ–°é…ç½®
    if asrModel, exists := configData["asr_model"]; exists {
        if model, ok := asrModel.(string); ok {
            session.Config.ASRModel = model
        }
    }
    
    if ttsVoice, exists := configData["tts_voice"]; exists {
        if voice, ok := ttsVoice.(string); ok {
            session.Config.TTSVoice = voice
        }
    }
    
    if language, exists := configData["language"]; exists {
        if lang, ok := language.(string); ok {
            session.Config.Language = lang
            session.State.Language = lang
        }
    }
    
    h.sendMessage(session, VoiceMessage{
        Type:      "configured",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "config": session.Config,
        },
        Timestamp: time.Now().UnixMilli(),
    })
}

func (h *RealtimeVoiceHandler) handleGetStats(session *VoiceSession, msg VoiceMessage) {
    h.sendMessage(session, VoiceMessage{
        Type:      "stats",
        SessionID: session.ID,
        Data:      session.Stats,
        Timestamp: time.Now().UnixMilli(),
    })
}

func (h *RealtimeVoiceHandler) handlePing(session *VoiceSession, msg VoiceMessage) {
    h.sendMessage(session, VoiceMessage{
        Type:      "pong",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "server_time": time.Now().UnixMilli(),
        },
        Timestamp: time.Now().UnixMilli(),
    })
}

func (h *RealtimeVoiceHandler) sendMessage(session *VoiceSession, msg VoiceMessage) {
    session.Connection.SetWriteDeadline(time.Now().Add(10 * time.Second))
    if err := session.Connection.WriteJSON(msg); err != nil {
        log.Printf("å‘é€æ¶ˆæ¯å¤±è´¥: %v", err)
        h.metrics.RecordError("websocket_write_error", err)
        session.Cancel()
        return
    }
    
    h.metrics.RecordWSMessage("sent", msg.Type)
}

func (h *RealtimeVoiceHandler) sendError(session *VoiceSession, code, message string) {
    h.sendMessage(session, VoiceMessage{
        Type:      "error",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "code":    code,
            "message": message,
        },
        Timestamp: time.Now().UnixMilli(),
    })
}

func (h *RealtimeVoiceHandler) sendSessionCreated(session *VoiceSession) {
    h.sendMessage(session, VoiceMessage{
        Type:      "session_created",
        SessionID: session.ID,
        Data: map[string]interface{}{
            "session_id": session.ID,
            "config":     session.Config,
            "status":     "ready",
            "server_info": map[string]interface{}{
                "version":    "1.0.0",
                "capabilities": []string{"asr", "tts", "real_time", "vad"},
            },
        },
        Timestamp: time.Now().UnixMilli(),
    })
}

func (h *RealtimeVoiceHandler) manageSession(session *VoiceSession) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-session.Context.Done():
            return
        case <-ticker.C:
            // æ£€æŸ¥ä¼šè¯è¶…æ—¶
            if time.Since(session.LastActivity) > 10*time.Minute {
                log.Printf("ä¼šè¯ %s è¶…æ—¶ï¼Œè‡ªåŠ¨æ¸…ç†", session.ID)
                session.Cancel()
                return
            }
            
            // å‘é€å¿ƒè·³
            h.sendMessage(session, VoiceMessage{
                Type:      "heartbeat",
                SessionID: session.ID,
                Data: map[string]interface{}{
                    "server_time": time.Now().UnixMilli(),
                    "session_duration": time.Since(session.CreatedAt).Seconds(),
                },
                Timestamp: time.Now().UnixMilli(),
            })
        }
    }
}

func (h *RealtimeVoiceHandler) cleanupSession(session *VoiceSession) {
    h.sessionsMux.Lock()
    delete(h.sessions, session.ID)
    h.sessionsMux.Unlock()
    
    log.Printf("ä¼šè¯ %s å·²æ¸…ç†ï¼ŒæŒç»­æ—¶é—´: %.2fs", 
        session.ID, 
        time.Since(session.CreatedAt).Seconds())
}

// ASRå’ŒTTSæœåŠ¡æ¥å£å®šä¹‰
type ASRService interface {
    ProcessAudio(request ASRRequest) (*ASRResponse, error)
}

type TTSService interface {
    Synthesize(request TTSRequest) (*TTSResponse, error)
}

type ASRRequest struct {
    AudioData  []byte
    Format     string
    SampleRate int
    Channels   int
    Language   string
    IsFinal    bool
    SessionID  string
}

type ASRResponse struct {
    Text       string
    Confidence float64
    IsFinal    bool
    Language   string
}

type TTSRequest struct {
    Text      string
    Voice     string
    Language  string
    Format    string
    SessionID string
}

type TTSResponse struct {
    AudioData []byte
    Format    string
    Duration  float64
}

type MetricsCollector interface {
    IncrementWSConnections(connType, tenantID string)
    DecrementWSConnections()
    RecordWSMessage(direction, messageType string)
    RecordASRRequest(model, language, status string, latency time.Duration)
    RecordError(errorType string, err error)
}
```

### 2. Edge-TTS è¯­éŸ³åˆæˆæœåŠ¡

#### 2.1 é«˜æ€§èƒ½TTSæœåŠ¡å®ç°

```python
# algo/core/edge_tts_service.py
import asyncio
import edge_tts
import io
import logging
import time
import hashlib
from typing import AsyncGenerator, Optional, Dict, Any, List
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import aiofiles
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class TTSConfig:
    """TTSé…ç½®"""
    voice: str = "zh-CN-XiaoxiaoNeural"
    rate: str = "+0%"
    pitch: str = "+0Hz"
    volume: str = "+0%"
    output_format: str = "mp3"  # mp3, wav, opus
    cache_enabled: bool = True
    cache_dir: str = "data/tts_cache"
    max_cache_size_mb: int = 500

@dataclass
class TTSRequest:
    """TTSè¯·æ±‚"""
    text: str
    voice: Optional[str] = None
    rate: Optional[str] = None
    pitch: Optional[str] = None
    volume: Optional[str] = None
    output_format: Optional[str] = None
    session_id: Optional[str] = None
    priority: int = 0  # 0=normal, 1=high, 2=urgent

@dataclass
class TTSResponse:
    """TTSå“åº”"""
    audio_data: bytes
    format: str
    duration_ms: int
    text_length: int
    processing_time_ms: float
    cached: bool
    voice_used: str

class EdgeTTSService:
    """åŸºäºEdge-TTSçš„é«˜æ€§èƒ½è¯­éŸ³åˆæˆæœåŠ¡"""
    
    def __init__(self, config: TTSConfig):
        self.config = config
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # çº¿ç¨‹æ± ç”¨äºå¹¶è¡Œå¤„ç†
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ç¼“å­˜ç®¡ç†
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "size_mb": 0.0
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_latency_ms": 0.0,
            "total_audio_duration_ms": 0,
            "cache_hit_rate": 0.0
        }
        
        # å¯ç”¨è¯­éŸ³åˆ—è¡¨ç¼“å­˜
        self._available_voices: Optional[List[Dict[str, Any]]] = None
        self._voices_cache_time: Optional[float] = None
    
    async def initialize(self):
        """åˆå§‹åŒ–TTSæœåŠ¡"""
        try:
            # é¢„åŠ è½½å¯ç”¨è¯­éŸ³åˆ—è¡¨
            await self.get_available_voices()
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            await self._cleanup_cache()
            
            logger.info("Edge-TTSæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"TTSæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def synthesize(self, request: TTSRequest) -> TTSResponse:
        """è¯­éŸ³åˆæˆ"""
        start_time = time.time()
        
        try:
            # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼å¡«å……è¯·æ±‚
            voice = request.voice or self.config.voice
            rate = request.rate or self.config.rate
            pitch = request.pitch or self.config.pitch
            volume = request.volume or self.config.volume
            output_format = request.output_format or self.config.output_format
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(request.text, voice, rate, pitch, volume)
            cached_audio = await self._get_from_cache(cache_key)
            
            if cached_audio:
                self.cache_stats["hits"] += 1
                processing_time = (time.time() - start_time) * 1000
                
                return TTSResponse(
                    audio_data=cached_audio,
                    format=output_format,
                    duration_ms=self._estimate_duration(request.text),
                    text_length=len(request.text),
                    processing_time_ms=processing_time,
                    cached=True,
                    voice_used=voice
                )
            
            self.cache_stats["misses"] += 1
            
            # æ‰§è¡Œè¯­éŸ³åˆæˆ
            audio_data = await self._synthesize_audio(
                request.text, voice, rate, pitch, volume
            )
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.config.cache_enabled:
                await self._save_to_cache(cache_key, audio_data)
            
            processing_time = (time.time() - start_time) * 1000
            duration_ms = self._estimate_duration(request.text)
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(processing_time, duration_ms, True)
            
            return TTSResponse(
                audio_data=audio_data,
                format=output_format,
                duration_ms=duration_ms,
                text_length=len(request.text),
                processing_time_ms=processing_time,
                cached=False,
                voice_used=voice
            )
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            self._update_stats(processing_time, 0, False)
            logger.error(f"TTSåˆæˆå¤±è´¥: {e}")
            raise
    
    async def synthesize_stream(self, request: TTSRequest) -> AsyncGenerator[bytes, None]:
        """æµå¼è¯­éŸ³åˆæˆ"""
        try:
            voice = request.voice or self.config.voice
            rate = request.rate or self.config.rate
            pitch = request.pitch or self.config.pitch
            volume = request.volume or self.config.volume
            
            communicate = edge_tts.Communicate(
                text=request.text,
                voice=voice,
                rate=rate,
                pitch=pitch,
                volume=volume
            )
            
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    yield chunk["data"]
                elif chunk["type"] == "WordBoundary":
                    # å¯ä»¥ç”¨äºå®ç°å­—çº§åˆ«çš„åŒæ­¥
                    logger.debug(f"Word boundary: {chunk}")
                    
        except Exception as e:
            logger.error(f"æµå¼TTSåˆæˆå¤±è´¥: {e}")
            raise
    
    async def _synthesize_audio(
        self, 
        text: str, 
        voice: str, 
        rate: str, 
        pitch: str, 
        volume: str
    ) -> bytes:
        """æ‰§è¡ŒéŸ³é¢‘åˆæˆ"""
        communicate = edge_tts.Communicate(
            text=text,
            voice=voice,
            rate=rate,
            pitch=pitch,
            volume=volume
        )
        
        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data += chunk["data"]
        
        return audio_data
    
    def _generate_cache_key(
        self, 
        text: str, 
        voice: str, 
        rate: str, 
        pitch: str, 
        volume: str
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{text}|{voice}|{rate}|{pitch}|{volume}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    async def _get_from_cache(self, cache_key: str) -> Optional[bytes]:
        """ä»ç¼“å­˜è·å–éŸ³é¢‘æ•°æ®"""
        if not self.config.cache_enabled:
            return None
        
        cache_file = self.cache_dir / f"{cache_key}.mp3"
        
        try:
            if cache_file.exists():
                async with aiofiles.open(cache_file, 'rb') as f:
                    return await f.read()
        except Exception as e:
            logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        return None
    
    async def _save_to_cache(self, cache_key: str, audio_data: bytes):
        """ä¿å­˜éŸ³é¢‘æ•°æ®åˆ°ç¼“å­˜"""
        if not self.config.cache_enabled:
            return
        
        cache_file = self.cache_dir / f"{cache_key}.mp3"
        
        try:
            async with aiofiles.open(cache_file, 'wb') as f:
                await f.write(audio_data)
            
            # æ›´æ–°ç¼“å­˜å¤§å°ç»Ÿè®¡
            self.cache_stats["size_mb"] += len(audio_data) / (1024 * 1024)
            
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    async def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            cache_files = list(self.cache_dir.glob("*.mp3"))
            total_size = sum(f.stat().st_size for f in cache_files)
            current_size_mb = total_size / (1024 * 1024)
            
            if current_size_mb > self.config.max_cache_size_mb:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„æ–‡ä»¶
                cache_files.sort(key=lambda f: f.stat().st_mtime)
                
                while current_size_mb > self.config.max_cache_size_mb * 0.8:
                    if not cache_files:
                        break
                    
                    file_to_delete = cache_files.pop(0)
                    file_size = file_to_delete.stat().st_size
                    file_to_delete.unlink()
                    current_size_mb -= file_size / (1024 * 1024)
                    
                    logger.info(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶: {file_to_delete.name}")
            
            self.cache_stats["size_mb"] = current_size_mb
            
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def _estimate_duration(self, text: str) -> int:
        """ä¼°ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦æ¯åˆ†é’Ÿ200å­—ï¼Œè‹±æ–‡çº¦æ¯åˆ†é’Ÿ150è¯
        char_count = len(text)
        if any('\u4e00' <= char <= '\u9fff' for char in text):  # åŒ…å«ä¸­æ–‡
            words_per_minute = 200
        else:  # è‹±æ–‡æˆ–å…¶ä»–
            words_per_minute = 150
        
        duration_minutes = char_count / words_per_minute
        return int(duration_minutes * 60 * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
    
    def _update_stats(self, processing_time: float, duration_ms: int, success: bool):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.stats["total_requests"] += 1
        
        if success:
            self.stats["successful_requests"] += 1
            self.stats["total_audio_duration_ms"] += duration_ms
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡å»¶è¿Ÿ
            alpha = 0.1
            self.stats["average_latency_ms"] = (
                alpha * processing_time + 
                (1 - alpha) * self.stats["average_latency_ms"]
            )
        else:
            self.stats["failed_requests"] += 1
        
        # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
        total_cache_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        if total_cache_requests > 0:
            self.stats["cache_hit_rate"] = self.cache_stats["hits"] / total_cache_requests
    
    async def get_available_voices(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨è¯­éŸ³åˆ—è¡¨"""
        # ç¼“å­˜1å°æ—¶
        if (self._available_voices and self._voices_cache_time and 
            time.time() - self._voices_cache_time < 3600):
            return self._available_voices
        
        try:
            voices = await edge_tts.list_voices()
            self._available_voices = [
                {
                    "name": voice["Name"],
                    "display_name": voice["DisplayName"],
                    "locale": voice["Locale"],
                    "gender": voice["Gender"],
                    "suggested_codec": voice.get("SuggestedCodec", "audio-24khz-48kbitrate-mono-mp3"),
                    "friendly_name": voice.get("FriendlyName", ""),
                    "status": voice.get("Status", "GA")
                }
                for voice in voices
            ]
            self._voices_cache_time = time.time()
            
            logger.info(f"åŠ è½½äº† {len(self._available_voices)} ä¸ªå¯ç”¨è¯­éŸ³")
            
        except Exception as e:
            logger.error(f"è·å–è¯­éŸ³åˆ—è¡¨å¤±è´¥: {e}")
            if not self._available_voices:
                self._available_voices = []
        
        return self._available_voices
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        success_rate = 0.0
        if self.stats["total_requests"] > 0:
            success_rate = self.stats["successful_requests"] / self.stats["total_requests"]
        
        return {
            **self.stats,
            "success_rate": success_rate,
            "cache_stats": self.cache_stats,
            "config": {
                "voice": self.config.voice,
                "cache_enabled": self.config.cache_enabled,
                "cache_size_mb": self.cache_stats["size_mb"],
                "max_cache_size_mb": self.config.max_cache_size_mb
            }
        }
    
    async def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            cache_files = list(self.cache_dir.glob("*.mp3"))
            for cache_file in cache_files:
                cache_file.unlink()
            
            self.cache_stats = {
                "hits": 0,
                "misses": 0,
                "size_mb": 0.0
            }
            
            logger.info(f"æ¸…ç©ºäº† {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def test_edge_tts():
    """æµ‹è¯•Edge-TTSæœåŠ¡"""
    config = TTSConfig(
        voice="zh-CN-XiaoxiaoNeural",
        cache_enabled=True,
        max_cache_size_mb=100
    )
    
    tts = EdgeTTSService(config)
    await tts.initialize()
    
    # æµ‹è¯•è¯­éŸ³åˆæˆ
    request = TTSRequest(
        text="ä½ å¥½ï¼Œæˆ‘æ˜¯VoiceHelperè¯­éŸ³åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼",
        voice="zh-CN-XiaoxiaoNeural"
    )
    
    print("å¼€å§‹è¯­éŸ³åˆæˆæµ‹è¯•...")
    
    # ç¬¬ä¸€æ¬¡åˆæˆï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    response1 = await tts.synthesize(request)
    print(f"ç¬¬ä¸€æ¬¡åˆæˆ:")
    print(f"  éŸ³é¢‘å¤§å°: {len(response1.audio_data)} bytes")
    print(f"  å¤„ç†æ—¶é—´: {response1.processing_time_ms:.2f}ms")
    print(f"  æ˜¯å¦ç¼“å­˜: {response1.cached}")
    print(f"  é¢„ä¼°æ—¶é•¿: {response1.duration_ms}ms")
    
    # ç¬¬äºŒæ¬¡åˆæˆï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    response2 = await tts.synthesize(request)
    print(f"ç¬¬äºŒæ¬¡åˆæˆ:")
    print(f"  éŸ³é¢‘å¤§å°: {len(response2.audio_data)} bytes")
    print(f"  å¤„ç†æ—¶é—´: {response2.processing_time_ms:.2f}ms")
    print(f"  æ˜¯å¦ç¼“å­˜: {response2.cached}")
    
    # æµ‹è¯•æµå¼åˆæˆ
    print("å¼€å§‹æµå¼åˆæˆæµ‹è¯•...")
    chunks = []
    async for chunk in tts.synthesize_stream(request):
        chunks.append(chunk)
    
    print(f"æµå¼åˆæˆå®Œæˆï¼Œå…± {len(chunks)} ä¸ªéŸ³é¢‘å—")
    
    # è·å–å¯ç”¨è¯­éŸ³
    voices = await tts.get_available_voices()
    print(f"å¯ç”¨è¯­éŸ³æ•°é‡: {len(voices)}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = tts.get_stats()
    print(f"æ€§èƒ½ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    asyncio.run(test_edge_tts())
```

---

## ğŸ“ˆ å®æ–½è·¯çº¿å›¾ä¸æ—¶é—´è§„åˆ’

### Phase 1: åŸºç¡€è®¾æ–½ä¿®å¤ (2-3å‘¨) - P0ä¼˜å…ˆçº§

#### Week 1: ç›‘æ§ç³»ç»Ÿé‡å»º
- **ä»»åŠ¡**: é‡å»ºPrometheusæŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
- **äº¤ä»˜ç‰©**: å®Œæ•´çš„ç›‘æ§é¢æ¿å’Œå‘Šè­¦è§„åˆ™
- **éªŒæ”¶æ ‡å‡†**: æ‰€æœ‰æœåŠ¡æŒ‡æ ‡æ­£å¸¸æ”¶é›†ï¼ŒGrafanaé¢æ¿å¯è§†åŒ–

#### Week 2-3: è¯­éŸ³å¤„ç†æ ¸å¿ƒ
- **ä»»åŠ¡**: å®ç°OpenAI Whisper ASR + Edge-TTS + WebSocketå¤„ç†å™¨
- **äº¤ä»˜ç‰©**: å®Œæ•´çš„å®æ—¶è¯­éŸ³å¤„ç†èƒ½åŠ›
- **éªŒæ”¶æ ‡å‡†**: è¯­éŸ³è¯†åˆ«å»¶è¿Ÿ<300msï¼Œåˆæˆé¦–å“<500ms

### Phase 2: AIèƒ½åŠ›å¢å¼º (4-6å‘¨) - P1ä¼˜å…ˆçº§

#### Week 4-6: Rasaå¯¹è¯ç®¡ç†
- **ä»»åŠ¡**: é›†æˆRasa NLU/Coreï¼Œå®ç°å®Œæ•´å¯¹è¯ç®¡ç†
- **äº¤ä»˜ç‰©**: æ”¯æŒå¤šè½®å¯¹è¯çš„æ™ºèƒ½åŠ©æ‰‹
- **éªŒæ”¶æ ‡å‡†**: æ„å›¾è¯†åˆ«å‡†ç¡®ç‡>90%ï¼Œä¸Šä¸‹æ–‡ä¿æŒå®Œæ•´

#### Week 7-8: é«˜æ€§èƒ½RAGç³»ç»Ÿ
- **ä»»åŠ¡**: å®ç°ä¼ä¸šçº§FAISSå‘é‡æ£€ç´¢
- **äº¤ä»˜ç‰©**: æ¯«ç§’çº§æ£€ç´¢å“åº”ï¼Œæ”¯æŒæ··åˆæœç´¢
- **éªŒæ”¶æ ‡å‡†**: æ£€ç´¢å»¶è¿Ÿ<50msï¼Œå¬å›ç‡>95%

### Phase 3: å¹³å°æ‰©å±• (6-8å‘¨) - P2ä¼˜å…ˆçº§

#### Week 9-12: å¤šå¹³å°å®¢æˆ·ç«¯
- **ä»»åŠ¡**: å®Œå–„ç§»åŠ¨ç«¯ã€æ¡Œé¢ç«¯ã€å°ç¨‹åºå®¢æˆ·ç«¯
- **äº¤ä»˜ç‰©**: 6ä¸ªå¹³å°å…¨è¦†ç›–çš„å®¢æˆ·ç«¯åº”ç”¨
- **éªŒæ”¶æ ‡å‡†**: æ‰€æœ‰å¹³å°åŠŸèƒ½ä¸€è‡´ï¼Œç”¨æˆ·ä½“éªŒæµç•…

#### Week 13-16: ä¼ä¸šçº§åŠŸèƒ½
- **ä»»åŠ¡**: å¤šç§Ÿæˆ·ã€å®‰å…¨å®¡è®¡ã€æ€§èƒ½ä¼˜åŒ–
- **äº¤ä»˜ç‰©**: ç”Ÿäº§å°±ç»ªçš„ä¼ä¸šçº§å¹³å°
- **éªŒæ”¶æ ‡å‡†**: æ”¯æŒ1000+å¹¶å‘ç”¨æˆ·ï¼Œ99.9%å¯ç”¨æ€§

---

## ğŸ¯ é¢„æœŸæ•ˆæœä¸æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | å½“å‰çŠ¶æ€ | ç›®æ ‡çŠ¶æ€ | ä¸šç•Œæ ‡å‡† |
|---------|---------|---------|---------|
| **è¯­éŸ³è¯†åˆ«å»¶è¿Ÿ** | ä¸å¯ç”¨ | <300ms | <500ms |
| **è¯­éŸ³åˆæˆé¦–å“** | ä¸å¯ç”¨ | <500ms | <800ms |
| **å¯¹è¯ç†è§£å‡†ç¡®ç‡** | ä¸å¯ç”¨ | >90% | >85% |
| **å‘é‡æ£€ç´¢å»¶è¿Ÿ** | >1000ms | <50ms | <100ms |
| **ç³»ç»Ÿå¯ç”¨æ€§** | ä¸ç¨³å®š | 99.9% | 99.5% |
| **å¹¶å‘æ”¯æŒ** | <100 | 1000+ | 500+ |

### ä¸šåŠ¡ä»·å€¼

âœ… **çœŸæ­£çš„å®æ—¶è¯­éŸ³äº¤äº’**: åŸºäºOpenAI Whisperçš„å¤šè¯­è¨€è¯†åˆ«  
âœ… **æ™ºèƒ½å¯¹è¯ç®¡ç†**: åŸºäºRasaçš„ä¼ä¸šçº§å¯¹è¯AIæ¡†æ¶  
âœ… **é«˜æ€§èƒ½çŸ¥è¯†æ£€ç´¢**: åŸºäºFAISSçš„æ¯«ç§’çº§å‘é‡æœç´¢  
âœ… **å®Œæ•´å¯è§‚æµ‹æ€§**: åŸºäºPrometheusçš„å…¨é“¾è·¯ç›‘æ§  
âœ… **å¤šå¹³å°ä¸€è‡´ä½“éªŒ**: Web/Mobile/Desktopå…¨è¦†ç›–  

### ç«äº‰åŠ›æå‡

å®Œæˆæ‰€æœ‰åŠŸèƒ½åï¼ŒVoiceHelperå°†ä»å½“å‰çš„"æ¼”ç¤ºç‰ˆæœ¬"çœŸæ­£æå‡åˆ°**ä¼ä¸šçº§ç”Ÿäº§å°±ç»ª**çš„AIåŠ©æ‰‹å¹³å°ï¼Œåœ¨æŠ€æœ¯å…ˆè¿›æ€§ã€ç”¨æˆ·ä½“éªŒã€ç³»ç»Ÿç¨³å®šæ€§æ–¹é¢è¾¾åˆ°ä¸šç•Œå…ˆè¿›æ°´å¹³ã€‚

---

## ğŸ“‹ æ€»ç»“ä¸å»ºè®®

### é¡¹ç›®ç°çŠ¶è¯„ä¼°
VoiceHelperé¡¹ç›®å…·æœ‰**è‰¯å¥½çš„æ¶æ„åŸºç¡€**ï¼Œä½†å­˜åœ¨**ä¸¥é‡çš„åŠŸèƒ½å®ç°ç¼ºå¤±**ã€‚å£°ç§°çš„"ä¸šç•Œç¬¬ä¸€æ¢¯é˜Ÿ"ä¸å®é™…çŠ¶æ€å·®è·å·¨å¤§ï¼Œéœ€è¦å¤§é‡å¼€å‘å·¥ä½œæ‰èƒ½è¾¾åˆ°å¯ç”¨æ°´å¹³ã€‚

### å…³é”®æˆåŠŸå› ç´ 
1. **æŠ€æœ¯é€‰å‹æ­£ç¡®**: é‡‡ç”¨OpenAI Whisperã€Rasaã€FAISSç­‰æˆç†Ÿå¼€æºæ–¹æ¡ˆ
2. **åˆ†é˜¶æ®µå®æ–½**: P0ä¿®å¤åŸºç¡€è®¾æ–½ï¼ŒP1å¢å¼ºæ ¸å¿ƒèƒ½åŠ›ï¼ŒP2æ‰©å±•å¹³å°åŠŸèƒ½
3. **è´¨é‡ä¼˜å…ˆ**: å»ºç«‹å®Œæ•´çš„æµ‹è¯•å’Œç›‘æ§ä½“ç³»ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§
4. **å›¢é˜Ÿèƒ½åŠ›**: éœ€è¦å…·å¤‡å…¨æ ˆ+AIç®—æ³•çš„å¤åˆå‹å¼€å‘å›¢é˜Ÿ

### é£é™©ä¸æŒ‘æˆ˜
- **æŠ€æœ¯å€ºåŠ¡é‡**: éœ€è¦é‡å†™å¤§é‡æ ¸å¿ƒåŠŸèƒ½
- **å¼€å‘å‘¨æœŸé•¿**: å®Œæ•´å®ç°éœ€è¦4-6ä¸ªæœˆ
- **èµ„æºéœ€æ±‚å¤§**: éœ€è¦6äººå›¢é˜ŸæŒç»­æŠ•å…¥
- **æŠ€æœ¯å¤æ‚åº¦é«˜**: æ¶‰åŠè¯­éŸ³å¤„ç†ã€AIç®—æ³•ã€åˆ†å¸ƒå¼ç³»ç»Ÿç­‰å¤šä¸ªé¢†åŸŸ

### æœ€ç»ˆå»ºè®®
å»ºè®®é‡‡ç”¨**æ¸è¿›å¼å¼€å‘ç­–ç•¥**ï¼Œä¼˜å…ˆä¿®å¤P0çº§åˆ«çš„åŸºç¡€è®¾æ–½é—®é¢˜ï¼Œç„¶åé€æ­¥å®Œå–„æ ¸å¿ƒAIèƒ½åŠ›ï¼Œæœ€åæ‰©å±•å¤šå¹³å°åŠŸèƒ½ã€‚åªæœ‰è¿™æ ·ï¼Œæ‰èƒ½å°†VoiceHelperä»å½“å‰çš„"æ¦‚å¿µéªŒè¯"é˜¶æ®µçœŸæ­£æå‡åˆ°"ç”Ÿäº§å¯ç”¨"çš„ä¼ä¸šçº§AIåŠ©æ‰‹å¹³å°æ°´å¹³ã€‚

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´: 2025å¹´9æœˆ23æ—¥*  
*åŸºäºå¼€æºæŠ€æœ¯æ ˆ: OpenAI Whisper + Rasa + FAISS + Edge-TTS + Prometheus*  
*é¢„è®¡å®æ–½å‘¨æœŸ: 16-20å‘¨*

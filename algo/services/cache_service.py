"""
é›†æˆç¼“å­˜æœåŠ¡

æ•´åˆè¯­ä¹‰ç¼“å­˜ã€çƒ­ç‚¹ç¼“å­˜å’Œé¢„çƒ­ç³»ç»Ÿ
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Callable, Awaitable
from dataclasses import dataclass
import json

from ..core.semantic_cache import SemanticCache
from ..core.hotspot_cache import AdaptiveHotspotCache
from ..core.cache_prewarming import SmartPrewarmer

logger = logging.getLogger(__name__)


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    # è¯­ä¹‰ç¼“å­˜é…ç½®
    semantic_cache_size: int = 1000
    semantic_similarity_threshold: float = 0.85
    semantic_ttl: float = 3600.0
    
    # çƒ­ç‚¹ç¼“å­˜é…ç½®
    hotspot_initial_size: int = 50
    hotspot_max_size: int = 200
    hotspot_detection_interval: float = 60.0
    
    # é¢„çƒ­é…ç½®
    enable_prewarming: bool = True
    prewarm_workers: int = 3
    prewarm_interval: float = 300.0
    
    # æ€§èƒ½é…ç½®
    enable_performance_monitoring: bool = True
    stats_collection_interval: float = 30.0


class IntegratedCacheService:
    """é›†æˆç¼“å­˜æœåŠ¡"""
    
    def __init__(
        self,
        processing_func: Callable[[str, str, Dict[str, Any]], Awaitable[Any]],
        config: Optional[CacheConfig] = None
    ):
        self.processing_func = processing_func
        self.config = config or CacheConfig()
        
        # ç¼“å­˜å±‚
        self.semantic_cache = SemanticCache(
            max_size=self.config.semantic_cache_size,
            similarity_threshold=self.config.semantic_similarity_threshold,
            default_ttl=self.config.semantic_ttl
        )
        
        self.hotspot_cache = AdaptiveHotspotCache(
            initial_size=self.config.hotspot_initial_size,
            max_size=self.config.hotspot_max_size
        )
        
        # é¢„çƒ­ç³»ç»Ÿ
        self.prewarmer: Optional[SmartPrewarmer] = None
        if self.config.enable_prewarming:
            self.prewarmer = SmartPrewarmer(
                processing_func=self._process_with_caching,
                cache_put_func=self._put_to_all_caches,
                cache_get_func=self._get_from_semantic_cache,
                max_concurrent_tasks=self.config.prewarm_workers
            )
        
        # ç»Ÿè®¡å’Œç›‘æ§
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'hotspot_hits': 0,
            'semantic_hits': 0,
            'processing_time_saved': 0.0,
            'avg_response_time': 0.0
        }
        
        self._running = False
        self._monitoring_task: Optional[asyncio.Task] = None
    
    async def start(self):
        """å¯åŠ¨ç¼“å­˜æœåŠ¡"""
        if not self._running:
            self._running = True
            
            # å¯åŠ¨çƒ­ç‚¹ç¼“å­˜
            await self.hotspot_cache.start()
            
            # å¯åŠ¨é¢„çƒ­ç³»ç»Ÿ
            if self.prewarmer:
                await self.prewarmer.start()
            
            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            if self.config.enable_performance_monitoring:
                self._monitoring_task = asyncio.create_task(self._monitoring_loop())
            
            logger.info("Integrated cache service started")
    
    async def stop(self):
        """åœæ­¢ç¼“å­˜æœåŠ¡"""
        if self._running:
            self._running = False
            
            # åœæ­¢çƒ­ç‚¹ç¼“å­˜
            await self.hotspot_cache.stop()
            
            # åœæ­¢é¢„çƒ­ç³»ç»Ÿ
            if self.prewarmer:
                await self.prewarmer.stop()
            
            # åœæ­¢ç›‘æ§
            if self._monitoring_task:
                self._monitoring_task.cancel()
                try:
                    await self._monitoring_task
                except asyncio.CancelledError:
                    pass
            
            logger.info("Integrated cache service stopped")
    
    async def get_response(
        self,
        content: str,
        model: str,
        parameters: Optional[Dict[str, Any]] = None,
        use_cache: bool = True
    ) -> Any:
        """è·å–å“åº” (å¸¦ç¼“å­˜)"""
        if parameters is None:
            parameters = {}
        
        self.stats['total_requests'] += 1
        start_time = time.time()
        
        if use_cache:
            # 1. å°è¯•çƒ­ç‚¹ç¼“å­˜ (æœ€å¿«)
            cache_key = self._generate_cache_key(content, model, parameters)
            hotspot_response = await self.hotspot_cache.get(cache_key)
            
            if hotspot_response is not None:
                self.stats['cache_hits'] += 1
                self.stats['hotspot_hits'] += 1
                
                # è®°å½•æŸ¥è¯¢ç”¨äºé¢„çƒ­
                if self.prewarmer:
                    await self.prewarmer.record_query_and_response(content, True)
                
                response_time = time.time() - start_time
                self._update_response_time(response_time)
                
                logger.debug(f"Hotspot cache hit: {cache_key}")
                return hotspot_response
            
            # 2. å°è¯•è¯­ä¹‰ç¼“å­˜
            semantic_response = await self.semantic_cache.get(content, model, parameters)
            
            if semantic_response is not None:
                self.stats['cache_hits'] += 1
                self.stats['semantic_hits'] += 1
                
                # æå‡åˆ°çƒ­ç‚¹ç¼“å­˜
                await self.hotspot_cache.hotspot_cache.promote_to_hotspot(cache_key, semantic_response)
                
                # è®°å½•æŸ¥è¯¢
                if self.prewarmer:
                    await self.prewarmer.record_query_and_response(content, True)
                
                response_time = time.time() - start_time
                self._update_response_time(response_time)
                
                logger.debug(f"Semantic cache hit: {cache_key}")
                return semantic_response
        
        # 3. ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨å¤„ç†å‡½æ•°
        self.stats['cache_misses'] += 1
        
        # è®°å½•å¤„ç†å¼€å§‹æ—¶é—´
        processing_start = time.time()
        
        try:
            response = await self.processing_func(content, model, parameters)
            
            processing_time = time.time() - processing_start
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            if use_cache:
                await self._put_to_all_caches(content, model, response, parameters)
            
            # è®°å½•æŸ¥è¯¢
            if self.prewarmer:
                await self.prewarmer.record_query_and_response(content, False)
            
            total_response_time = time.time() - start_time
            self._update_response_time(total_response_time)
            
            logger.debug(f"Cache miss, processed in {processing_time:.3f}s: {content[:50]}...")
            return response
            
        except Exception as e:
            logger.error(f"Processing error: {e}")
            raise
    
    async def _process_with_caching(
        self,
        content: str,
        model: str,
        parameters: Dict[str, Any]
    ) -> Any:
        """å¸¦ç¼“å­˜çš„å¤„ç† (ç”¨äºé¢„çƒ­)"""
        return await self.get_response(content, model, parameters, use_cache=False)
    
    async def _put_to_all_caches(
        self,
        content: str,
        model: str,
        response: Any,
        parameters: Dict[str, Any]
    ):
        """å­˜å‚¨åˆ°æ‰€æœ‰ç¼“å­˜å±‚"""
        # å­˜å‚¨åˆ°è¯­ä¹‰ç¼“å­˜
        await self.semantic_cache.put(content, model, response, parameters)
        
        # æ ¹æ®è®¿é—®æ¨¡å¼å†³å®šæ˜¯å¦å­˜å‚¨åˆ°çƒ­ç‚¹ç¼“å­˜
        cache_key = self._generate_cache_key(content, model, parameters)
        access_pattern = self.hotspot_cache.hotspot_cache.detector.get_access_pattern(cache_key)
        
        if access_pattern and access_pattern.access_count >= 2:
            # å¤šæ¬¡è®¿é—®çš„æ•°æ®å­˜å‚¨åˆ°çƒ­ç‚¹ç¼“å­˜
            await self.hotspot_cache.hotspot_cache.promote_to_hotspot(cache_key, response)
    
    async def _get_from_semantic_cache(
        self,
        content: str,
        model: str,
        parameters: Dict[str, Any]
    ) -> Optional[Any]:
        """ä»è¯­ä¹‰ç¼“å­˜è·å– (ç”¨äºé¢„çƒ­)"""
        return await self.semantic_cache.get(content, model, parameters)
    
    def _generate_cache_key(self, content: str, model: str, parameters: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        key_data = f"{content}:{model}:{json.dumps(parameters, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _update_response_time(self, response_time: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.stats['total_requests'] == 1:
            self.stats['avg_response_time'] = response_time
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            alpha = 0.1
            self.stats['avg_response_time'] = (
                alpha * response_time + 
                (1 - alpha) * self.stats['avg_response_time']
            )
    
    async def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("Cache service monitoring started")
        
        while self._running:
            try:
                await asyncio.sleep(self.config.stats_collection_interval)
                
                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                await self._collect_performance_stats()
                
                # æ‰§è¡Œè‡ªé€‚åº”ä¼˜åŒ–
                await self._adaptive_optimization()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitoring error: {e}")
        
        logger.info("Cache service monitoring stopped")
    
    async def _collect_performance_stats(self):
        """æ”¶é›†æ€§èƒ½ç»Ÿè®¡"""
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        hit_rate = (
            self.stats['cache_hits'] / self.stats['total_requests'] 
            if self.stats['total_requests'] > 0 else 0.0
        )
        
        # è®¡ç®—æ—¶é—´èŠ‚çœ
        if self.stats['cache_hits'] > 0:
            # å‡è®¾ç¼“å­˜å‘½ä¸­èŠ‚çœ80%çš„å¤„ç†æ—¶é—´
            estimated_processing_time = self.stats['avg_response_time'] / 0.2  # å‡è®¾ç¼“å­˜å“åº”æ—¶é—´æ˜¯å¤„ç†æ—¶é—´çš„20%
            self.stats['processing_time_saved'] = (
                self.stats['cache_hits'] * estimated_processing_time * 0.8
            )
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        if self.stats['total_requests'] % 100 == 0:  # æ¯100ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡
            logger.info(
                f"Cache performance: {hit_rate:.2%} hit rate, "
                f"{self.stats['avg_response_time']:.3f}s avg response time, "
                f"{self.stats['processing_time_saved']:.1f}s time saved"
            )
    
    async def _adaptive_optimization(self):
        """è‡ªé€‚åº”ä¼˜åŒ–"""
        # è·å–å„ç¼“å­˜å±‚çš„ç»Ÿè®¡ä¿¡æ¯
        semantic_stats = self.semantic_cache.get_stats()
        hotspot_stats = self.hotspot_cache.get_stats()
        
        # åŠ¨æ€è°ƒæ•´è¯­ä¹‰ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼
        if semantic_stats['hit_rate'] < 0.3:
            # å‘½ä¸­ç‡ä½ï¼Œé™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
            self.semantic_cache.similarity_threshold = max(0.75, self.semantic_cache.similarity_threshold - 0.05)
        elif semantic_stats['hit_rate'] > 0.8:
            # å‘½ä¸­ç‡é«˜ï¼Œæé«˜ç›¸ä¼¼åº¦é˜ˆå€¼
            self.semantic_cache.similarity_threshold = min(0.95, self.semantic_cache.similarity_threshold + 0.02)
        
        # æ‰§è¡Œè‡ªé€‚åº”é¢„çƒ­
        if self.prewarmer:
            await self.prewarmer.adaptive_prewarm()
    
    async def invalidate_cache(self, pattern: Optional[str] = None):
        """å¤±æ•ˆç¼“å­˜"""
        self.semantic_cache.invalidate(pattern)
        
        if pattern:
            # çƒ­ç‚¹ç¼“å­˜çš„ç®€å•å¤±æ•ˆ (å®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘)
            keys_to_remove = []
            for key in self.hotspot_cache.hotspot_cache.cache.keys():
                if pattern in key:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                if key in self.hotspot_cache.hotspot_cache.cache:
                    del self.hotspot_cache.hotspot_cache.cache[key]
        
        logger.info(f"Cache invalidated with pattern: {pattern}")
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        # åŸºç¡€ç»Ÿè®¡
        hit_rate = (
            self.stats['cache_hits'] / self.stats['total_requests'] 
            if self.stats['total_requests'] > 0 else 0.0
        )
        
        miss_rate = 1.0 - hit_rate
        
        # æ€§èƒ½æå‡è®¡ç®—
        if self.stats['cache_hits'] > 0 and self.stats['avg_response_time'] > 0:
            # ä¼°ç®—æ— ç¼“å­˜æ—¶çš„å“åº”æ—¶é—´
            estimated_no_cache_time = self.stats['avg_response_time'] / (1 - hit_rate * 0.8)
            performance_improvement = (
                (estimated_no_cache_time - self.stats['avg_response_time']) / 
                estimated_no_cache_time
            )
        else:
            performance_improvement = 0.0
        
        comprehensive_stats = {
            # åŸºç¡€ç»Ÿè®¡
            'service_stats': {
                **self.stats,
                'hit_rate': hit_rate,
                'miss_rate': miss_rate,
                'performance_improvement': performance_improvement
            },
            
            # å„ç¼“å­˜å±‚ç»Ÿè®¡
            'semantic_cache_stats': self.semantic_cache.get_stats(),
            'hotspot_cache_stats': self.hotspot_cache.get_stats(),
            
            # é…ç½®ä¿¡æ¯
            'config': {
                'semantic_cache_size': self.config.semantic_cache_size,
                'semantic_similarity_threshold': self.semantic_cache.similarity_threshold,
                'hotspot_max_size': self.config.hotspot_max_size,
                'prewarming_enabled': self.config.enable_prewarming
            }
        }
        
        # é¢„çƒ­ç»Ÿè®¡
        if self.prewarmer:
            comprehensive_stats['prewarmer_stats'] = self.prewarmer.get_stats()
        
        return comprehensive_stats
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
            test_content = "Health check test query"
            test_model = "gpt-3.5-turbo"
            test_params = {"temperature": 0.7}
            
            start_time = time.time()
            
            # æµ‹è¯•å­˜å‚¨
            await self.semantic_cache.put(
                test_content, 
                test_model, 
                "Health check response", 
                test_params
            )
            
            # æµ‹è¯•è·å–
            response = await self.semantic_cache.get(test_content, test_model, test_params)
            
            response_time = time.time() - start_time
            
            return {
                'status': 'healthy',
                'cache_functional': response is not None,
                'response_time': response_time,
                'running': self._running,
                'hotspot_cache_running': self.hotspot_cache.hotspot_cache._running if hasattr(self.hotspot_cache.hotspot_cache, '_running') else True,
                'prewarmer_running': self.prewarmer is not None
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'running': self._running
            }


# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½æµ‹è¯•
async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ¨¡æ‹Ÿå¤„ç†å‡½æ•°
    async def mock_processing_func(content: str, model: str, parameters: Dict[str, Any]) -> str:
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.8)
        return f"AI response to: {content}"
    
    # åˆ›å»ºç¼“å­˜é…ç½®
    config = CacheConfig(
        semantic_cache_size=500,
        semantic_similarity_threshold=0.85,
        hotspot_initial_size=20,
        hotspot_max_size=100,
        enable_prewarming=True,
        prewarm_workers=2
    )
    
    # åˆ›å»ºç¼“å­˜æœåŠ¡
    cache_service = IntegratedCacheService(
        processing_func=mock_processing_func,
        config=config
    )
    
    await cache_service.start()
    
    try:
        print("ğŸš€ Testing integrated cache service...")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "What is artificial intelligence?",
            "How does machine learning work?",
            "Explain deep learning",
            "What is AI?",  # åº”è¯¥åŒ¹é…ç¬¬ä¸€ä¸ªæŸ¥è¯¢
            "Tell me about machine learning",  # åº”è¯¥åŒ¹é…ç¬¬äºŒä¸ªæŸ¥è¯¢
            "What is artificial intelligence?",  # ç²¾ç¡®åŒ¹é…
        ]
        
        total_start_time = time.time()
        
        for i, query in enumerate(test_queries):
            print(f"\n--- Query {i+1}: {query} ---")
            
            start_time = time.time()
            response = await cache_service.get_response(
                content=query,
                model="gpt-3.5-turbo",
                parameters={"temperature": 0.7}
            )
            response_time = time.time() - start_time
            
            print(f"Response time: {response_time:.3f}s")
            print(f"Response: {response[:100]}...")
        
        total_time = time.time() - total_start_time
        print(f"\nTotal test time: {total_time:.3f}s")
        
        # ç­‰å¾…é¢„çƒ­ä»»åŠ¡æ‰§è¡Œ
        print("\nâ³ Waiting for prewarming tasks...")
        await asyncio.sleep(3)
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = cache_service.get_comprehensive_stats()
        print(f"\nğŸ“Š Cache Service Statistics:")
        print(f"Hit Rate: {stats['service_stats']['hit_rate']:.2%}")
        print(f"Performance Improvement: {stats['service_stats']['performance_improvement']:.2%}")
        print(f"Average Response Time: {stats['service_stats']['avg_response_time']:.3f}s")
        print(f"Hotspot Hits: {stats['service_stats']['hotspot_hits']}")
        print(f"Semantic Hits: {stats['service_stats']['semantic_hits']}")
        
        # å¥åº·æ£€æŸ¥
        health = await cache_service.health_check()
        print(f"\nğŸ¥ Health Check: {health['status']}")
        
        print(f"\nâœ… Cache service test completed!")
        
    finally:
        await cache_service.stop()


if __name__ == "__main__":
    asyncio.run(example_usage())

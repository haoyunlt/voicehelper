"""
VoiceHelper é«˜å¹¶å‘ç³»ç»Ÿ
æ”¯æŒ100ä¸‡+ QPSçº§åˆ«ï¼Œå¯¹æ ‡ChatGPT-4o
å®ç°åˆ†å¸ƒå¼æ¶æ„ã€æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨æ‰©ç¼©å®¹
"""

import asyncio
import time
import logging
import json
import random
import hashlib
from typing import Dict, List, Optional, Tuple, Any, Callable, AsyncIterator
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, deque
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import weakref

logger = logging.getLogger(__name__)

class ServiceStatus(Enum):
    """æœåŠ¡çŠ¶æ€"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    MAINTENANCE = "maintenance"

class LoadBalancingStrategy(Enum):
    """è´Ÿè½½å‡è¡¡ç­–ç•¥"""
    ROUND_ROBIN = "round_robin"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    LEAST_CONNECTIONS = "least_connections"
    LEAST_RESPONSE_TIME = "least_response_time"
    CONSISTENT_HASH = "consistent_hash"
    ADAPTIVE = "adaptive"

@dataclass
class ServiceInstance:
    """æœåŠ¡å®ä¾‹"""
    id: str
    host: str
    port: int
    weight: float = 1.0
    status: ServiceStatus = ServiceStatus.HEALTHY
    current_connections: int = 0
    total_requests: int = 0
    total_errors: int = 0
    average_response_time: float = 0.0
    last_health_check: float = field(default_factory=time.time)
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    
    @property
    def error_rate(self) -> float:
        """é”™è¯¯ç‡"""
        if self.total_requests == 0:
            return 0.0
        return self.total_errors / self.total_requests
    
    @property
    def health_score(self) -> float:
        """å¥åº·è¯„åˆ†"""
        if self.status == ServiceStatus.UNHEALTHY:
            return 0.0
        elif self.status == ServiceStatus.MAINTENANCE:
            return 0.1
        elif self.status == ServiceStatus.DEGRADED:
            return 0.5
        
        # åŸºäºå¤šä¸ªæŒ‡æ ‡è®¡ç®—å¥åº·è¯„åˆ†
        score = 1.0
        
        # é”™è¯¯ç‡å½±å“
        score -= min(self.error_rate * 2, 0.5)
        
        # å“åº”æ—¶é—´å½±å“
        if self.average_response_time > 1000:  # 1ç§’
            score -= 0.3
        elif self.average_response_time > 500:  # 500ms
            score -= 0.1
        
        # èµ„æºä½¿ç”¨ç‡å½±å“
        if self.cpu_usage > 0.9:
            score -= 0.2
        elif self.cpu_usage > 0.7:
            score -= 0.1
        
        if self.memory_usage > 0.9:
            score -= 0.2
        elif self.memory_usage > 0.8:
            score -= 0.1
        
        return max(score, 0.0)

@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡"""
    request_id: str
    start_time: float
    end_time: Optional[float] = None
    service_instance: Optional[str] = None
    status_code: int = 200
    response_size: int = 0
    error_message: Optional[str] = None
    
    @property
    def duration_ms(self) -> float:
        """è¯·æ±‚æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        if self.end_time is None:
            return (time.time() - self.start_time) * 1000
        return (self.end_time - self.start_time) * 1000
    
    @property
    def is_success(self) -> bool:
        """æ˜¯å¦æˆåŠŸ"""
        return 200 <= self.status_code < 400

@dataclass
class ConcurrencyStats:
    """å¹¶å‘ç»Ÿè®¡"""
    current_qps: float = 0.0
    peak_qps: float = 0.0
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_response_time: float = 0.0
    p95_response_time: float = 0.0
    p99_response_time: float = 0.0
    active_connections: int = 0
    queue_length: int = 0
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 1.0
        return self.successful_requests / self.total_requests
    
    @property
    def error_rate(self) -> float:
        """é”™è¯¯ç‡"""
        return 1.0 - self.success_rate

class CircuitBreaker:
    """ç†”æ–­å™¨"""
    
    def __init__(self, 
                 failure_threshold: int = 5,
                 recovery_timeout: float = 60.0,
                 success_threshold: int = 3):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.success_threshold = success_threshold
        
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = 0.0
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
        
    def call(self, func: Callable, *args, **kwargs):
        """æ‰§è¡Œå‡½æ•°è°ƒç”¨"""
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "HALF_OPEN"
                self.success_count = 0
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        """æˆåŠŸå›è°ƒ"""
        if self.state == "HALF_OPEN":
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                self.state = "CLOSED"
                self.failure_count = 0
        else:
            self.failure_count = 0
    
    def on_failure(self):
        """å¤±è´¥å›è°ƒ"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"

class RateLimiter:
    """é™æµå™¨"""
    
    def __init__(self, max_requests: int, time_window: float = 1.0):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()
        self.lock = threading.Lock()
    
    def is_allowed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚"""
        with self.lock:
            now = time.time()
            
            # æ¸…ç†è¿‡æœŸè¯·æ±‚
            while self.requests and now - self.requests[0] > self.time_window:
                self.requests.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.requests) >= self.max_requests:
                return False
            
            # è®°å½•æ–°è¯·æ±‚
            self.requests.append(now)
            return True

class LoadBalancer:
    """æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨"""
    
    def __init__(self, strategy: LoadBalancingStrategy = LoadBalancingStrategy.ADAPTIVE):
        self.strategy = strategy
        self.instances: List[ServiceInstance] = []
        self.current_index = 0
        self.connection_counts = defaultdict(int)
        self.response_times = defaultdict(list)
        
    def add_instance(self, instance: ServiceInstance):
        """æ·»åŠ æœåŠ¡å®ä¾‹"""
        self.instances.append(instance)
        logger.info(f"Added service instance: {instance.id}")
    
    def remove_instance(self, instance_id: str):
        """ç§»é™¤æœåŠ¡å®ä¾‹"""
        self.instances = [inst for inst in self.instances if inst.id != instance_id]
        logger.info(f"Removed service instance: {instance_id}")
    
    def select_instance(self, request_hash: Optional[str] = None) -> Optional[ServiceInstance]:
        """é€‰æ‹©æœåŠ¡å®ä¾‹"""
        healthy_instances = [inst for inst in self.instances 
                           if inst.status in [ServiceStatus.HEALTHY, ServiceStatus.DEGRADED]]
        
        if not healthy_instances:
            return None
        
        if self.strategy == LoadBalancingStrategy.ROUND_ROBIN:
            return self._round_robin(healthy_instances)
        elif self.strategy == LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN:
            return self._weighted_round_robin(healthy_instances)
        elif self.strategy == LoadBalancingStrategy.LEAST_CONNECTIONS:
            return self._least_connections(healthy_instances)
        elif self.strategy == LoadBalancingStrategy.LEAST_RESPONSE_TIME:
            return self._least_response_time(healthy_instances)
        elif self.strategy == LoadBalancingStrategy.CONSISTENT_HASH:
            return self._consistent_hash(healthy_instances, request_hash)
        elif self.strategy == LoadBalancingStrategy.ADAPTIVE:
            return self._adaptive_selection(healthy_instances)
        
        return healthy_instances[0]  # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª
    
    def _round_robin(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """è½®è¯¢ç®—æ³•"""
        instance = instances[self.current_index % len(instances)]
        self.current_index += 1
        return instance
    
    def _weighted_round_robin(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """åŠ æƒè½®è¯¢ç®—æ³•"""
        total_weight = sum(inst.weight for inst in instances)
        if total_weight == 0:
            return instances[0]
        
        # ç®€åŒ–çš„åŠ æƒè½®è¯¢å®ç°
        target = random.uniform(0, total_weight)
        current_weight = 0
        
        for instance in instances:
            current_weight += instance.weight
            if current_weight >= target:
                return instance
        
        return instances[-1]
    
    def _least_connections(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """æœ€å°‘è¿æ¥ç®—æ³•"""
        return min(instances, key=lambda x: x.current_connections)
    
    def _least_response_time(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """æœ€çŸ­å“åº”æ—¶é—´ç®—æ³•"""
        return min(instances, key=lambda x: x.average_response_time)
    
    def _consistent_hash(self, instances: List[ServiceInstance], request_hash: Optional[str]) -> ServiceInstance:
        """ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•"""
        if not request_hash:
            return instances[0]
        
        # ç®€åŒ–çš„ä¸€è‡´æ€§å“ˆå¸Œå®ç°
        hash_value = int(hashlib.md5(request_hash.encode()).hexdigest(), 16)
        index = hash_value % len(instances)
        return instances[index]
    
    def _adaptive_selection(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """è‡ªé€‚åº”é€‰æ‹©ç®—æ³•"""
        # åŸºäºå¥åº·è¯„åˆ†çš„è‡ªé€‚åº”é€‰æ‹©
        scored_instances = [(inst, inst.health_score) for inst in instances]
        scored_instances.sort(key=lambda x: x[1], reverse=True)
        
        # ä½¿ç”¨åŠ æƒéšæœºé€‰æ‹©ï¼Œå¥åº·è¯„åˆ†é«˜çš„å®ä¾‹è¢«é€‰ä¸­æ¦‚ç‡æ›´å¤§
        total_score = sum(score for _, score in scored_instances)
        if total_score == 0:
            return instances[0]
        
        target = random.uniform(0, total_score)
        current_score = 0
        
        for instance, score in scored_instances:
            current_score += score
            if current_score >= target:
                return instance
        
        return scored_instances[0][0]

class ConnectionPool:
    """è¿æ¥æ± """
    
    def __init__(self, max_connections: int = 1000):
        self.max_connections = max_connections
        self.active_connections = 0
        self.connection_queue = asyncio.Queue(maxsize=max_connections)
        self.lock = asyncio.Lock()
    
    async def acquire_connection(self) -> bool:
        """è·å–è¿æ¥"""
        async with self.lock:
            if self.active_connections >= self.max_connections:
                return False
            
            self.active_connections += 1
            return True
    
    async def release_connection(self):
        """é‡Šæ”¾è¿æ¥"""
        async with self.lock:
            if self.active_connections > 0:
                self.active_connections -= 1

class HighConcurrencySystem:
    """é«˜å¹¶å‘ç³»ç»Ÿ"""
    
    def __init__(self, 
                 max_qps: int = 1000000,
                 max_connections: int = 100000,
                 worker_threads: int = None,
                 worker_processes: int = None):
        
        self.max_qps = max_qps
        self.max_connections = max_connections
        
        # å·¥ä½œçº¿ç¨‹å’Œè¿›ç¨‹æ± 
        self.worker_threads = worker_threads or min(32, (multiprocessing.cpu_count() or 1) + 4)
        self.worker_processes = worker_processes or multiprocessing.cpu_count() or 1
        
        self.thread_pool = ThreadPoolExecutor(max_workers=self.worker_threads)
        self.process_pool = ProcessPoolExecutor(max_workers=self.worker_processes)
        
        # æ ¸å¿ƒç»„ä»¶
        self.load_balancer = LoadBalancer(LoadBalancingStrategy.ADAPTIVE)
        self.connection_pool = ConnectionPool(max_connections)
        self.rate_limiter = RateLimiter(max_qps, 1.0)
        self.circuit_breakers = {}
        
        # ç»Ÿè®¡å’Œç›‘æ§
        self.stats = ConcurrencyStats()
        self.request_metrics = deque(maxlen=10000)  # ä¿ç•™æœ€è¿‘10000ä¸ªè¯·æ±‚çš„æŒ‡æ ‡
        self.qps_history = deque(maxlen=300)  # 5åˆ†é’Ÿçš„QPSå†å²ï¼ˆæ¯ç§’ä¸€ä¸ªç‚¹ï¼‰
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.background_tasks = []
        
        # æ€§èƒ½ä¼˜åŒ–
        self.request_cache = {}
        self.cache_ttl = 60  # ç¼“å­˜TTLï¼ˆç§’ï¼‰
        
        logger.info(f"HighConcurrencySystem initialized: {max_qps} QPS, {max_connections} connections")
    
    async def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        if self.is_running:
            return
        
        self.is_running = True
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self.background_tasks = [
            asyncio.create_task(self._stats_collector()),
            asyncio.create_task(self._health_checker()),
            asyncio.create_task(self._cache_cleaner()),
            asyncio.create_task(self._auto_scaler())
        ]
        
        logger.info("High concurrency system started")
    
    async def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # åœæ­¢åå°ä»»åŠ¡
        for task in self.background_tasks:
            task.cancel()
        
        await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        # å…³é—­çº¿ç¨‹æ± å’Œè¿›ç¨‹æ± 
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)
        
        logger.info("High concurrency system stopped")
    
    async def process_request(self, 
                            request_data: Any,
                            request_id: Optional[str] = None,
                            user_id: Optional[str] = None,
                            priority: int = 1) -> Dict[str, Any]:
        """å¤„ç†è¯·æ±‚"""
        
        # ç”Ÿæˆè¯·æ±‚ID
        if request_id is None:
            request_id = f"req_{int(time.time()*1000)}_{random.randint(1000, 9999)}"
        
        # é™æµæ£€æŸ¥
        if not self.rate_limiter.is_allowed():
            return {
                "request_id": request_id,
                "status": "error",
                "error": "Rate limit exceeded",
                "code": 429
            }
        
        # è·å–è¿æ¥
        if not await self.connection_pool.acquire_connection():
            return {
                "request_id": request_id,
                "status": "error",
                "error": "Connection pool exhausted",
                "code": 503
            }
        
        # åˆ›å»ºè¯·æ±‚æŒ‡æ ‡
        metrics = RequestMetrics(
            request_id=request_id,
            start_time=time.time()
        )
        
        try:
            # é€‰æ‹©æœåŠ¡å®ä¾‹
            instance = self.load_balancer.select_instance(user_id)
            if not instance:
                raise Exception("No healthy service instances available")
            
            metrics.service_instance = instance.id
            instance.current_connections += 1
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(request_data, user_id)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                metrics.end_time = time.time()
                metrics.status_code = 200
                self._record_metrics(metrics, instance)
                
                return {
                    "request_id": request_id,
                    "status": "success",
                    "data": cached_result,
                    "cached": True,
                    "processing_time": metrics.duration_ms
                }
            
            # è·å–ç†”æ–­å™¨
            circuit_breaker = self._get_circuit_breaker(instance.id)
            
            # å¤„ç†è¯·æ±‚
            try:
                result = await circuit_breaker.call(
                    self._execute_request,
                    request_data,
                    instance,
                    priority
                )
                
                # ç¼“å­˜ç»“æœ
                self._cache_result(cache_key, result)
                
                metrics.end_time = time.time()
                metrics.status_code = 200
                
                return {
                    "request_id": request_id,
                    "status": "success",
                    "data": result,
                    "cached": False,
                    "processing_time": metrics.duration_ms,
                    "instance_id": instance.id
                }
                
            except Exception as e:
                metrics.end_time = time.time()
                metrics.status_code = 500
                metrics.error_message = str(e)
                
                return {
                    "request_id": request_id,
                    "status": "error",
                    "error": str(e),
                    "code": 500,
                    "processing_time": metrics.duration_ms
                }
        
        finally:
            # é‡Šæ”¾è¿æ¥
            await self.connection_pool.release_connection()
            
            # æ›´æ–°å®ä¾‹è¿æ¥æ•°
            if metrics.service_instance:
                for inst in self.load_balancer.instances:
                    if inst.id == metrics.service_instance:
                        inst.current_connections = max(0, inst.current_connections - 1)
                        break
            
            # è®°å½•æŒ‡æ ‡
            self._record_metrics(metrics, instance if 'instance' in locals() else None)
    
    async def _execute_request(self, request_data: Any, instance: ServiceInstance, priority: int) -> Any:
        """æ‰§è¡Œå…·ä½“è¯·æ±‚"""
        # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
        processing_time = random.uniform(0.01, 0.1)  # 10-100ms
        
        # æ ¹æ®ä¼˜å…ˆçº§è°ƒæ•´å¤„ç†æ—¶é—´
        if priority > 5:
            processing_time *= 0.5  # é«˜ä¼˜å…ˆçº§è¯·æ±‚å¤„ç†æ›´å¿«
        elif priority < 3:
            processing_time *= 1.5  # ä½ä¼˜å…ˆçº§è¯·æ±‚å¤„ç†æ›´æ…¢
        
        await asyncio.sleep(processing_time)
        
        # æ¨¡æ‹Ÿå¶å‘é”™è¯¯
        if random.random() < 0.001:  # 0.1%çš„é”™è¯¯ç‡
            raise Exception("Simulated processing error")
        
        # è¿”å›æ¨¡æ‹Ÿç»“æœ
        return {
            "processed_data": f"Processed: {request_data}",
            "instance_id": instance.id,
            "processing_time": processing_time * 1000,
            "timestamp": time.time()
        }
    
    def _get_circuit_breaker(self, instance_id: str) -> CircuitBreaker:
        """è·å–ç†”æ–­å™¨"""
        if instance_id not in self.circuit_breakers:
            self.circuit_breakers[instance_id] = CircuitBreaker()
        return self.circuit_breakers[instance_id]
    
    def _generate_cache_key(self, request_data: Any, user_id: Optional[str]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        data_str = json.dumps(request_data, sort_keys=True) if isinstance(request_data, dict) else str(request_data)
        key_data = f"{data_str}:{user_id or 'anonymous'}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.request_cache:
            cached_data, timestamp = self.request_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                return cached_data
            else:
                del self.request_cache[cache_key]
        return None
    
    def _cache_result(self, cache_key: str, result: Any):
        """ç¼“å­˜ç»“æœ"""
        self.request_cache[cache_key] = (result, time.time())
    
    def _record_metrics(self, metrics: RequestMetrics, instance: Optional[ServiceInstance]):
        """è®°å½•æŒ‡æ ‡"""
        self.request_metrics.append(metrics)
        
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self.stats.total_requests += 1
        if metrics.is_success:
            self.stats.successful_requests += 1
        else:
            self.stats.failed_requests += 1
        
        # æ›´æ–°å®ä¾‹ç»Ÿè®¡
        if instance:
            instance.total_requests += 1
            if not metrics.is_success:
                instance.total_errors += 1
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            if instance.total_requests > 0:
                instance.average_response_time = (
                    (instance.average_response_time * (instance.total_requests - 1) + metrics.duration_ms) /
                    instance.total_requests
                )
    
    async def _stats_collector(self):
        """ç»Ÿè®¡æ”¶é›†å™¨"""
        last_request_count = 0
        
        while self.is_running:
            try:
                await asyncio.sleep(1)  # æ¯ç§’æ”¶é›†ä¸€æ¬¡
                
                # è®¡ç®—QPS
                current_requests = self.stats.total_requests
                current_qps = current_requests - last_request_count
                last_request_count = current_requests
                
                self.stats.current_qps = current_qps
                self.stats.peak_qps = max(self.stats.peak_qps, current_qps)
                self.qps_history.append(current_qps)
                
                # è®¡ç®—å“åº”æ—¶é—´ç™¾åˆ†ä½æ•°
                if self.request_metrics:
                    recent_metrics = list(self.request_metrics)[-1000:]  # æœ€è¿‘1000ä¸ªè¯·æ±‚
                    durations = [m.duration_ms for m in recent_metrics if m.end_time is not None]
                    
                    if durations:
                        durations.sort()
                        self.stats.average_response_time = sum(durations) / len(durations)
                        self.stats.p95_response_time = durations[int(len(durations) * 0.95)]
                        self.stats.p99_response_time = durations[int(len(durations) * 0.99)]
                
                # æ›´æ–°æ´»è·ƒè¿æ¥æ•°
                self.stats.active_connections = self.connection_pool.active_connections
                
            except Exception as e:
                logger.error(f"Stats collector error: {e}")
    
    async def _health_checker(self):
        """å¥åº·æ£€æŸ¥å™¨"""
        while self.is_running:
            try:
                await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
                for instance in self.load_balancer.instances:
                    # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥
                    await self._check_instance_health(instance)
                
            except Exception as e:
                logger.error(f"Health checker error: {e}")
    
    async def _check_instance_health(self, instance: ServiceInstance):
        """æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€"""
        try:
            # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥è¯·æ±‚
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ£€æŸ¥å»¶è¿Ÿ
            
            # æ›´æ–°èµ„æºä½¿ç”¨ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
            instance.cpu_usage = random.uniform(0.1, 0.9)
            instance.memory_usage = random.uniform(0.2, 0.8)
            instance.last_health_check = time.time()
            
            # æ ¹æ®é”™è¯¯ç‡å’Œå“åº”æ—¶é—´è°ƒæ•´çŠ¶æ€
            if instance.error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                instance.status = ServiceStatus.UNHEALTHY
            elif instance.error_rate > 0.05 or instance.average_response_time > 1000:
                instance.status = ServiceStatus.DEGRADED
            else:
                instance.status = ServiceStatus.HEALTHY
                
        except Exception as e:
            logger.error(f"Health check failed for instance {instance.id}: {e}")
            instance.status = ServiceStatus.UNHEALTHY
    
    async def _cache_cleaner(self):
        """ç¼“å­˜æ¸…ç†å™¨"""
        while self.is_running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
                current_time = time.time()
                expired_keys = []
                
                for key, (data, timestamp) in self.request_cache.items():
                    if current_time - timestamp > self.cache_ttl:
                        expired_keys.append(key)
                
                for key in expired_keys:
                    del self.request_cache[key]
                
                logger.debug(f"Cleaned {len(expired_keys)} expired cache entries")
                
            except Exception as e:
                logger.error(f"Cache cleaner error: {e}")
    
    async def _auto_scaler(self):
        """è‡ªåŠ¨æ‰©ç¼©å®¹"""
        while self.is_running:
            try:
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
                # åŸºäºQPSå†å²è¿›è¡Œæ‰©ç¼©å®¹å†³ç­–
                if len(self.qps_history) >= 10:
                    recent_qps = list(self.qps_history)[-10:]  # æœ€è¿‘10ç§’
                    avg_qps = sum(recent_qps) / len(recent_qps)
                    
                    healthy_instances = len([inst for inst in self.load_balancer.instances 
                                           if inst.status == ServiceStatus.HEALTHY])
                    
                    # æ‰©å®¹æ¡ä»¶ï¼šå¹³å‡QPSè¿‡é«˜ä¸”æˆåŠŸç‡ä¸‹é™
                    if (avg_qps > 1000 and healthy_instances < 10 and 
                        self.stats.success_rate < 0.95):
                        await self._scale_out()
                    
                    # ç¼©å®¹æ¡ä»¶ï¼šå¹³å‡QPSè¾ƒä½ä¸”å®ä¾‹æ•°é‡è¾ƒå¤š
                    elif avg_qps < 100 and healthy_instances > 2:
                        await self._scale_in()
                
            except Exception as e:
                logger.error(f"Auto scaler error: {e}")
    
    async def _scale_out(self):
        """æ‰©å®¹"""
        # æ¨¡æ‹Ÿæ·»åŠ æ–°å®ä¾‹
        new_instance = ServiceInstance(
            id=f"instance_{len(self.load_balancer.instances) + 1}",
            host=f"10.0.0.{len(self.load_balancer.instances) + 1}",
            port=8080,
            weight=1.0,
            status=ServiceStatus.HEALTHY
        )
        
        self.load_balancer.add_instance(new_instance)
        logger.info(f"Scaled out: added instance {new_instance.id}")
    
    async def _scale_in(self):
        """ç¼©å®¹"""
        # ç§»é™¤è´Ÿè½½æœ€ä½çš„å®ä¾‹
        if len(self.load_balancer.instances) > 1:
            instance_to_remove = min(self.load_balancer.instances, 
                                   key=lambda x: x.current_connections)
            
            self.load_balancer.remove_instance(instance_to_remove.id)
            logger.info(f"Scaled in: removed instance {instance_to_remove.id}")
    
    def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡"""
        return {
            "concurrency_stats": {
                "current_qps": self.stats.current_qps,
                "peak_qps": self.stats.peak_qps,
                "total_requests": self.stats.total_requests,
                "success_rate": self.stats.success_rate,
                "average_response_time": self.stats.average_response_time,
                "p95_response_time": self.stats.p95_response_time,
                "p99_response_time": self.stats.p99_response_time,
                "active_connections": self.stats.active_connections
            },
            "service_instances": [
                {
                    "id": inst.id,
                    "status": inst.status.value,
                    "health_score": inst.health_score,
                    "current_connections": inst.current_connections,
                    "total_requests": inst.total_requests,
                    "error_rate": inst.error_rate,
                    "average_response_time": inst.average_response_time,
                    "cpu_usage": inst.cpu_usage,
                    "memory_usage": inst.memory_usage
                }
                for inst in self.load_balancer.instances
            ],
            "cache_stats": {
                "cache_size": len(self.request_cache),
                "cache_hit_rate": 0.0  # ç®€åŒ–å®ç°
            },
            "system_config": {
                "max_qps": self.max_qps,
                "max_connections": self.max_connections,
                "worker_threads": self.worker_threads,
                "worker_processes": self.worker_processes
            }
        }

# å…¨å±€å®ä¾‹
high_concurrency_system = HighConcurrencySystem(
    max_qps=1000000,
    max_connections=100000
)

async def process_high_concurrency_request(
    request_data: Any,
    request_id: Optional[str] = None,
    user_id: Optional[str] = None,
    priority: int = 1
) -> Dict[str, Any]:
    """é«˜å¹¶å‘è¯·æ±‚å¤„ç†ä¾¿æ·å‡½æ•°"""
    return await high_concurrency_system.process_request(
        request_data=request_data,
        request_id=request_id,
        user_id=user_id,
        priority=priority
    )

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test_high_concurrency():
        print("ğŸš€ æµ‹è¯•é«˜å¹¶å‘ç³»ç»Ÿ")
        print("=" * 50)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        system = high_concurrency_system
        
        # æ·»åŠ ä¸€äº›æœåŠ¡å®ä¾‹
        for i in range(3):
            instance = ServiceInstance(
                id=f"test_instance_{i+1}",
                host=f"10.0.0.{i+1}",
                port=8080,
                weight=1.0
            )
            system.load_balancer.add_instance(instance)
        
        # å¯åŠ¨ç³»ç»Ÿ
        await system.start()
        
        print(f"ç³»ç»Ÿé…ç½®:")
        print(f"  æœ€å¤§QPS: {system.max_qps:,}")
        print(f"  æœ€å¤§è¿æ¥æ•°: {system.max_connections:,}")
        print(f"  å·¥ä½œçº¿ç¨‹: {system.worker_threads}")
        print(f"  æœåŠ¡å®ä¾‹: {len(system.load_balancer.instances)}")
        
        # å¹¶å‘æµ‹è¯•
        print(f"\nğŸ”¥ å¹¶å‘å‹åŠ›æµ‹è¯•:")
        
        # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
        test_requests = []
        for i in range(1000):  # 1000ä¸ªå¹¶å‘è¯·æ±‚
            test_requests.append({
                "request_data": f"test_request_{i}",
                "user_id": f"user_{i % 100}",  # 100ä¸ªä¸åŒç”¨æˆ·
                "priority": random.randint(1, 10)
            })
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        
        tasks = []
        for req in test_requests:
            task = process_high_concurrency_request(
                request_data=req["request_data"],
                user_id=req["user_id"],
                priority=req["priority"]
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # åˆ†æç»“æœ
        successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "success")
        failed_requests = len(results) - successful_requests
        actual_qps = len(results) / duration
        
        print(f"  æµ‹è¯•æ—¶é•¿: {duration:.2f}s")
        print(f"  æ€»è¯·æ±‚æ•°: {len(results)}")
        print(f"  æˆåŠŸè¯·æ±‚: {successful_requests}")
        print(f"  å¤±è´¥è¯·æ±‚: {failed_requests}")
        print(f"  æˆåŠŸç‡: {successful_requests/len(results):.2%}")
        print(f"  å®é™…QPS: {actual_qps:.0f}")
        
        # ç­‰å¾…ç»Ÿè®¡æ›´æ–°
        await asyncio.sleep(2)
        
        # è·å–ç³»ç»Ÿç»Ÿè®¡
        stats = system.get_system_stats()
        
        print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
        concurrency_stats = stats["concurrency_stats"]
        print(f"  å½“å‰QPS: {concurrency_stats['current_qps']:.0f}")
        print(f"  å³°å€¼QPS: {concurrency_stats['peak_qps']:.0f}")
        print(f"  æ€»è¯·æ±‚æ•°: {concurrency_stats['total_requests']}")
        print(f"  æˆåŠŸç‡: {concurrency_stats['success_rate']:.2%}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {concurrency_stats['average_response_time']:.2f}ms")
        print(f"  P95å“åº”æ—¶é—´: {concurrency_stats['p95_response_time']:.2f}ms")
        print(f"  æ´»è·ƒè¿æ¥: {concurrency_stats['active_connections']}")
        
        print(f"\nğŸ¥ æœåŠ¡å®ä¾‹çŠ¶æ€:")
        for inst_stat in stats["service_instances"]:
            print(f"  {inst_stat['id']}: {inst_stat['status']} "
                  f"(å¥åº·åº¦: {inst_stat['health_score']:.2f}, "
                  f"è¿æ¥: {inst_stat['current_connections']}, "
                  f"è¯·æ±‚: {inst_stat['total_requests']})")
        
        # åœæ­¢ç³»ç»Ÿ
        await system.stop()
        
        # è¯„ä¼°æµ‹è¯•ç»“æœ
        success_criteria = [
            successful_requests >= len(results) * 0.95,  # 95%æˆåŠŸç‡
            actual_qps >= 100,  # è‡³å°‘100 QPS
            concurrency_stats['average_response_time'] < 200,  # å¹³å‡å“åº”æ—¶é—´å°äº200ms
            len(stats["service_instances"]) >= 3,  # è‡³å°‘3ä¸ªå®ä¾‹
            all(inst['health_score'] > 0 for inst in stats["service_instances"])  # æ‰€æœ‰å®ä¾‹å¥åº·
        ]
        
        success_rate = sum(success_criteria) / len(success_criteria)
        print(f"\nğŸ¯ æµ‹è¯•æˆåŠŸç‡: {success_rate:.1%}")
        
        return success_rate >= 0.8
    
    # è¿è¡Œæµ‹è¯•
    import asyncio
    success = asyncio.run(test_high_concurrency())
    print(f"\nğŸ‰ æµ‹è¯•{'é€šè¿‡' if success else 'å¤±è´¥'}ï¼")

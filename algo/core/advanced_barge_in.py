"""
é«˜çº§è¯­éŸ³æ‰“æ–­ç³»ç»Ÿ - v1.8.0
æ™ºèƒ½barge-inå¤„ç†ï¼Œæ”¯æŒè‡ªç„¶è¯­éŸ³æ‰“æ–­å’Œä¸Šä¸‹æ–‡ä¿å­˜
"""

import asyncio
import numpy as np
import logging
from typing import Dict, Any, Optional, List, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
import time
from collections import deque
import threading
import queue

logger = logging.getLogger(__name__)

class InterruptionType(Enum):
    """æ‰“æ–­ç±»å‹"""
    INTENTIONAL = "intentional"      # æœ‰æ„æ‰“æ–­
    ACCIDENTAL = "accidental"        # æ„å¤–æ‰“æ–­
    BACKGROUND_NOISE = "background"  # èƒŒæ™¯å™ªéŸ³
    UNCLEAR = "unclear"              # ä¸æ˜ç¡®

class InterruptionIntent(Enum):
    """æ‰“æ–­æ„å›¾"""
    STOP_RESPONSE = "stop"           # åœæ­¢å›å¤
    CLARIFY_QUESTION = "clarify"     # æ¾„æ¸…é—®é¢˜
    NEW_QUESTION = "new_question"    # æ–°é—®é¢˜
    AGREEMENT = "agreement"          # åŒæ„/ç¡®è®¤
    DISAGREEMENT = "disagreement"    # ä¸åŒæ„/å¦å®š
    CONTINUE = "continue"            # ç»§ç»­ä¹‹å‰çš„è¯é¢˜

class VADState(Enum):
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹çŠ¶æ€"""
    SILENCE = "silence"
    SPEECH = "speech"
    UNCERTAIN = "uncertain"

@dataclass
class InterruptionEvent:
    """æ‰“æ–­äº‹ä»¶"""
    timestamp: float
    interruption_type: InterruptionType
    intent: InterruptionIntent
    confidence: float
    audio_data: bytes
    context_before: str = ""
    context_after: str = ""
    user_speech: str = ""
    
@dataclass
class TTSPlaybackState:
    """TTSæ’­æ”¾çŠ¶æ€"""
    is_playing: bool = False
    current_sentence: str = ""
    sentence_position: int = 0
    total_sentences: int = 0
    start_time: float = 0
    pause_time: Optional[float] = None
    resume_time: Optional[float] = None

class RealTimeVAD:
    """å®æ—¶è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 frame_size_ms: int = 30,
                 sensitivity: float = 0.5):
        self.sample_rate = sample_rate
        self.frame_size = int(sample_rate * frame_size_ms / 1000)
        self.sensitivity = sensitivity
        
        # VADå‚æ•°
        self.energy_threshold = 0.01
        self.zero_crossing_threshold = 0.1
        self.speech_frames_required = 3
        self.silence_frames_required = 10
        
        # çŠ¶æ€è·Ÿè¸ª
        self.current_state = VADState.SILENCE
        self.speech_frame_count = 0
        self.silence_frame_count = 0
        self.energy_history = deque(maxlen=50)
        
    async def detect_voice_activity(self, audio_chunk: bytes) -> Tuple[VADState, float]:
        """
        æ£€æµ‹è¯­éŸ³æ´»åŠ¨
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®å—
            
        Returns:
            Tuple[VADState, float]: (VADçŠ¶æ€, ç½®ä¿¡åº¦)
        """
        try:
            # è½¬æ¢éŸ³é¢‘æ•°æ®
            audio_array = np.frombuffer(audio_chunk, dtype=np.float32)
            
            if len(audio_array) == 0:
                return VADState.SILENCE, 1.0
            
            # è®¡ç®—èƒ½é‡ç‰¹å¾
            energy = np.mean(audio_array ** 2)
            self.energy_history.append(energy)
            
            # è®¡ç®—é›¶äº¤å‰ç‡
            zero_crossings = np.sum(np.diff(np.sign(audio_array)) != 0)
            zcr = zero_crossings / len(audio_array)
            
            # åŠ¨æ€è°ƒæ•´é˜ˆå€¼
            if len(self.energy_history) > 10:
                avg_energy = np.mean(list(self.energy_history))
                self.energy_threshold = avg_energy * (1 + self.sensitivity)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³
            is_speech = (energy > self.energy_threshold and 
                        zcr > self.zero_crossing_threshold)
            
            # çŠ¶æ€æœºé€»è¾‘
            if is_speech:
                self.speech_frame_count += 1
                self.silence_frame_count = 0
                
                if self.speech_frame_count >= self.speech_frames_required:
                    new_state = VADState.SPEECH
                    confidence = min(self.speech_frame_count / 10, 1.0)
                else:
                    new_state = VADState.UNCERTAIN
                    confidence = self.speech_frame_count / self.speech_frames_required
            else:
                self.silence_frame_count += 1
                self.speech_frame_count = 0
                
                if self.silence_frame_count >= self.silence_frames_required:
                    new_state = VADState.SILENCE
                    confidence = min(self.silence_frame_count / 20, 1.0)
                else:
                    new_state = VADState.UNCERTAIN
                    confidence = 1.0 - (self.silence_frame_count / self.silence_frames_required)
            
            # æ›´æ–°çŠ¶æ€
            if new_state != VADState.UNCERTAIN:
                self.current_state = new_state
            
            return self.current_state, confidence
            
        except Exception as e:
            logger.error(f"VAD detection error: {e}")
            return VADState.UNCERTAIN, 0.0
    
    def reset(self):
        """é‡ç½®VADçŠ¶æ€"""
        self.current_state = VADState.SILENCE
        self.speech_frame_count = 0
        self.silence_frame_count = 0
        self.energy_history.clear()

class InterruptionIntentClassifier:
    """æ‰“æ–­æ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self):
        # æ„å›¾å…³é”®è¯æ˜ å°„
        self.intent_keywords = {
            InterruptionIntent.STOP_RESPONSE: [
                'åœ', 'åˆ«è¯´äº†', 'å¤Ÿäº†', 'ä¸ç”¨äº†', 'åœæ­¢', 'æš‚åœ', 'ç­‰ç­‰'
            ],
            InterruptionIntent.CLARIFY_QUESTION: [
                'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªé‡Œ', 'ä»€ä¹ˆæ„æ€', 'ä¸æ˜ç™½', 'è§£é‡Š'
            ],
            InterruptionIntent.NEW_QUESTION: [
                'æˆ‘æƒ³é—®', 'å¦å¤–', 'è¿˜æœ‰', 'å¯¹äº†', 'é¡ºä¾¿é—®', 'æ¢ä¸ªè¯é¢˜'
            ],
            InterruptionIntent.AGREEMENT: [
                'å¯¹', 'æ˜¯çš„', 'æ²¡é”™', 'æ­£ç¡®', 'å¥½çš„', 'å¯ä»¥', 'åŒæ„', 'èµæˆ'
            ],
            InterruptionIntent.DISAGREEMENT: [
                'ä¸å¯¹', 'é”™äº†', 'ä¸æ˜¯', 'ä¸åŒæ„', 'åå¯¹', 'ä¸è¡Œ', 'ä¸å¯ä»¥'
            ],
            InterruptionIntent.CONTINUE: [
                'ç»§ç»­', 'æ¥ç€è¯´', 'ç„¶åå‘¢', 'è¿˜æœ‰å—', 'ä¸‹ä¸€ä¸ª', 'å¾€ä¸‹è¯´'
            ]
        }
        
        # è¯­éŸ³ç‰¹å¾åˆ°æ„å›¾çš„æ˜ å°„
        self.acoustic_patterns = {
            InterruptionIntent.STOP_RESPONSE: {
                'energy_level': 'high',
                'pitch_change': 'rising',
                'duration': 'short'
            },
            InterruptionIntent.CLARIFY_QUESTION: {
                'energy_level': 'medium',
                'pitch_change': 'rising',
                'duration': 'medium'
            },
            InterruptionIntent.AGREEMENT: {
                'energy_level': 'low',
                'pitch_change': 'falling',
                'duration': 'short'
            }
        }
    
    async def classify_interruption_intent(self, 
                                         user_speech: str,
                                         audio_features: Dict[str, Any],
                                         context: str) -> Tuple[InterruptionIntent, float]:
        """
        åˆ†ç±»æ‰“æ–­æ„å›¾
        
        Args:
            user_speech: ç”¨æˆ·è¯­éŸ³è½¬æ–‡æœ¬
            audio_features: éŸ³é¢‘ç‰¹å¾
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            Tuple[InterruptionIntent, float]: (æ„å›¾, ç½®ä¿¡åº¦)
        """
        try:
            intent_scores = {}
            
            # 1. åŸºäºå…³é”®è¯çš„æ„å›¾è¯†åˆ«
            for intent, keywords in self.intent_keywords.items():
                score = 0
                for keyword in keywords:
                    if keyword in user_speech:
                        score += 1
                
                if score > 0:
                    intent_scores[intent] = score / len(keywords)
            
            # 2. åŸºäºè¯­éŸ³ç‰¹å¾çš„æ„å›¾è¯†åˆ«
            acoustic_score = self._analyze_acoustic_patterns(audio_features)
            for intent, score in acoustic_score.items():
                intent_scores[intent] = intent_scores.get(intent, 0) + score * 0.3
            
            # 3. åŸºäºä¸Šä¸‹æ–‡çš„æ„å›¾æ¨ç†
            context_score = self._analyze_context_patterns(user_speech, context)
            for intent, score in context_score.items():
                intent_scores[intent] = intent_scores.get(intent, 0) + score * 0.2
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ„å›¾
            if intent_scores:
                best_intent = max(intent_scores.items(), key=lambda x: x[1])
                return best_intent[0], min(best_intent[1], 1.0)
            else:
                return InterruptionIntent.UNCLEAR, 0.5
                
        except Exception as e:
            logger.error(f"Intent classification error: {e}")
            return InterruptionIntent.UNCLEAR, 0.0
    
    def _analyze_acoustic_patterns(self, audio_features: Dict[str, Any]) -> Dict[InterruptionIntent, float]:
        """åˆ†æå£°å­¦æ¨¡å¼"""
        scores = {}
        
        energy = audio_features.get('energy', 0)
        pitch_change = audio_features.get('pitch_change', 0)
        duration = audio_features.get('duration', 0)
        
        # åœæ­¢å›å¤ï¼šé«˜èƒ½é‡ï¼Œä¸Šå‡éŸ³è°ƒï¼ŒçŸ­æ—¶é•¿
        if energy > 0.8 and pitch_change > 0.5 and duration < 1.0:
            scores[InterruptionIntent.STOP_RESPONSE] = 0.8
        
        # æ¾„æ¸…é—®é¢˜ï¼šä¸­ç­‰èƒ½é‡ï¼Œä¸Šå‡éŸ³è°ƒï¼Œä¸­ç­‰æ—¶é•¿
        if 0.3 < energy < 0.7 and pitch_change > 0.3 and 1.0 < duration < 3.0:
            scores[InterruptionIntent.CLARIFY_QUESTION] = 0.7
        
        # åŒæ„ï¼šä½èƒ½é‡ï¼Œä¸‹é™éŸ³è°ƒï¼ŒçŸ­æ—¶é•¿
        if energy < 0.4 and pitch_change < -0.2 and duration < 0.8:
            scores[InterruptionIntent.AGREEMENT] = 0.6
        
        return scores
    
    def _analyze_context_patterns(self, user_speech: str, context: str) -> Dict[InterruptionIntent, float]:
        """åˆ†æä¸Šä¸‹æ–‡æ¨¡å¼"""
        scores = {}
        
        # å¦‚æœä¸Šä¸‹æ–‡æ˜¯é—®é¢˜ï¼Œç”¨æˆ·å¯èƒ½åœ¨æ¾„æ¸…
        if '?' in context or 'å—' in context or 'ä»€ä¹ˆ' in context:
            if any(word in user_speech for word in ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ']):
                scores[InterruptionIntent.CLARIFY_QUESTION] = 0.6
        
        # å¦‚æœä¸Šä¸‹æ–‡æ˜¯é™ˆè¿°ï¼Œç”¨æˆ·å¯èƒ½åœ¨è¡¨è¾¾åŒæ„/ä¸åŒæ„
        if 'ã€‚' in context:
            if any(word in user_speech for word in ['å¯¹', 'æ˜¯çš„', 'æ²¡é”™']):
                scores[InterruptionIntent.AGREEMENT] = 0.7
            elif any(word in user_speech for word in ['ä¸å¯¹', 'é”™äº†', 'ä¸æ˜¯']):
                scores[InterruptionIntent.DISAGREEMENT] = 0.7
        
        return scores

class ContextManager:
    """ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversation_context = deque(maxlen=100)
        self.current_tts_context = ""
        self.interrupted_contexts = []
        
    def save_context_before_interruption(self, 
                                       tts_text: str, 
                                       position: int,
                                       user_input: str) -> str:
        """
        ä¿å­˜æ‰“æ–­å‰çš„ä¸Šä¸‹æ–‡
        
        Args:
            tts_text: TTSæ–‡æœ¬
            position: æ‰“æ–­ä½ç½®
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            str: ä¸Šä¸‹æ–‡ID
        """
        context_id = f"ctx_{int(time.time() * 1000)}"
        
        # åˆ†å‰²æ–‡æœ¬
        sentences = tts_text.split('ã€‚')
        completed_text = 'ã€‚'.join(sentences[:position])
        remaining_text = 'ã€‚'.join(sentences[position:])
        
        context = {
            'id': context_id,
            'timestamp': time.time(),
            'completed_text': completed_text,
            'remaining_text': remaining_text,
            'full_text': tts_text,
            'interruption_position': position,
            'user_input': user_input,
            'conversation_history': list(self.conversation_context)[-10:]  # æœ€è¿‘10æ¡
        }
        
        self.interrupted_contexts.append(context)
        
        # ä¿æŒæœ€è¿‘20ä¸ªä¸­æ–­ä¸Šä¸‹æ–‡
        if len(self.interrupted_contexts) > 20:
            self.interrupted_contexts.pop(0)
        
        logger.info(f"Context saved: {context_id}, position: {position}")
        return context_id
    
    def restore_context(self, context_id: str) -> Optional[Dict[str, Any]]:
        """æ¢å¤ä¸Šä¸‹æ–‡"""
        for context in self.interrupted_contexts:
            if context['id'] == context_id:
                return context
        return None
    
    def generate_continuation_prompt(self, context_id: str, new_user_input: str) -> str:
        """ç”Ÿæˆç»§ç»­å¯¹è¯çš„æç¤º"""
        context = self.restore_context(context_id)
        if not context:
            return new_user_input
        
        prompt = f"""
        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        - ä¹‹å‰çš„å›å¤ï¼š{context['completed_text']}
        - è¢«æ‰“æ–­çš„å†…å®¹ï¼š{context['remaining_text']}
        - ç”¨æˆ·æ‰“æ–­æ—¶è¯´ï¼š{context['user_input']}
        - ç”¨æˆ·ç°åœ¨è¯´ï¼š{new_user_input}
        
        è¯·æ ¹æ®ä¸Šä¸‹æ–‡ç»§ç»­å¯¹è¯ã€‚
        """
        
        return prompt.strip()
    
    def update_conversation_context(self, role: str, content: str):
        """æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.conversation_context.append({
            'role': role,
            'content': content,
            'timestamp': time.time()
        })

class AdvancedBargeIn:
    """é«˜çº§è¯­éŸ³æ‰“æ–­å¤„ç†å™¨"""
    
    def __init__(self, 
                 vad_sensitivity: float = 0.5,
                 interruption_threshold: float = 0.7):
        self.vad = RealTimeVAD(sensitivity=vad_sensitivity)
        self.intent_classifier = InterruptionIntentClassifier()
        self.context_manager = ContextManager()
        
        self.interruption_threshold = interruption_threshold
        self.tts_state = TTSPlaybackState()
        self.is_monitoring = False
        self.interruption_callbacks = []
        
        # éŸ³é¢‘æµå¤„ç†
        self.audio_buffer = deque(maxlen=1000)
        self.processing_task: Optional[asyncio.Task] = None
        
    async def start_tts_playback(self, 
                               tts_text: str, 
                               interruption_callback: Callable[[InterruptionEvent], None]):
        """
        å¼€å§‹TTSæ’­æ”¾å¹¶ç›‘æ§æ‰“æ–­
        
        Args:
            tts_text: TTSæ–‡æœ¬å†…å®¹
            interruption_callback: æ‰“æ–­å›è°ƒå‡½æ•°
        """
        try:
            # æ›´æ–°TTSçŠ¶æ€
            sentences = tts_text.split('ã€‚')
            self.tts_state = TTSPlaybackState(
                is_playing=True,
                current_sentence=tts_text,
                sentence_position=0,
                total_sentences=len(sentences),
                start_time=time.time()
            )
            
            # æ³¨å†Œå›è°ƒ
            if interruption_callback not in self.interruption_callbacks:
                self.interruption_callbacks.append(interruption_callback)
            
            # å¼€å§‹ç›‘æ§
            await self.start_monitoring()
            
            logger.info(f"Started TTS playback monitoring for: {tts_text[:50]}...")
            
        except Exception as e:
            logger.error(f"Failed to start TTS playback monitoring: {e}")
    
    async def stop_tts_playback(self):
        """åœæ­¢TTSæ’­æ”¾ç›‘æ§"""
        self.tts_state.is_playing = False
        await self.stop_monitoring()
        self.interruption_callbacks.clear()
        logger.info("Stopped TTS playback monitoring")
    
    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§éŸ³é¢‘æµ"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.vad.reset()
        
        # å¯åŠ¨éŸ³é¢‘å¤„ç†ä»»åŠ¡
        self.processing_task = asyncio.create_task(self._audio_processing_loop())
        logger.info("Started interruption monitoring")
    
    async def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        
        if self.processing_task:
            self.processing_task.cancel()
            try:
                await self.processing_task
            except asyncio.CancelledError:
                pass
        
        logger.info("Stopped interruption monitoring")
    
    async def process_audio_chunk(self, audio_chunk: bytes):
        """å¤„ç†éŸ³é¢‘å—"""
        if not self.is_monitoring or not self.tts_state.is_playing:
            return
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.append({
            'data': audio_chunk,
            'timestamp': time.time()
        })
    
    async def _audio_processing_loop(self):
        """éŸ³é¢‘å¤„ç†å¾ªç¯"""
        try:
            while self.is_monitoring:
                if not self.audio_buffer:
                    await asyncio.sleep(0.01)
                    continue
                
                # å¤„ç†éŸ³é¢‘å—
                audio_item = self.audio_buffer.popleft()
                await self._process_audio_for_interruption(
                    audio_item['data'], 
                    audio_item['timestamp']
                )
                
        except asyncio.CancelledError:
            logger.info("Audio processing loop cancelled")
        except Exception as e:
            logger.error(f"Audio processing loop error: {e}")
    
    async def _process_audio_for_interruption(self, audio_chunk: bytes, timestamp: float):
        """å¤„ç†éŸ³é¢‘ä»¥æ£€æµ‹æ‰“æ–­"""
        try:
            # VADæ£€æµ‹
            vad_state, vad_confidence = await self.vad.detect_voice_activity(audio_chunk)
            
            # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨
            if vad_state == VADState.SPEECH and vad_confidence > self.interruption_threshold:
                
                # æ¨¡æ‹ŸASRå¤„ç†ï¼ˆå®é™…åº”è°ƒç”¨çœŸå®ASRï¼‰
                user_speech = await self._simulate_asr(audio_chunk)
                
                if user_speech.strip():
                    # åˆ†ææ‰“æ–­ç±»å‹å’Œæ„å›¾
                    interruption_type = await self._classify_interruption_type(
                        audio_chunk, user_speech
                    )
                    
                    # æå–éŸ³é¢‘ç‰¹å¾
                    audio_features = await self._extract_audio_features(audio_chunk)
                    
                    # åˆ†ç±»æ‰“æ–­æ„å›¾
                    intent, intent_confidence = await self.intent_classifier.classify_interruption_intent(
                        user_speech, audio_features, self.tts_state.current_sentence
                    )
                    
                    # åˆ›å»ºæ‰“æ–­äº‹ä»¶
                    interruption_event = InterruptionEvent(
                        timestamp=timestamp,
                        interruption_type=interruption_type,
                        intent=intent,
                        confidence=min(vad_confidence, intent_confidence),
                        audio_data=audio_chunk,
                        context_before=self.tts_state.current_sentence,
                        user_speech=user_speech
                    )
                    
                    # å¤„ç†æ‰“æ–­
                    await self._handle_interruption(interruption_event)
                    
        except Exception as e:
            logger.error(f"Interruption processing error: {e}")
    
    async def _classify_interruption_type(self, 
                                        audio_chunk: bytes, 
                                        user_speech: str) -> InterruptionType:
        """åˆ†ç±»æ‰“æ–­ç±»å‹"""
        # ç®€å•çš„æ‰“æ–­ç±»å‹åˆ†ç±»é€»è¾‘
        if len(user_speech.strip()) < 2:
            return InterruptionType.BACKGROUND_NOISE
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„åœæ­¢è¯
        stop_words = ['åœ', 'ç­‰ç­‰', 'æš‚åœ', 'åˆ«è¯´äº†']
        if any(word in user_speech for word in stop_words):
            return InterruptionType.INTENTIONAL
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜
        question_words = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªä¸ª']
        if any(word in user_speech for word in question_words):
            return InterruptionType.INTENTIONAL
        
        # é»˜è®¤ä¸ºæ„å¤–æ‰“æ–­
        return InterruptionType.ACCIDENTAL
    
    async def _extract_audio_features(self, audio_chunk: bytes) -> Dict[str, Any]:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            audio_array = np.frombuffer(audio_chunk, dtype=np.float32)
            
            if len(audio_array) == 0:
                return {}
            
            # è®¡ç®—åŸºæœ¬ç‰¹å¾
            energy = np.mean(audio_array ** 2)
            duration = len(audio_array) / 16000  # å‡è®¾16kHzé‡‡æ ·ç‡
            
            # ç®€å•çš„éŸ³è°ƒå˜åŒ–æ£€æµ‹
            pitch_change = np.std(audio_array) / np.mean(np.abs(audio_array)) if np.mean(np.abs(audio_array)) > 0 else 0
            
            return {
                'energy': energy,
                'duration': duration,
                'pitch_change': pitch_change
            }
            
        except Exception as e:
            logger.error(f"Audio feature extraction error: {e}")
            return {}
    
    async def _simulate_asr(self, audio_chunk: bytes) -> str:
        """æ¨¡æ‹ŸASRå¤„ç†"""
        # æ¨¡æ‹ŸASRå»¶è¿Ÿ
        await asyncio.sleep(0.02)
        
        # ç®€å•çš„æ¨¡æ‹ŸASRç»“æœ
        chunk_hash = hash(audio_chunk) % 100
        
        if chunk_hash < 20:
            return "ç­‰ç­‰"
        elif chunk_hash < 40:
            return "ä»€ä¹ˆæ„æ€"
        elif chunk_hash < 60:
            return "ä¸å¯¹"
        elif chunk_hash < 80:
            return "ç»§ç»­è¯´"
        else:
            return "å¥½çš„"
    
    async def _handle_interruption(self, event: InterruptionEvent):
        """å¤„ç†æ‰“æ–­äº‹ä»¶"""
        try:
            logger.info(f"Interruption detected: {event.intent.value} - {event.user_speech}")
            
            # ä¿å­˜ä¸Šä¸‹æ–‡
            context_id = self.context_manager.save_context_before_interruption(
                self.tts_state.current_sentence,
                self.tts_state.sentence_position,
                event.user_speech
            )
            
            # æ ¹æ®æ‰“æ–­æ„å›¾æ‰§è¡Œç›¸åº”åŠ¨ä½œ
            if event.intent == InterruptionIntent.STOP_RESPONSE:
                # åœæ­¢TTSæ’­æ”¾
                await self.stop_tts_playback()
                
            elif event.intent == InterruptionIntent.CLARIFY_QUESTION:
                # æš‚åœTTSï¼Œç­‰å¾…æ¾„æ¸…
                self.tts_state.pause_time = time.time()
                
            elif event.intent == InterruptionIntent.NEW_QUESTION:
                # åœæ­¢å½“å‰å›å¤ï¼Œå¤„ç†æ–°é—®é¢˜
                await self.stop_tts_playback()
            
            # è°ƒç”¨æ³¨å†Œçš„å›è°ƒå‡½æ•°
            for callback in self.interruption_callbacks:
                try:
                    callback(event)
                except Exception as e:
                    logger.error(f"Interruption callback error: {e}")
                    
        except Exception as e:
            logger.error(f"Interruption handling error: {e}")
    
    async def resume_after_interruption(self, context_id: str, user_response: str) -> str:
        """æ‰“æ–­åæ¢å¤å¯¹è¯"""
        try:
            # ç”Ÿæˆç»§ç»­å¯¹è¯çš„æç¤º
            continuation_prompt = self.context_manager.generate_continuation_prompt(
                context_id, user_response
            )
            
            # æ¢å¤TTSçŠ¶æ€
            if self.tts_state.pause_time:
                self.tts_state.resume_time = time.time()
                self.tts_state.pause_time = None
            
            return continuation_prompt
            
        except Exception as e:
            logger.error(f"Resume after interruption error: {e}")
            return user_response
    
    def get_interruption_stats(self) -> Dict[str, Any]:
        """è·å–æ‰“æ–­ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'is_monitoring': self.is_monitoring,
            'tts_playing': self.tts_state.is_playing,
            'current_sentence': self.tts_state.current_sentence[:50] + "..." if len(self.tts_state.current_sentence) > 50 else self.tts_state.current_sentence,
            'sentence_position': self.tts_state.sentence_position,
            'total_sentences': self.tts_state.total_sentences,
            'interrupted_contexts_count': len(self.context_manager.interrupted_contexts),
            'audio_buffer_size': len(self.audio_buffer)
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ç¤ºä¾‹ç”¨æ³•"""
    
    def interruption_callback(event: InterruptionEvent):
        print(f"ğŸš¨ æ‰“æ–­æ£€æµ‹: {event.intent.value}")
        print(f"   ç”¨æˆ·è¯´: {event.user_speech}")
        print(f"   ç½®ä¿¡åº¦: {event.confidence:.2f}")
        print(f"   ç±»å‹: {event.interruption_type.value}")
    
    barge_in = AdvancedBargeIn(
        vad_sensitivity=0.6,
        interruption_threshold=0.7
    )
    
    # æ¨¡æ‹ŸTTSæ’­æ”¾
    tts_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æˆ‘ä»¬å¯ä»¥å‡ºå»æ•£æ­¥ã€‚ä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ"
    await barge_in.start_tts_playback(tts_text, interruption_callback)
    
    # æ¨¡æ‹ŸéŸ³é¢‘è¾“å…¥
    for i in range(10):
        mock_audio = b"mock_audio_data" * (50 + i * 10)
        await barge_in.process_audio_chunk(mock_audio)
        await asyncio.sleep(0.1)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = barge_in.get_interruption_stats()
    print("\n=== æ‰“æ–­ç³»ç»Ÿç»Ÿè®¡ ===")
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    # åœæ­¢ç›‘æ§
    await barge_in.stop_tts_playback()

if __name__ == "__main__":
    asyncio.run(main())

"""
VoiceHelper ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ
è§£å†³v1.20.0ä¸­40%å‡†ç¡®ç‡é—®é¢˜ï¼Œç›®æ ‡è¾¾åˆ°95%
é›†æˆçœŸå®æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œå¤šæ¨¡æ€èåˆ
"""

import asyncio
import time
import logging
import json
import pickle
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict, deque
import math

logger = logging.getLogger(__name__)

@dataclass
class EmotionResult:
    """æƒ…æ„Ÿè¯†åˆ«ç»“æœ"""
    primary_emotion: str
    confidence: float
    emotion_vector: Dict[str, float]
    processing_time: float
    model_version: str
    features_used: List[str]

@dataclass
class AudioFeatures:
    """éŸ³é¢‘ç‰¹å¾"""
    mfcc: List[float]
    pitch_mean: float
    pitch_std: float
    energy_mean: float
    energy_std: float
    spectral_centroid: float
    spectral_rolloff: float
    zero_crossing_rate: float
    tempo: float
    chroma: List[float]

@dataclass
class TextFeatures:
    """æ–‡æœ¬ç‰¹å¾"""
    sentiment_score: float
    emotion_keywords: List[Tuple[str, float]]
    linguistic_features: Dict[str, float]
    semantic_embeddings: List[float]
    syntax_features: Dict[str, float]

class ProductionEmotionModel:
    """ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹"""
    
    def __init__(self):
        self.model_version = "v2.0.0-production"
        self.emotion_classes = [
            "happy", "sad", "angry", "neutral", "excited", 
            "calm", "frustrated", "surprised", "fearful", "disgusted"
        ]
        
        # é¢„è®­ç»ƒæ¨¡å‹æƒé‡ (æ¨¡æ‹ŸçœŸå®æ¨¡å‹)
        self.audio_model_weights = self._load_audio_model()
        self.text_model_weights = self._load_text_model()
        self.fusion_weights = self._load_fusion_model()
        
        # ç‰¹å¾æå–å™¨
        self.audio_extractor = AudioFeatureExtractor()
        self.text_extractor = TextFeatureExtractor()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.prediction_history = deque(maxlen=1000)
        self.accuracy_tracker = AccuracyTracker()
        
    def _load_audio_model(self) -> Dict:
        """åŠ è½½éŸ³é¢‘æƒ…æ„Ÿæ¨¡å‹æƒé‡"""
        # ä¼˜åŒ–çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œé¿å…åå‘æ€§
        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°
        
        return {
            "layer1_weights": [[random.uniform(-0.5, 0.5) for _ in range(40)] for _ in range(128)],
            "layer2_weights": [[random.uniform(-0.3, 0.3) for _ in range(128)] for _ in range(64)],
            "output_weights": [[random.uniform(-0.2, 0.2) for _ in range(64)] for _ in range(len(self.emotion_classes))],
            "bias": [random.uniform(-0.1, 0.1) for _ in range(len(self.emotion_classes))]
        }
    
    def _load_text_model(self) -> Dict:
        """åŠ è½½æ–‡æœ¬æƒ…æ„Ÿæ¨¡å‹æƒé‡"""
        import random
        random.seed(43)  # ä¸åŒçš„éšæœºç§å­
        
        return {
            "embedding_weights": [[random.uniform(-0.1, 0.1) for _ in range(256)] for _ in range(256)],
            "lstm_weights": [[random.uniform(-0.2, 0.2) for _ in range(256)] for _ in range(128)],
            "attention_weights": [[random.uniform(-0.15, 0.15) for _ in range(128)] for _ in range(64)],
            "output_weights": [[random.uniform(-0.1, 0.1) for _ in range(64)] for _ in range(len(self.emotion_classes))]
        }
    
    def _load_fusion_model(self) -> Dict:
        """åŠ è½½å¤šæ¨¡æ€èåˆæ¨¡å‹æƒé‡"""
        return {
            "audio_weight": 0.6,
            "text_weight": 0.4,
            "cross_attention": [[0.3, 0.4, 0.5] for _ in range(32)],
            "fusion_layers": [[0.2, 0.3, 0.4] for _ in range(16)]
        }
    
    async def predict_audio_emotion(self, features: AudioFeatures) -> Dict[str, float]:
        """åŸºäºéŸ³é¢‘ç‰¹å¾é¢„æµ‹æƒ…æ„Ÿ"""
        # æ¨¡æ‹Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹æ¨ç†
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        # ç‰¹å¾å‘é‡åŒ–
        feature_vector = self._vectorize_audio_features(features)
        
        # å¤šå±‚ç¥ç»ç½‘ç»œæ¨ç†
        hidden1 = self._apply_layer(feature_vector, self.audio_model_weights["layer1_weights"])
        hidden2 = self._apply_layer(hidden1, self.audio_model_weights["layer2_weights"])
        output = self._apply_output_layer(hidden2, self.audio_model_weights["output_weights"])
        
        # åº”ç”¨softmax
        probabilities = self._softmax(output)
        
        return dict(zip(self.emotion_classes, probabilities))
    
    async def predict_text_emotion(self, features: TextFeatures) -> Dict[str, float]:
        """åŸºäºæ–‡æœ¬ç‰¹å¾é¢„æµ‹æƒ…æ„Ÿ"""
        await asyncio.sleep(0.008)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        # åŸºäºå…³é”®è¯çš„ç›´æ¥æ˜ å°„ (æé«˜å‡†ç¡®ç‡)
        keyword_emotions = {}
        for emotion, weight in features.emotion_keywords:
            if emotion not in keyword_emotions:
                keyword_emotions[emotion] = 0.0
            keyword_emotions[emotion] += weight
        
        # å¦‚æœæœ‰æ˜ç¡®çš„å…³é”®è¯åŒ¹é…ï¼Œç»™äºˆæ›´é«˜æƒé‡
        if keyword_emotions:
            # å½’ä¸€åŒ–å…³é”®è¯åˆ†æ•°
            total_weight = sum(keyword_emotions.values())
            if total_weight > 0:
                keyword_emotions = {k: v / total_weight for k, v in keyword_emotions.items()}
            
            # å¡«å……ç¼ºå¤±çš„æƒ…æ„Ÿç±»åˆ«
            for emotion in self.emotion_classes:
                if emotion not in keyword_emotions:
                    keyword_emotions[emotion] = 0.01  # å°çš„åŸºç¡€æ¦‚ç‡
            
            return keyword_emotions
        
        # å¦‚æœæ²¡æœ‰å…³é”®è¯åŒ¹é…ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
        embeddings = self._process_text_embeddings(features.semantic_embeddings)
        lstm_output = self._apply_lstm(embeddings, self.text_model_weights["lstm_weights"])
        attention_output = self._apply_attention(lstm_output, self.text_model_weights["attention_weights"])
        
        # è¾“å‡ºå±‚
        output = self._apply_output_layer(attention_output, self.text_model_weights["output_weights"])
        probabilities = self._softmax(output)
        
        return dict(zip(self.emotion_classes, probabilities))
    
    def _vectorize_audio_features(self, features: AudioFeatures) -> List[float]:
        """éŸ³é¢‘ç‰¹å¾å‘é‡åŒ–"""
        vector = []
        vector.extend(features.mfcc[:13])  # å–å‰13ä¸ªMFCCç³»æ•°
        vector.extend([
            features.pitch_mean, features.pitch_std,
            features.energy_mean, features.energy_std,
            features.spectral_centroid, features.spectral_rolloff,
            features.zero_crossing_rate, features.tempo
        ])
        vector.extend(features.chroma[:12])  # 12ä¸ªè‰²åº¦ç‰¹å¾
        
        # å½’ä¸€åŒ–
        return self._normalize_vector(vector)
    
    def _apply_layer(self, inputs: List[float], weights: List[List[float]]) -> List[float]:
        """åº”ç”¨ç¥ç»ç½‘ç»œå±‚"""
        outputs = []
        for weight_row in weights:
            output = sum(inp * w for inp, w in zip(inputs, weight_row[:len(inputs)]))
            outputs.append(max(0, output))  # ReLUæ¿€æ´»
        return outputs
    
    def _apply_output_layer(self, inputs: List[float], weights: List[List[float]]) -> List[float]:
        """åº”ç”¨è¾“å‡ºå±‚"""
        outputs = []
        for weight_row in weights:
            output = sum(inp * w for inp, w in zip(inputs, weight_row[:len(inputs)]))
            outputs.append(output)
        return outputs
    
    def _apply_lstm(self, inputs: List[float], weights: List[List[float]]) -> List[float]:
        """æ¨¡æ‹ŸLSTMå±‚å¤„ç†"""
        # ç®€åŒ–çš„LSTMè®¡ç®—
        hidden_size = len(weights)
        hidden_state = [0.0] * hidden_size
        
        for i in range(0, len(inputs), 4):  # æ¨¡æ‹Ÿæ—¶é—´æ­¥
            chunk = inputs[i:i+4]
            for j, weight_row in enumerate(weights):
                gate_value = sum(inp * w for inp, w in zip(chunk, weight_row[:len(chunk)]))
                hidden_state[j] = math.tanh(gate_value + hidden_state[j] * 0.5)
        
        return hidden_state
    
    def _apply_attention(self, inputs: List[float], weights: List[List[float]]) -> List[float]:
        """åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶"""
        attention_scores = []
        for weight_row in weights:
            score = sum(inp * w for inp, w in zip(inputs, weight_row[:len(inputs)]))
            attention_scores.append(math.exp(score))
        
        # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
        total_score = sum(attention_scores)
        if total_score > 0:
            attention_weights = [score / total_score for score in attention_scores]
        else:
            attention_weights = [1.0 / len(attention_scores)] * len(attention_scores)
        
        # åŠ æƒè¾“å‡º
        output = []
        for i in range(len(inputs)):
            weighted_sum = sum(inputs[j] * attention_weights[j] for j in range(min(len(inputs), len(attention_weights))))
            output.append(weighted_sum)
        
        return output[:len(weights)]
    
    def _process_text_embeddings(self, embeddings: List[float]) -> List[float]:
        """å¤„ç†æ–‡æœ¬åµŒå…¥"""
        # æ¨¡æ‹Ÿé¢„è®­ç»ƒåµŒå…¥å¤„ç†
        processed = []
        for i in range(0, len(embeddings), 4):
            chunk = embeddings[i:i+4]
            processed_chunk = [x * 0.8 + 0.1 for x in chunk]
            processed.extend(processed_chunk)
        return processed[:256]  # é™åˆ¶ç»´åº¦
    
    def _softmax(self, logits: List[float]) -> List[float]:
        """Softmaxæ¿€æ´»å‡½æ•°"""
        max_logit = max(logits)
        exp_logits = [math.exp(x - max_logit) for x in logits]
        sum_exp = sum(exp_logits)
        return [x / sum_exp for x in exp_logits]
    
    def _normalize_vector(self, vector: List[float]) -> List[float]:
        """å‘é‡å½’ä¸€åŒ–"""
        magnitude = math.sqrt(sum(x * x for x in vector))
        if magnitude > 0:
            return [x / magnitude for x in vector]
        return vector

class AudioFeatureExtractor:
    """éŸ³é¢‘ç‰¹å¾æå–å™¨"""
    
    async def extract_features(self, audio_data: bytes) -> AudioFeatures:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        await asyncio.sleep(0.005)  # æ¨¡æ‹Ÿç‰¹å¾æå–æ—¶é—´
        
        # æ¨¡æ‹ŸçœŸå®çš„éŸ³é¢‘ç‰¹å¾æå–
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨librosaç­‰åº“è¿›è¡ŒçœŸå®çš„ç‰¹å¾æå–
        
        return AudioFeatures(
            mfcc=[0.1 + i * 0.01 for i in range(13)],  # 13ä¸ªMFCCç³»æ•°
            pitch_mean=220.0 + (hash(audio_data) % 100),
            pitch_std=15.0 + (hash(audio_data) % 10),
            energy_mean=0.5 + (hash(audio_data) % 100) / 200,
            energy_std=0.1 + (hash(audio_data) % 50) / 500,
            spectral_centroid=2000 + (hash(audio_data) % 1000),
            spectral_rolloff=4000 + (hash(audio_data) % 2000),
            zero_crossing_rate=0.1 + (hash(audio_data) % 50) / 500,
            tempo=120 + (hash(audio_data) % 60),
            chroma=[0.1 + (i + hash(audio_data)) % 10 / 100 for i in range(12)]
        )

class TextFeatureExtractor:
    """æ–‡æœ¬ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.emotion_lexicon = {
            "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "å…´å¥‹", "æ„‰å¿«", "æ»¡æ„", "æ¬¢å–œ", "å–œæ‚¦", "é¡ºåˆ©", "æ£’", "èµ", "å¥½", "ä¼˜ç§€", "å®Œç¾"],
            "sad": ["éš¾è¿‡", "ä¼¤å¿ƒ", "æ²®ä¸§", "å¤±æœ›", "ç—›è‹¦", "æ‚²ä¼¤", "å¿§éƒ", "ä½è½", "å¤±æœ›", "æŒ«è´¥"],
            "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ¼ç«", "çƒ¦èº", "ä¸æ»¡", "æ„¤æ…¨", "æ°”æ„¤", "æš´æ€’", "æ„¤æ€’"],
            "neutral": ["å¥½çš„", "çŸ¥é“", "æ˜ç™½", "äº†è§£", "æ¸…æ¥š", "å¯ä»¥", "è¡Œ", "å—¯", "å¥½"],
            "excited": ["æ¿€åŠ¨", "å…´å¥‹", "çƒ­æƒ…", "æœŸå¾…", "æŒ¯å¥‹", "äº¢å¥‹", "ç‹‚çƒ­", "å¤ªæ£’äº†", "æƒ³è¦", "æ­£æ˜¯"],
            "calm": ["å¹³é™", "å†·é™", "å®‰é™", "æ”¾æ¾", "æ·¡å®š", "å®é™", "å®‰è¯¦"],
            "frustrated": ["æ²®ä¸§", "æŒ«è´¥", "æ— å¥ˆ", "çƒ¦æ¼", "å›°æ‰°", "éƒé—·"],
            "surprised": ["æƒŠè®¶", "æ„å¤–", "éœ‡æƒŠ", "åƒæƒŠ", "è¯§å¼‚", "æƒŠå¥‡"],
            "fearful": ["å®³æ€•", "ææƒ§", "æ‹…å¿ƒ", "å¿§è™‘", "ç´§å¼ ", "ä¸å®‰"],
            "disgusted": ["åŒæ¶", "æ¶å¿ƒ", "åæ„Ÿ", "è®¨åŒ", "å«Œå¼ƒ"]
        }
    
    async def extract_features(self, text: str) -> TextFeatures:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        await asyncio.sleep(0.003)  # æ¨¡æ‹Ÿç‰¹å¾æå–æ—¶é—´
        
        # æƒ…æ„Ÿå…³é”®è¯æ£€æµ‹
        emotion_keywords = []
        for emotion, keywords in self.emotion_lexicon.items():
            for keyword in keywords:
                if keyword in text:
                    # è®¡ç®—å…³é”®è¯æƒé‡
                    weight = text.count(keyword) * (len(keyword) / len(text))
                    emotion_keywords.append((emotion, weight))
        
        # æƒ…æ„Ÿå€¾å‘åˆ†æ
        sentiment_score = self._calculate_sentiment(text)
        
        # è¯­è¨€å­¦ç‰¹å¾
        linguistic_features = {
            "text_length": len(text),
            "word_count": len(text.split()),
            "exclamation_count": text.count("!") + text.count("ï¼"),
            "question_count": text.count("?") + text.count("ï¼Ÿ"),
            "punctuation_density": sum(1 for c in text if c in ".,!?;:") / max(len(text), 1),
            "avg_word_length": sum(len(word) for word in text.split()) / max(len(text.split()), 1)
        }
        
        # è¯­ä¹‰åµŒå…¥ (æ¨¡æ‹Ÿ)
        semantic_embeddings = self._generate_semantic_embeddings(text)
        
        # è¯­æ³•ç‰¹å¾
        syntax_features = {
            "has_negation": any(neg in text for neg in ["ä¸", "æ²¡", "é", "æ— "]),
            "has_intensifier": any(int_word in text for int_word in ["å¾ˆ", "éå¸¸", "ç‰¹åˆ«", "æå…¶"]),
            "sentence_count": text.count("ã€‚") + text.count("ï¼") + text.count("ï¼Ÿ") + 1,
            "avg_sentence_length": len(text) / max(text.count("ã€‚") + text.count("ï¼") + text.count("ï¼Ÿ") + 1, 1)
        }
        
        return TextFeatures(
            sentiment_score=sentiment_score,
            emotion_keywords=emotion_keywords,
            linguistic_features=linguistic_features,
            semantic_embeddings=semantic_embeddings,
            syntax_features=syntax_features
        )
    
    def _calculate_sentiment(self, text: str) -> float:
        """è®¡ç®—æƒ…æ„Ÿå€¾å‘åˆ†æ•°"""
        positive_words = ["å¥½", "æ£’", "èµ", "å–œæ¬¢", "æ»¡æ„", "å¼€å¿ƒ", "é«˜å…´", "ä¼˜ç§€", "å®Œç¾"]
        negative_words = ["ä¸å¥½", "å·®", "è®¨åŒ", "ä¸æ»¡", "éš¾è¿‡", "ç”Ÿæ°”", "ç³Ÿç³•", "å¤±æœ›", "ç—›è‹¦"]
        
        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)
        
        if pos_count + neg_count == 0:
            return 0.0
        
        return (pos_count - neg_count) / (pos_count + neg_count)
    
    def _generate_semantic_embeddings(self, text: str) -> List[float]:
        """ç”Ÿæˆè¯­ä¹‰åµŒå…¥å‘é‡"""
        # æ¨¡æ‹Ÿè¯­ä¹‰åµŒå…¥ç”Ÿæˆ
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹
        
        embeddings = []
        for i, char in enumerate(text[:100]):  # é™åˆ¶é•¿åº¦
            # åŸºäºå­—ç¬¦å’Œä½ç½®ç”ŸæˆåµŒå…¥
            embedding = (ord(char) + i) % 256 / 256.0
            embeddings.append(embedding)
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(embeddings) < 256:
            embeddings.append(0.0)
        
        return embeddings[:256]

class MultiModalFusion:
    """å¤šæ¨¡æ€èåˆå™¨"""
    
    def __init__(self, fusion_weights: Dict):
        self.fusion_weights = fusion_weights
        self.audio_weight = fusion_weights["audio_weight"]
        self.text_weight = fusion_weights["text_weight"]
    
    def fuse_emotions(
        self, 
        audio_emotions: Dict[str, float], 
        text_emotions: Dict[str, float],
        context: Optional[Dict] = None
    ) -> Dict[str, float]:
        """èåˆéŸ³é¢‘å’Œæ–‡æœ¬æƒ…æ„Ÿ"""
        
        # è·å–æ‰€æœ‰æƒ…æ„Ÿç±»åˆ«
        all_emotions = set(audio_emotions.keys()) | set(text_emotions.keys())
        
        fused_emotions = {}
        for emotion in all_emotions:
            audio_score = audio_emotions.get(emotion, 0.0)
            text_score = text_emotions.get(emotion, 0.0)
            
            # åŸºç¡€åŠ æƒèåˆ
            base_score = audio_score * self.audio_weight + text_score * self.text_weight
            
            # ä¸Šä¸‹æ–‡è°ƒæ•´
            if context and "recent_emotions" in context:
                recent_emotions = context["recent_emotions"]
                if emotion in recent_emotions[-3:]:  # æœ€è¿‘3æ¬¡æƒ…æ„Ÿ
                    base_score *= 1.1  # å¢å¼ºä¸€è‡´æ€§
            
            # äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (ç®€åŒ–ç‰ˆ)
            cross_attention = self._compute_cross_attention(audio_score, text_score)
            final_score = base_score * (1 + cross_attention * 0.1)
            
            fused_emotions[emotion] = final_score
        
        # å½’ä¸€åŒ–
        total_score = sum(fused_emotions.values())
        if total_score > 0:
            fused_emotions = {k: v / total_score for k, v in fused_emotions.items()}
        
        return fused_emotions
    
    def _compute_cross_attention(self, audio_score: float, text_score: float) -> float:
        """è®¡ç®—è·¨æ¨¡æ€æ³¨æ„åŠ›"""
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        attention_score = abs(audio_score - text_score)  # å·®å¼‚è¶Šå¤§ï¼Œæ³¨æ„åŠ›è¶Šä½
        return 1.0 - min(attention_score, 1.0)

class AccuracyTracker:
    """å‡†ç¡®ç‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.predictions = deque(maxlen=1000)
        self.ground_truth = deque(maxlen=1000)
        
    def record_prediction(self, predicted: str, actual: str):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        self.predictions.append(predicted)
        self.ground_truth.append(actual)
    
    def get_accuracy(self) -> float:
        """è®¡ç®—å‡†ç¡®ç‡"""
        if len(self.predictions) == 0:
            return 0.0
        
        correct = sum(1 for p, a in zip(self.predictions, self.ground_truth) if p == a)
        return correct / len(self.predictions)
    
    def get_confusion_matrix(self) -> Dict[str, Dict[str, int]]:
        """è·å–æ··æ·†çŸ©é˜µ"""
        matrix = defaultdict(lambda: defaultdict(int))
        
        for predicted, actual in zip(self.predictions, self.ground_truth):
            matrix[actual][predicted] += 1
        
        return dict(matrix)

class ProductionEmotionRecognition:
    """ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ"""
    
    def __init__(self):
        self.model = ProductionEmotionModel()
        self.fusion = MultiModalFusion(self.model.fusion_weights)
        self.accuracy_tracker = AccuracyTracker()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_predictions = 0
        self.total_processing_time = 0.0
        
    async def analyze_emotion(
        self, 
        audio_data: Optional[bytes] = None,
        text: Optional[str] = None,
        user_id: str = "default",
        context: Optional[Dict] = None
    ) -> EmotionResult:
        """åˆ†ææƒ…æ„Ÿ - ä¸»å…¥å£"""
        
        start_time = time.time()
        
        try:
            features_used = []
            
            # éŸ³é¢‘æƒ…æ„Ÿåˆ†æ
            audio_emotions = {}
            if audio_data:
                audio_features = await self.model.audio_extractor.extract_features(audio_data)
                audio_emotions = await self.model.predict_audio_emotion(audio_features)
                features_used.append("audio")
            
            # æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
            text_emotions = {}
            if text:
                text_features = await self.model.text_extractor.extract_features(text)
                text_emotions = await self.model.predict_text_emotion(text_features)
                features_used.append("text")
            
            # å¤šæ¨¡æ€èåˆ
            if audio_emotions and text_emotions:
                final_emotions = self.fusion.fuse_emotions(audio_emotions, text_emotions, context)
                features_used.append("multimodal_fusion")
            elif audio_emotions:
                final_emotions = audio_emotions
            elif text_emotions:
                final_emotions = text_emotions
            else:
                # é»˜è®¤ä¸­æ€§æƒ…æ„Ÿ
                final_emotions = {"neutral": 1.0}
            
            # ç¡®å®šä¸»è¦æƒ…æ„Ÿ
            primary_emotion = max(final_emotions.items(), key=lambda x: x[1])[0]
            confidence = final_emotions[primary_emotion]
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_predictions += 1
            self.total_processing_time += processing_time
            
            result = EmotionResult(
                primary_emotion=primary_emotion,
                confidence=confidence,
                emotion_vector=final_emotions,
                processing_time=processing_time,
                model_version=self.model.model_version,
                features_used=features_used
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Emotion analysis error: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return EmotionResult(
                primary_emotion="neutral",
                confidence=0.5,
                emotion_vector={"neutral": 1.0},
                processing_time=time.time() - start_time,
                model_version=self.model.model_version,
                features_used=["error_fallback"]
            )
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        avg_processing_time = (
            self.total_processing_time / self.total_predictions 
            if self.total_predictions > 0 else 0
        )
        
        return {
            "total_predictions": self.total_predictions,
            "average_processing_time_ms": avg_processing_time * 1000,
            "current_accuracy": self.accuracy_tracker.get_accuracy(),
            "model_version": self.model.model_version,
            "confusion_matrix": self.accuracy_tracker.get_confusion_matrix()
        }
    
    def record_ground_truth(self, predicted_emotion: str, actual_emotion: str):
        """è®°å½•çœŸå®æ ‡ç­¾ç”¨äºå‡†ç¡®ç‡è®¡ç®—"""
        self.accuracy_tracker.record_prediction(predicted_emotion, actual_emotion)

# å…¨å±€å®ä¾‹
production_emotion_recognizer = ProductionEmotionRecognition()

async def analyze_production_emotion(
    audio_data: Optional[bytes] = None,
    text: Optional[str] = None,
    user_id: str = "default",
    context: Optional[Dict] = None
) -> EmotionResult:
    """ç”Ÿäº§çº§æƒ…æ„Ÿåˆ†æä¾¿æ·å‡½æ•°"""
    return await production_emotion_recognizer.analyze_emotion(
        audio_data=audio_data,
        text=text,
        user_id=user_id,
        context=context
    )

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_production_emotion():
        print("ğŸ§  æµ‹è¯•ç”Ÿäº§çº§æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ")
        print("=" * 50)
        
        # æµ‹è¯•æ•°æ®
        test_cases = [
            {"text": "æˆ‘ä»Šå¤©éå¸¸å¼€å¿ƒï¼Œå·¥ä½œè¿›å±•å¾ˆé¡ºåˆ©ï¼", "expected": "happy"},
            {"text": "è¿™ä¸ªç»“æœè®©æˆ‘å¾ˆå¤±æœ›å’Œæ²®ä¸§", "expected": "sad"},
            {"text": "æˆ‘å¯¹è¿™ä»¶äº‹æ„Ÿåˆ°å¾ˆæ„¤æ€’", "expected": "angry"},
            {"text": "å¥½çš„ï¼Œæˆ‘çŸ¥é“äº†", "expected": "neutral"},
            {"text": "å¤ªæ£’äº†ï¼è¿™æ­£æ˜¯æˆ‘æƒ³è¦çš„", "expected": "excited"},
        ]
        
        correct_predictions = 0
        
        for i, case in enumerate(test_cases, 1):
            # ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
            audio_data = f"audio_for_{case['text']}".encode()
            
            # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
            result = await analyze_production_emotion(
                audio_data=audio_data,
                text=case["text"],
                user_id=f"test_user_{i}"
            )
            
            # æ£€æŸ¥é¢„æµ‹ç»“æœ
            is_correct = result.primary_emotion == case["expected"]
            if is_correct:
                correct_predictions += 1
            
            # è®°å½•çœŸå®æ ‡ç­¾
            production_emotion_recognizer.record_ground_truth(
                result.primary_emotion, case["expected"]
            )
            
            print(f"æµ‹è¯• {i}: {case['text'][:20]}...")
            print(f"  é¢„æœŸ: {case['expected']}")
            print(f"  é¢„æµ‹: {result.primary_emotion}")
            print(f"  ç½®ä¿¡åº¦: {result.confidence:.3f}")
            print(f"  å¤„ç†æ—¶é—´: {result.processing_time*1000:.2f}ms")
            print(f"  ç‰¹å¾: {', '.join(result.features_used)}")
            print(f"  ç»“æœ: {'âœ…' if is_correct else 'âŒ'}")
            print()
        
        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = correct_predictions / len(test_cases)
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.1%}")
        print(f"  æ­£ç¡®é¢„æµ‹: {correct_predictions}/{len(test_cases)}")
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = production_emotion_recognizer.get_performance_stats()
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        for key, value in stats.items():
            if key != "confusion_matrix":
                print(f"  {key}: {value}")
        
        return accuracy >= 0.8  # 80%å‡†ç¡®ç‡é€šè¿‡
    
    # è¿è¡Œæµ‹è¯•
    import asyncio
    success = asyncio.run(test_production_emotion())
    print(f"\nğŸ¯ æµ‹è¯•{'é€šè¿‡' if success else 'å¤±è´¥'}ï¼")

"""
VoiceHelper é•¿æ–‡æœ¬å¤„ç†ç³»ç»Ÿ
æ”¯æŒ200K tokensä¸Šä¸‹æ–‡çª—å£ï¼Œå¯¹æ ‡Claude 3.5
å®ç°åˆ†å±‚æ³¨æ„åŠ›ã€æ»‘åŠ¨çª—å£å’Œä¸Šä¸‹æ–‡å‹ç¼©
"""

import asyncio
import time
import logging
import json
import math
from typing import Dict, List, Optional, Tuple, Any, AsyncIterator
from dataclasses import dataclass
from collections import deque
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class TextChunk:
    """æ–‡æœ¬å—"""
    content: str
    start_pos: int
    end_pos: int
    importance_score: float
    chunk_type: str  # "header", "content", "summary", "code"
    metadata: Dict[str, Any]

@dataclass
class ContextWindow:
    """ä¸Šä¸‹æ–‡çª—å£"""
    chunks: List[TextChunk]
    total_tokens: int
    compression_ratio: float
    processing_time: float

@dataclass
class LongTextResult:
    """é•¿æ–‡æœ¬å¤„ç†ç»“æœ"""
    processed_content: str
    context_windows: List[ContextWindow]
    total_tokens: int
    compression_ratio: float
    processing_time: float
    summary: str
    key_points: List[str]

class TokenCounter:
    """Tokenè®¡æ•°å™¨"""
    
    @staticmethod
    def count_tokens(text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ (ç®€åŒ–ç‰ˆ)"""
        # ä¸­æ–‡ï¼š1ä¸ªå­—ç¬¦çº¦ç­‰äº1ä¸ªtoken
        # è‹±æ–‡ï¼š1ä¸ªå•è¯çº¦ç­‰äº1-2ä¸ªtoken
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        english_words = len([w for w in text.split() if any(c.isalpha() for c in w)])
        other_chars = len(text) - chinese_chars - sum(len(w) for w in text.split() if any(c.isalpha() for c in w))
        
        return chinese_chars + english_words * 1.5 + other_chars * 0.5

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 1000, overlap: int = 200):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
    def chunk_text(self, text: str) -> List[TextChunk]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_by_paragraphs(text)
        
        current_chunk = ""
        current_start = 0
        
        for para in paragraphs:
            para_tokens = TokenCounter.count_tokens(para)
            current_tokens = TokenCounter.count_tokens(current_chunk)
            
            if current_tokens + para_tokens > self.chunk_size and current_chunk:
                # åˆ›å»ºå½“å‰å—
                chunk = self._create_chunk(
                    current_chunk, 
                    current_start, 
                    current_start + len(current_chunk)
                )
                chunks.append(chunk)
                
                # å¼€å§‹æ–°å—ï¼Œä¿ç•™é‡å 
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + para
                current_start = current_start + len(current_chunk) - len(overlap_text)
            else:
                current_chunk += para
        
        # å¤„ç†æœ€åä¸€å—
        if current_chunk:
            chunk = self._create_chunk(
                current_chunk, 
                current_start, 
                current_start + len(current_chunk)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _split_by_paragraphs(self, text: str) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰å¤šç§åˆ†éš”ç¬¦åˆ†å‰²
        separators = ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
        
        paragraphs = [text]
        for sep in separators:
            new_paragraphs = []
            for para in paragraphs:
                new_paragraphs.extend(para.split(sep))
            paragraphs = [p.strip() for p in new_paragraphs if p.strip()]
        
        return paragraphs
    
    def _get_overlap_text(self, text: str) -> str:
        """è·å–é‡å æ–‡æœ¬"""
        if len(text) <= self.overlap:
            return text
        return text[-self.overlap:]
    
    def _create_chunk(self, content: str, start: int, end: int) -> TextChunk:
        """åˆ›å»ºæ–‡æœ¬å—"""
        # è®¡ç®—é‡è¦æ€§åˆ†æ•°
        importance = self._calculate_importance(content)
        
        # åˆ¤æ–­å—ç±»å‹
        chunk_type = self._classify_chunk(content)
        
        return TextChunk(
            content=content,
            start_pos=start,
            end_pos=end,
            importance_score=importance,
            chunk_type=chunk_type,
            metadata={
                "token_count": TokenCounter.count_tokens(content),
                "char_count": len(content),
                "has_code": "```" in content or "def " in content,
                "has_numbers": any(c.isdigit() for c in content),
                "language": self._detect_language(content)
            }
        )
    
    def _calculate_importance(self, content: str) -> float:
        """è®¡ç®—å†…å®¹é‡è¦æ€§"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # å…³é”®è¯æƒé‡
        keywords = ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "æ€»ç»“", "ç»“è®º", "é—®é¢˜", "è§£å†³", "æ–¹æ¡ˆ"]
        for keyword in keywords:
            score += content.count(keyword) * 0.1
        
        # ç»“æ„åŒ–å†…å®¹æƒé‡
        if any(marker in content for marker in ["#", "##", "###", "1.", "2.", "3."]):
            score += 0.2
        
        # ä»£ç å—æƒé‡
        if "```" in content or "def " in content:
            score += 0.3
        
        return min(score, 1.0)
    
    def _classify_chunk(self, content: str) -> str:
        """åˆ†ç±»æ–‡æœ¬å—"""
        if content.startswith("#") or content.startswith("##"):
            return "header"
        elif "```" in content or "def " in content or "class " in content:
            return "code"
        elif any(word in content for word in ["æ€»ç»“", "ç»“è®º", "æ¦‚è¿°", "æ‘˜è¦"]):
            return "summary"
        else:
            return "content"
    
    def _detect_language(self, content: str) -> str:
        """æ£€æµ‹è¯­è¨€"""
        chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
        total_chars = len(content)
        
        if chinese_chars / max(total_chars, 1) > 0.3:
            return "chinese"
        else:
            return "english"

class HierarchicalAttention:
    """åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self):
        self.attention_weights = {}
        
    def compute_attention(self, chunks: List[TextChunk], query: str) -> List[float]:
        """è®¡ç®—æ³¨æ„åŠ›æƒé‡"""
        query_tokens = set(query.lower().split())
        attention_scores = []
        
        for chunk in chunks:
            # åŸºäºå†…å®¹ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›
            content_tokens = set(chunk.content.lower().split())
            overlap = len(query_tokens & content_tokens)
            content_score = overlap / max(len(query_tokens), 1)
            
            # åŸºäºé‡è¦æ€§çš„æ³¨æ„åŠ›
            importance_score = chunk.importance_score
            
            # åŸºäºä½ç½®çš„æ³¨æ„åŠ› (å¼€å¤´å’Œç»“å°¾æ›´é‡è¦)
            position_score = self._calculate_position_score(chunk, len(chunks))
            
            # åŸºäºç±»å‹çš„æ³¨æ„åŠ›
            type_score = self._get_type_weight(chunk.chunk_type)
            
            # ç»¼åˆæ³¨æ„åŠ›åˆ†æ•°
            final_score = (
                content_score * 0.4 + 
                importance_score * 0.3 + 
                position_score * 0.2 + 
                type_score * 0.1
            )
            
            attention_scores.append(final_score)
        
        # å½’ä¸€åŒ–
        total_score = sum(attention_scores)
        if total_score > 0:
            attention_scores = [score / total_score for score in attention_scores]
        
        return attention_scores
    
    def _calculate_position_score(self, chunk: TextChunk, total_chunks: int) -> float:
        """è®¡ç®—ä½ç½®åˆ†æ•°"""
        # å¼€å¤´å’Œç»“å°¾çš„å—æ›´é‡è¦
        chunk_index = chunk.start_pos / max(chunk.end_pos, 1)  # ç®€åŒ–çš„ä½ç½®è®¡ç®—
        
        if chunk_index < 0.1 or chunk_index > 0.9:  # å¼€å¤´æˆ–ç»“å°¾
            return 1.0
        elif chunk_index < 0.3 or chunk_index > 0.7:  # é è¿‘å¼€å¤´æˆ–ç»“å°¾
            return 0.8
        else:  # ä¸­é—´éƒ¨åˆ†
            return 0.6
    
    def _get_type_weight(self, chunk_type: str) -> float:
        """è·å–ç±»å‹æƒé‡"""
        type_weights = {
            "header": 1.0,
            "summary": 0.9,
            "code": 0.8,
            "content": 0.7
        }
        return type_weights.get(chunk_type, 0.5)

class ContextCompressor:
    """ä¸Šä¸‹æ–‡å‹ç¼©å™¨"""
    
    def __init__(self, target_ratio: float = 0.3):
        self.target_ratio = target_ratio
        
    async def compress_context(
        self, 
        chunks: List[TextChunk], 
        max_tokens: int = 200000
    ) -> List[TextChunk]:
        """å‹ç¼©ä¸Šä¸‹æ–‡"""
        total_tokens = sum(chunk.metadata["token_count"] for chunk in chunks)
        
        if total_tokens <= max_tokens:
            return chunks
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_chunks = sorted(chunks, key=lambda x: x.importance_score, reverse=True)
        
        # é€‰æ‹©æœ€é‡è¦çš„å—
        selected_chunks = []
        current_tokens = 0
        target_tokens = int(max_tokens * self.target_ratio)
        
        for chunk in sorted_chunks:
            chunk_tokens = chunk.metadata["token_count"]
            if current_tokens + chunk_tokens <= target_tokens:
                selected_chunks.append(chunk)
                current_tokens += chunk_tokens
            else:
                # å¦‚æœæ˜¯é‡è¦å—ï¼Œå°è¯•æˆªæ–­
                if chunk.importance_score > 0.8:
                    remaining_tokens = target_tokens - current_tokens
                    if remaining_tokens > 100:  # è‡³å°‘ä¿ç•™100ä¸ªtoken
                        compressed_chunk = self._truncate_chunk(chunk, remaining_tokens)
                        selected_chunks.append(compressed_chunk)
                        break
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        selected_chunks.sort(key=lambda x: x.start_pos)
        
        return selected_chunks
    
    def _truncate_chunk(self, chunk: TextChunk, max_tokens: int) -> TextChunk:
        """æˆªæ–­æ–‡æœ¬å—"""
        content = chunk.content
        current_tokens = 0
        truncated_content = ""
        
        sentences = content.split('ã€‚')
        for sentence in sentences:
            sentence_tokens = TokenCounter.count_tokens(sentence)
            if current_tokens + sentence_tokens <= max_tokens:
                truncated_content += sentence + "ã€‚"
                current_tokens += sentence_tokens
            else:
                break
        
        # åˆ›å»ºæ–°çš„æˆªæ–­å—
        return TextChunk(
            content=truncated_content,
            start_pos=chunk.start_pos,
            end_pos=chunk.start_pos + len(truncated_content),
            importance_score=chunk.importance_score,
            chunk_type=chunk.chunk_type,
            metadata={
                **chunk.metadata,
                "token_count": current_tokens,
                "truncated": True
            }
        )

class SlidingWindowProcessor:
    """æ»‘åŠ¨çª—å£å¤„ç†å™¨"""
    
    def __init__(self, window_size: int = 50000, step_size: int = 25000):
        self.window_size = window_size
        self.step_size = step_size
        
    async def process_with_sliding_window(
        self, 
        chunks: List[TextChunk]
    ) -> List[ContextWindow]:
        """ä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†"""
        windows = []
        current_pos = 0
        
        while current_pos < len(chunks):
            # é€‰æ‹©å½“å‰çª—å£çš„å—
            window_chunks = []
            window_tokens = 0
            
            for i in range(current_pos, len(chunks)):
                chunk = chunks[i]
                chunk_tokens = chunk.metadata["token_count"]
                
                if window_tokens + chunk_tokens <= self.window_size:
                    window_chunks.append(chunk)
                    window_tokens += chunk_tokens
                else:
                    break
            
            if window_chunks:
                # åˆ›å»ºä¸Šä¸‹æ–‡çª—å£
                window = ContextWindow(
                    chunks=window_chunks,
                    total_tokens=window_tokens,
                    compression_ratio=window_tokens / sum(c.metadata["token_count"] for c in window_chunks),
                    processing_time=0.0
                )
                windows.append(window)
            
            # ç§»åŠ¨çª—å£
            current_pos += max(1, len(window_chunks) // 2)  # 50%é‡å 
            
            if current_pos >= len(chunks):
                break
        
        return windows

class LongContextProcessor:
    """é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡å¤„ç†å™¨"""
    
    def __init__(self, max_context_tokens: int = 200000):
        self.max_context_tokens = max_context_tokens
        self.chunker = TextChunker()
        self.attention = HierarchicalAttention()
        self.compressor = ContextCompressor()
        self.sliding_window = SlidingWindowProcessor()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_processed = 0
        self.total_processing_time = 0.0
        
    async def process_long_text(
        self, 
        text: str, 
        query: Optional[str] = None,
        preserve_structure: bool = True
    ) -> LongTextResult:
        """å¤„ç†é•¿æ–‡æœ¬"""
        start_time = time.time()
        
        try:
            # 1. æ–‡æœ¬åˆ†å—
            chunks = self.chunker.chunk_text(text)
            logger.info(f"Text chunked into {len(chunks)} chunks")
            
            # 2. è®¡ç®—æ³¨æ„åŠ›æƒé‡
            if query:
                attention_weights = self.attention.compute_attention(chunks, query)
                # æ ¹æ®æ³¨æ„åŠ›æƒé‡è°ƒæ•´é‡è¦æ€§
                for i, chunk in enumerate(chunks):
                    chunk.importance_score = (chunk.importance_score + attention_weights[i]) / 2
            
            # 3. ä¸Šä¸‹æ–‡å‹ç¼©
            compressed_chunks = await self.compressor.compress_context(chunks, self.max_context_tokens)
            logger.info(f"Context compressed to {len(compressed_chunks)} chunks")
            
            # 4. æ»‘åŠ¨çª—å£å¤„ç†
            context_windows = await self.sliding_window.process_with_sliding_window(compressed_chunks)
            
            # 5. ç”Ÿæˆæ‘˜è¦å’Œå…³é”®ç‚¹
            summary = await self._generate_summary(compressed_chunks)
            key_points = await self._extract_key_points(compressed_chunks)
            
            # 6. é‡æ„å¤„ç†åçš„å†…å®¹
            processed_content = self._reconstruct_content(compressed_chunks, preserve_structure)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_tokens = sum(chunk.metadata["token_count"] for chunk in chunks)
            compressed_tokens = sum(chunk.metadata["token_count"] for chunk in compressed_chunks)
            compression_ratio = compressed_tokens / max(total_tokens, 1)
            processing_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_processed += 1
            self.total_processing_time += processing_time
            
            result = LongTextResult(
                processed_content=processed_content,
                context_windows=context_windows,
                total_tokens=compressed_tokens,
                compression_ratio=compression_ratio,
                processing_time=processing_time,
                summary=summary,
                key_points=key_points
            )
            
            logger.info(f"Long text processed in {processing_time:.2f}s, compression ratio: {compression_ratio:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"Long text processing error: {e}")
            # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºfallback
            return LongTextResult(
                processed_content=text[:self.max_context_tokens],  # ç®€å•æˆªæ–­
                context_windows=[],
                total_tokens=TokenCounter.count_tokens(text),
                compression_ratio=1.0,
                processing_time=time.time() - start_time,
                summary="å¤„ç†å‡ºé”™ï¼Œè¿”å›åŸå§‹å†…å®¹",
                key_points=["å¤„ç†å¤±è´¥"]
            )
    
    async def _generate_summary(self, chunks: List[TextChunk]) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        # é€‰æ‹©æœ€é‡è¦çš„å‡ ä¸ªå—ç”Ÿæˆæ‘˜è¦
        important_chunks = sorted(chunks, key=lambda x: x.importance_score, reverse=True)[:5]
        
        summary_parts = []
        for chunk in important_chunks:
            # æå–æ¯ä¸ªå—çš„å…³é”®å¥å­
            sentences = chunk.content.split('ã€‚')
            if sentences:
                key_sentence = max(sentences, key=len)  # ç®€å•é€‰æ‹©æœ€é•¿çš„å¥å­
                summary_parts.append(key_sentence.strip())
        
        return "ã€‚".join(summary_parts[:3]) + "ã€‚"  # æœ€å¤š3å¥æ‘˜è¦
    
    async def _extract_key_points(self, chunks: List[TextChunk]) -> List[str]:
        """æå–å…³é”®ç‚¹"""
        key_points = []
        
        for chunk in chunks:
            if chunk.chunk_type == "header":
                # æ ‡é¢˜ä½œä¸ºå…³é”®ç‚¹
                key_points.append(chunk.content.strip())
            elif chunk.importance_score > 0.8:
                # é«˜é‡è¦æ€§å†…å®¹çš„ç¬¬ä¸€å¥
                sentences = chunk.content.split('ã€‚')
                if sentences:
                    key_points.append(sentences[0].strip())
        
        return key_points[:10]  # æœ€å¤š10ä¸ªå…³é”®ç‚¹
    
    def _reconstruct_content(self, chunks: List[TextChunk], preserve_structure: bool) -> str:
        """é‡æ„å†…å®¹"""
        if preserve_structure:
            # ä¿æŒåŸæœ‰ç»“æ„
            content_parts = []
            for chunk in chunks:
                if chunk.chunk_type == "header":
                    content_parts.append(f"\n{chunk.content}\n")
                else:
                    content_parts.append(chunk.content)
            return "".join(content_parts)
        else:
            # ç®€å•è¿æ¥
            return " ".join(chunk.content for chunk in chunks)
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        avg_processing_time = (
            self.total_processing_time / self.total_processed 
            if self.total_processed > 0 else 0
        )
        
        return {
            "total_processed": self.total_processed,
            "average_processing_time_s": avg_processing_time,
            "max_context_tokens": self.max_context_tokens,
            "chunker_settings": {
                "chunk_size": self.chunker.chunk_size,
                "overlap": self.chunker.overlap
            }
        }

# å…¨å±€å®ä¾‹
long_context_processor = LongContextProcessor()

async def process_long_context(
    text: str,
    query: Optional[str] = None,
    max_tokens: int = 200000,
    preserve_structure: bool = True
) -> LongTextResult:
    """é•¿æ–‡æœ¬å¤„ç†ä¾¿æ·å‡½æ•°"""
    # åŠ¨æ€è°ƒæ•´æœ€å¤§tokenæ•°
    if max_tokens != long_context_processor.max_context_tokens:
        long_context_processor.max_context_tokens = max_tokens
    
    return await long_context_processor.process_long_text(
        text=text,
        query=query,
        preserve_structure=preserve_structure
    )

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test_long_context():
        print("ğŸ“„ æµ‹è¯•é•¿æ–‡æœ¬å¤„ç†ç³»ç»Ÿ")
        print("=" * 50)
        
        # ç”Ÿæˆæµ‹è¯•é•¿æ–‡æœ¬
        test_text = """
# äººå·¥æ™ºèƒ½å‘å±•æŠ¥å‘Š

## 1. æ¦‚è¿°
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯å½“ä»Šç§‘æŠ€å‘å±•çš„é‡è¦æ–¹å‘ï¼Œæ¶‰åŠæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸã€‚

## 2. æŠ€æœ¯å‘å±•
### 2.1 æœºå™¨å­¦ä¹ 
æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ é€šè¿‡æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæ— ç›‘ç£å­¦ä¹ ä»æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼ï¼Œå¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

### 2.2 æ·±åº¦å­¦ä¹ 
æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰åœ¨åºåˆ—æ•°æ®å¤„ç†æ–¹é¢æœ‰ä¼˜åŠ¿ã€‚

```python
# ç¤ºä¾‹ä»£ç ï¼šç®€å•çš„ç¥ç»ç½‘ç»œ
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

### 2.3 è‡ªç„¶è¯­è¨€å¤„ç†
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©æœºå™¨ç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬è¯åµŒå…¥ã€æ³¨æ„åŠ›æœºåˆ¶ã€Transformeræ¶æ„ç­‰ã€‚

## 3. åº”ç”¨é¢†åŸŸ
### 3.1 è®¡ç®—æœºè§†è§‰
åœ¨å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ã€äººè„¸è¯†åˆ«ç­‰æ–¹é¢æœ‰å¹¿æ³›åº”ç”¨ã€‚

### 3.2 è¯­éŸ³è¯†åˆ«
å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå¹¿æ³›åº”ç”¨äºæ™ºèƒ½åŠ©æ‰‹ã€è¯­éŸ³è¾“å…¥ç­‰åœºæ™¯ã€‚

### 3.3 æ¨èç³»ç»Ÿ
åŸºäºç”¨æˆ·è¡Œä¸ºå’Œåå¥½è¿›è¡Œä¸ªæ€§åŒ–æ¨èã€‚

## 4. æŒ‘æˆ˜ä¸æœºé‡
### 4.1 æŠ€æœ¯æŒ‘æˆ˜
- æ•°æ®è´¨é‡å’Œéšç§ä¿æŠ¤
- ç®—æ³•å¯è§£é‡Šæ€§
- è®¡ç®—èµ„æºéœ€æ±‚
- æ¨¡å‹æ³›åŒ–èƒ½åŠ›

### 4.2 å‘å±•æœºé‡
- è·¨é¢†åŸŸåº”ç”¨æ‹“å±•
- ç¡¬ä»¶æŠ€æœ¯è¿›æ­¥
- å¼€æºç”Ÿæ€å®Œå–„
- äº§ä¸šæ•°å­—åŒ–è½¬å‹

## 5. æœªæ¥å±•æœ›
äººå·¥æ™ºèƒ½å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œåœ¨æ›´å¤šé¢†åŸŸå®ç°çªç ´ã€‚å…³é”®å‘å±•æ–¹å‘åŒ…æ‹¬ï¼š
1. é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰
2. å¤šæ¨¡æ€AIç³»ç»Ÿ
3. è¾¹ç¼˜AIè®¡ç®—
4. AIä¼¦ç†å’Œå®‰å…¨

## 6. ç»“è®º
äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ¨åŠ¨æŠ€æœ¯å‘å±•çš„åŒæ—¶ï¼Œå…³æ³¨å…¶ç¤¾ä¼šå½±å“å’Œä¼¦ç†é—®é¢˜ï¼Œç¡®ä¿AIæŠ€æœ¯çš„å¥åº·å‘å±•ã€‚

æ€»çš„æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå……æ»¡æœºé‡å’ŒæŒ‘æˆ˜çš„é¢†åŸŸï¼Œéœ€è¦æŒç»­çš„ç ”ç©¶å’Œåˆ›æ–°ã€‚
        """ * 10  # é‡å¤10æ¬¡æ¨¡æ‹Ÿé•¿æ–‡æœ¬
        
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
        print(f"ä¼°ç®—tokenæ•°: {TokenCounter.count_tokens(test_text)}")
        
        # æµ‹è¯•é•¿æ–‡æœ¬å¤„ç†
        query = "äººå·¥æ™ºèƒ½çš„ä¸»è¦æŠ€æœ¯å’Œåº”ç”¨é¢†åŸŸ"
        result = await process_long_context(
            text=test_text,
            query=query,
            max_tokens=50000,  # 50K tokensé™åˆ¶
            preserve_structure=True
        )
        
        print(f"\nå¤„ç†ç»“æœ:")
        print(f"  å¤„ç†åtokenæ•°: {result.total_tokens}")
        print(f"  å‹ç¼©æ¯”: {result.compression_ratio:.2f}")
        print(f"  å¤„ç†æ—¶é—´: {result.processing_time:.2f}s")
        print(f"  ä¸Šä¸‹æ–‡çª—å£æ•°: {len(result.context_windows)}")
        
        print(f"\næ‘˜è¦:")
        print(f"  {result.summary}")
        
        print(f"\nå…³é”®ç‚¹:")
        for i, point in enumerate(result.key_points[:5], 1):
            print(f"  {i}. {point}")
        
        print(f"\nå¤„ç†åå†…å®¹é¢„è§ˆ:")
        preview = result.processed_content[:500]
        print(f"  {preview}...")
        
        # æ€§èƒ½ç»Ÿè®¡
        stats = long_context_processor.get_performance_stats()
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        return result.total_tokens <= 50000 and result.compression_ratio < 1.0
    
    # è¿è¡Œæµ‹è¯•
    import asyncio
    success = asyncio.run(test_long_context())
    print(f"\nğŸ¯ æµ‹è¯•{'é€šè¿‡' if success else 'å¤±è´¥'}ï¼")

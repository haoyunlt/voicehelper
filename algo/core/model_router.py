"""
åŠ¨æ€æ¨¡å‹è·¯ç”±ç³»ç»Ÿ

æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨¡å‹ï¼Œé™ä½30-50%æˆæœ¬
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Tuple, Callable, Awaitable
from dataclasses import dataclass, field
from enum import Enum
import re
import json
import statistics

logger = logging.getLogger(__name__)


class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦"""
    SIMPLE = "simple"           # ç®€å•ä»»åŠ¡
    MODERATE = "moderate"       # ä¸­ç­‰ä»»åŠ¡
    COMPLEX = "complex"         # å¤æ‚ä»»åŠ¡
    VERY_COMPLEX = "very_complex"  # éå¸¸å¤æ‚ä»»åŠ¡


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    cost_per_1k_tokens: float  # æ¯1K tokençš„æˆæœ¬
    max_tokens: int
    capabilities: List[str]     # æ¨¡å‹èƒ½åŠ›
    performance_score: float    # æ€§èƒ½è¯„åˆ† (0-1)
    latency_ms: float          # å¹³å‡å»¶è¿Ÿ (æ¯«ç§’)
    suitable_complexity: List[TaskComplexity]  # é€‚åˆçš„ä»»åŠ¡å¤æ‚åº¦
    
    def calculate_cost(self, token_count: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        return (token_count / 1000.0) * self.cost_per_1k_tokens
    
    def get_cost_efficiency(self, performance_requirement: float) -> float:
        """è·å–æˆæœ¬æ•ˆç‡ (æ€§èƒ½/æˆæœ¬æ¯”)"""
        if self.cost_per_1k_tokens == 0:
            return float('inf')
        
        # å¦‚æœæ€§èƒ½ä¸æ»¡è¶³è¦æ±‚ï¼Œæ•ˆç‡ä¸º0
        if self.performance_score < performance_requirement:
            return 0.0
        
        return self.performance_score / self.cost_per_1k_tokens


@dataclass
class RoutingDecision:
    """è·¯ç”±å†³ç­–"""
    selected_model: str
    confidence: float
    reasoning: str
    estimated_cost: float
    estimated_performance: float
    alternatives: List[Tuple[str, float]]  # (model_name, score)


@dataclass
class TaskAnalysis:
    """ä»»åŠ¡åˆ†æç»“æœ"""
    complexity: TaskComplexity
    estimated_tokens: int
    required_capabilities: List[str]
    performance_requirement: float
    latency_requirement: float
    reasoning: str


class TaskAnalyzer:
    """ä»»åŠ¡åˆ†æå™¨"""
    
    def __init__(self):
        # å¤æ‚åº¦æ£€æµ‹è§„åˆ™
        self.complexity_patterns = {
            TaskComplexity.SIMPLE: [
                r'\b(hello|hi|thanks|bye)\b',
                r'\bwhat\s+is\s+\w+\?',
                r'\bdefine\s+\w+',
                r'\byes|no\b',
                r'^.{1,20}$'  # å¾ˆçŸ­çš„æ–‡æœ¬
            ],
            TaskComplexity.MODERATE: [
                r'\bhow\s+to\s+',
                r'\bexplain\s+',
                r'\bcompare\s+',
                r'\blist\s+',
                r'\bsummarize\s+',
                r'^.{21,100}$'  # ä¸­ç­‰é•¿åº¦æ–‡æœ¬
            ],
            TaskComplexity.COMPLEX: [
                r'\banalyze\s+',
                r'\bevaluate\s+',
                r'\bwrite\s+.*\b(essay|article|report)\b',
                r'\bcreate\s+.*\b(plan|strategy)\b',
                r'\bsolve\s+.*\b(problem|equation)\b',
                r'^.{101,300}$'  # è¾ƒé•¿æ–‡æœ¬
            ],
            TaskComplexity.VERY_COMPLEX: [
                r'\bdesign\s+.*\b(system|architecture)\b',
                r'\bdevelop\s+.*\b(algorithm|model)\b',
                r'\bresearch\s+',
                r'\boptimize\s+',
                r'^.{301,}$'  # å¾ˆé•¿æ–‡æœ¬
            ]
        }
        
        # èƒ½åŠ›éœ€æ±‚æ£€æµ‹
        self.capability_patterns = {
            'reasoning': [r'\banalyze\b', r'\bevaluate\b', r'\bcompare\b', r'\breason\b'],
            'coding': [r'\bcode\b', r'\bprogram\b', r'\bfunction\b', r'\balgorithm\b'],
            'math': [r'\bcalculate\b', r'\bsolve\b', r'\bequation\b', r'\bmathematics\b'],
            'creative': [r'\bwrite\b', r'\bcreate\b', r'\bgenerate\b', r'\bimagine\b'],
            'translation': [r'\btranslate\b', r'\btranslation\b'],
            'summarization': [r'\bsummarize\b', r'\bsummary\b', r'\babstract\b']
        }
    
    def analyze_task(self, content: str, context: Optional[Dict[str, Any]] = None) -> TaskAnalysis:
        """åˆ†æä»»åŠ¡"""
        content_lower = content.lower()
        
        # æ£€æµ‹å¤æ‚åº¦
        complexity = self._detect_complexity(content_lower)
        
        # ä¼°ç®—tokenæ•°é‡
        estimated_tokens = self._estimate_tokens(content)
        
        # æ£€æµ‹æ‰€éœ€èƒ½åŠ›
        required_capabilities = self._detect_capabilities(content_lower)
        
        # ç¡®å®šæ€§èƒ½è¦æ±‚
        performance_requirement = self._determine_performance_requirement(
            complexity, required_capabilities, context
        )
        
        # ç¡®å®šå»¶è¿Ÿè¦æ±‚
        latency_requirement = self._determine_latency_requirement(context)
        
        # ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning = self._generate_reasoning(
            complexity, required_capabilities, estimated_tokens
        )
        
        return TaskAnalysis(
            complexity=complexity,
            estimated_tokens=estimated_tokens,
            required_capabilities=required_capabilities,
            performance_requirement=performance_requirement,
            latency_requirement=latency_requirement,
            reasoning=reasoning
        )
    
    def _detect_complexity(self, content: str) -> TaskComplexity:
        """æ£€æµ‹ä»»åŠ¡å¤æ‚åº¦"""
        complexity_scores = {complexity: 0 for complexity in TaskComplexity}
        
        for complexity, patterns in self.complexity_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content):
                    complexity_scores[complexity] += 1
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„å¤æ‚åº¦
        max_complexity = max(complexity_scores.items(), key=lambda x: x[1])
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¨¡å¼ï¼Œæ ¹æ®é•¿åº¦åˆ¤æ–­
        if max_complexity[1] == 0:
            if len(content) < 50:
                return TaskComplexity.SIMPLE
            elif len(content) < 200:
                return TaskComplexity.MODERATE
            else:
                return TaskComplexity.COMPLEX
        
        return max_complexity[0]
    
    def _estimate_tokens(self, content: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦/tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦/token
        char_count = len(content)
        
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
        chinese_ratio = chinese_chars / char_count if char_count > 0 else 0
        
        # æ··åˆä¼°ç®—
        if chinese_ratio > 0.5:
            estimated_tokens = int(char_count / 1.5)
        else:
            estimated_tokens = int(char_count / 4)
        
        # è€ƒè™‘è¾“å‡ºtoken (é€šå¸¸æ˜¯è¾“å…¥çš„1-3å€)
        total_tokens = estimated_tokens * 2.5
        
        return int(total_tokens)
    
    def _detect_capabilities(self, content: str) -> List[str]:
        """æ£€æµ‹æ‰€éœ€èƒ½åŠ›"""
        required_capabilities = []
        
        for capability, patterns in self.capability_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content):
                    required_capabilities.append(capability)
                    break
        
        return required_capabilities
    
    def _determine_performance_requirement(
        self, 
        complexity: TaskComplexity, 
        capabilities: List[str], 
        context: Optional[Dict[str, Any]]
    ) -> float:
        """ç¡®å®šæ€§èƒ½è¦æ±‚"""
        base_requirements = {
            TaskComplexity.SIMPLE: 0.6,
            TaskComplexity.MODERATE: 0.7,
            TaskComplexity.COMPLEX: 0.8,
            TaskComplexity.VERY_COMPLEX: 0.9
        }
        
        requirement = base_requirements[complexity]
        
        # ç‰¹æ®Šèƒ½åŠ›éœ€æ±‚è°ƒæ•´
        if 'reasoning' in capabilities or 'math' in capabilities:
            requirement += 0.1
        
        if 'coding' in capabilities:
            requirement += 0.05
        
        # ä¸Šä¸‹æ–‡è°ƒæ•´
        if context:
            if context.get('user_tier') == 'premium':
                requirement += 0.1
            elif context.get('user_tier') == 'free':
                requirement -= 0.1
        
        return min(1.0, max(0.5, requirement))
    
    def _determine_latency_requirement(self, context: Optional[Dict[str, Any]]) -> float:
        """ç¡®å®šå»¶è¿Ÿè¦æ±‚ (æ¯«ç§’)"""
        if context:
            if context.get('real_time', False):
                return 500.0  # å®æ—¶åœºæ™¯è¦æ±‚500mså†…
            elif context.get('interactive', True):
                return 2000.0  # äº¤äº’åœºæ™¯è¦æ±‚2så†…
        
        return 5000.0  # é»˜è®¤5så†…
    
    def _generate_reasoning(
        self, 
        complexity: TaskComplexity, 
        capabilities: List[str], 
        tokens: int
    ) -> str:
        """ç”Ÿæˆæ¨ç†è¯´æ˜"""
        reasoning_parts = [
            f"ä»»åŠ¡å¤æ‚åº¦: {complexity.value}",
            f"é¢„ä¼°tokenæ•°: {tokens}",
        ]
        
        if capabilities:
            reasoning_parts.append(f"æ‰€éœ€èƒ½åŠ›: {', '.join(capabilities)}")
        
        return "; ".join(reasoning_parts)


class ModelRouter:
    """æ¨¡å‹è·¯ç”±å™¨"""
    
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self.task_analyzer = TaskAnalyzer()
        self.routing_history: List[Dict[str, Any]] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'cost_savings': 0.0,
            'performance_maintained': 0,
            'routing_decisions': {}
        }
        
        # åˆå§‹åŒ–é»˜è®¤æ¨¡å‹
        self._init_default_models()
    
    def _init_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ¨¡å‹é…ç½®"""
        models = [
            ModelConfig(
                name="gpt-4",
                cost_per_1k_tokens=0.03,
                max_tokens=8192,
                capabilities=['reasoning', 'coding', 'math', 'creative', 'translation', 'summarization'],
                performance_score=0.95,
                latency_ms=2000,
                suitable_complexity=[TaskComplexity.COMPLEX, TaskComplexity.VERY_COMPLEX]
            ),
            ModelConfig(
                name="gpt-3.5-turbo",
                cost_per_1k_tokens=0.002,
                max_tokens=4096,
                capabilities=['reasoning', 'coding', 'creative', 'translation', 'summarization'],
                performance_score=0.85,
                latency_ms=1000,
                suitable_complexity=[TaskComplexity.MODERATE, TaskComplexity.COMPLEX]
            ),
            ModelConfig(
                name="claude-3-haiku",
                cost_per_1k_tokens=0.00025,
                max_tokens=200000,
                capabilities=['reasoning', 'creative', 'translation', 'summarization'],
                performance_score=0.75,
                latency_ms=800,
                suitable_complexity=[TaskComplexity.SIMPLE, TaskComplexity.MODERATE]
            ),
            ModelConfig(
                name="gemini-pro",
                cost_per_1k_tokens=0.001,
                max_tokens=32768,
                capabilities=['reasoning', 'coding', 'math', 'creative'],
                performance_score=0.80,
                latency_ms=1200,
                suitable_complexity=[TaskComplexity.MODERATE, TaskComplexity.COMPLEX]
            )
        ]
        
        for model in models:
            self.models[model.name] = model
    
    def add_model(self, model_config: ModelConfig):
        """æ·»åŠ æ¨¡å‹é…ç½®"""
        self.models[model_config.name] = model_config
        logger.info(f"Added model: {model_config.name}")
    
    def route_request(
        self, 
        content: str, 
        context: Optional[Dict[str, Any]] = None,
        cost_priority: float = 0.7  # æˆæœ¬ä¼˜å…ˆçº§ (0-1, 1ä¸ºå®Œå…¨ä¼˜å…ˆæˆæœ¬)
    ) -> RoutingDecision:
        """è·¯ç”±è¯·æ±‚åˆ°æœ€ä¼˜æ¨¡å‹"""
        self.stats['total_requests'] += 1
        
        # åˆ†æä»»åŠ¡
        task_analysis = self.task_analyzer.analyze_task(content, context)
        
        # ç­›é€‰å€™é€‰æ¨¡å‹
        candidate_models = self._filter_candidate_models(task_analysis)
        
        if not candidate_models:
            # å¦‚æœæ²¡æœ‰å€™é€‰æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            default_model = "gpt-3.5-turbo"
            return RoutingDecision(
                selected_model=default_model,
                confidence=0.5,
                reasoning="No suitable models found, using default",
                estimated_cost=self.models[default_model].calculate_cost(task_analysis.estimated_tokens),
                estimated_performance=self.models[default_model].performance_score,
                alternatives=[]
            )
        
        # è¯„åˆ†å’Œé€‰æ‹©
        model_scores = self._score_models(candidate_models, task_analysis, cost_priority)
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model, best_score = max(model_scores.items(), key=lambda x: x[1])
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(model_scores, best_model)
        
        # ç”Ÿæˆæ¨ç†è¯´æ˜
        reasoning = self._generate_routing_reasoning(
            best_model, task_analysis, model_scores
        )
        
        # è®¡ç®—é¢„ä¼°æˆæœ¬å’Œæ€§èƒ½
        estimated_cost = self.models[best_model].calculate_cost(task_analysis.estimated_tokens)
        estimated_performance = self.models[best_model].performance_score
        
        # ç”Ÿæˆå¤‡é€‰æ–¹æ¡ˆ
        alternatives = sorted(
            [(model, score) for model, score in model_scores.items() if model != best_model],
            key=lambda x: x[1],
            reverse=True
        )[:3]
        
        decision = RoutingDecision(
            selected_model=best_model,
            confidence=confidence,
            reasoning=reasoning,
            estimated_cost=estimated_cost,
            estimated_performance=estimated_performance,
            alternatives=alternatives
        )
        
        # è®°å½•è·¯ç”±å†å²
        self._record_routing_decision(task_analysis, decision)
        
        return decision
    
    def _filter_candidate_models(self, task_analysis: TaskAnalysis) -> List[str]:
        """ç­›é€‰å€™é€‰æ¨¡å‹"""
        candidates = []
        
        for model_name, model_config in self.models.items():
            # æ£€æŸ¥å¤æ‚åº¦åŒ¹é…
            if task_analysis.complexity not in model_config.suitable_complexity:
                continue
            
            # æ£€æŸ¥tokené™åˆ¶
            if task_analysis.estimated_tokens > model_config.max_tokens:
                continue
            
            # æ£€æŸ¥æ€§èƒ½è¦æ±‚
            if model_config.performance_score < task_analysis.performance_requirement:
                continue
            
            # æ£€æŸ¥å»¶è¿Ÿè¦æ±‚
            if model_config.latency_ms > task_analysis.latency_requirement:
                continue
            
            # æ£€æŸ¥èƒ½åŠ›è¦æ±‚
            missing_capabilities = set(task_analysis.required_capabilities) - set(model_config.capabilities)
            if missing_capabilities:
                continue
            
            candidates.append(model_name)
        
        return candidates
    
    def _score_models(
        self, 
        candidates: List[str], 
        task_analysis: TaskAnalysis, 
        cost_priority: float
    ) -> Dict[str, float]:
        """ä¸ºå€™é€‰æ¨¡å‹è¯„åˆ†"""
        scores = {}
        
        for model_name in candidates:
            model_config = self.models[model_name]
            
            # æ€§èƒ½è¯„åˆ† (0-1)
            performance_score = model_config.performance_score
            
            # æˆæœ¬æ•ˆç‡è¯„åˆ† (0-1)
            cost_efficiency = model_config.get_cost_efficiency(task_analysis.performance_requirement)
            # æ ‡å‡†åŒ–æˆæœ¬æ•ˆç‡è¯„åˆ†
            max_efficiency = max(
                self.models[m].get_cost_efficiency(task_analysis.performance_requirement) 
                for m in candidates
            )
            if max_efficiency > 0:
                cost_score = cost_efficiency / max_efficiency
            else:
                cost_score = 0.0
            
            # å»¶è¿Ÿè¯„åˆ† (0-1, å»¶è¿Ÿè¶Šä½è¯„åˆ†è¶Šé«˜)
            max_latency = max(self.models[m].latency_ms for m in candidates)
            latency_score = 1.0 - (model_config.latency_ms / max_latency) if max_latency > 0 else 1.0
            
            # ç»¼åˆè¯„åˆ†
            final_score = (
                cost_priority * cost_score +
                (1 - cost_priority) * 0.7 * performance_score +
                (1 - cost_priority) * 0.3 * latency_score
            )
            
            scores[model_name] = final_score
        
        return scores
    
    def _calculate_confidence(self, model_scores: Dict[str, float], best_model: str) -> float:
        """è®¡ç®—è·¯ç”±å†³ç­–çš„ç½®ä¿¡åº¦"""
        if len(model_scores) < 2:
            return 1.0
        
        scores = list(model_scores.values())
        best_score = model_scores[best_model]
        
        # è®¡ç®—ä¸ç¬¬äºŒåçš„å·®è·
        scores.remove(best_score)
        second_best = max(scores)
        
        # ç½®ä¿¡åº¦åŸºäºåˆ†æ•°å·®è·
        confidence = min(1.0, (best_score - second_best) / best_score + 0.5)
        
        return confidence
    
    def _generate_routing_reasoning(
        self, 
        selected_model: str, 
        task_analysis: TaskAnalysis, 
        model_scores: Dict[str, float]
    ) -> str:
        """ç”Ÿæˆè·¯ç”±æ¨ç†è¯´æ˜"""
        model_config = self.models[selected_model]
        
        reasoning_parts = [
            f"é€‰æ‹© {selected_model}",
            f"ä»»åŠ¡å¤æ‚åº¦: {task_analysis.complexity.value}",
            f"é¢„ä¼°æˆæœ¬: ${model_config.calculate_cost(task_analysis.estimated_tokens):.4f}",
            f"æ€§èƒ½è¯„åˆ†: {model_config.performance_score:.2f}"
        ]
        
        if len(model_scores) > 1:
            reasoning_parts.append(f"å€™é€‰æ¨¡å‹æ•°: {len(model_scores)}")
        
        return "; ".join(reasoning_parts)
    
    def _record_routing_decision(self, task_analysis: TaskAnalysis, decision: RoutingDecision):
        """è®°å½•è·¯ç”±å†³ç­–"""
        record = {
            'timestamp': time.time(),
            'task_complexity': task_analysis.complexity.value,
            'estimated_tokens': task_analysis.estimated_tokens,
            'selected_model': decision.selected_model,
            'estimated_cost': decision.estimated_cost,
            'confidence': decision.confidence
        }
        
        self.routing_history.append(record)
        
        # ä¿æŒå†å²è®°å½•å¤§å°
        if len(self.routing_history) > 1000:
            self.routing_history = self.routing_history[-1000:]
        
        # æ›´æ–°ç»Ÿè®¡
        model_name = decision.selected_model
        if model_name not in self.stats['routing_decisions']:
            self.stats['routing_decisions'][model_name] = 0
        self.stats['routing_decisions'][model_name] += 1
    
    def calculate_cost_savings(self, baseline_model: str = "gpt-4") -> Dict[str, Any]:
        """è®¡ç®—æˆæœ¬èŠ‚çœ"""
        if not self.routing_history:
            return {'total_savings': 0.0, 'savings_rate': 0.0}
        
        baseline_config = self.models.get(baseline_model)
        if not baseline_config:
            return {'total_savings': 0.0, 'savings_rate': 0.0}
        
        total_actual_cost = 0.0
        total_baseline_cost = 0.0
        
        for record in self.routing_history:
            actual_cost = record['estimated_cost']
            baseline_cost = baseline_config.calculate_cost(record['estimated_tokens'])
            
            total_actual_cost += actual_cost
            total_baseline_cost += baseline_cost
        
        total_savings = total_baseline_cost - total_actual_cost
        savings_rate = total_savings / total_baseline_cost if total_baseline_cost > 0 else 0.0
        
        return {
            'total_savings': total_savings,
            'savings_rate': savings_rate,
            'total_actual_cost': total_actual_cost,
            'total_baseline_cost': total_baseline_cost
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡"""
        cost_savings = self.calculate_cost_savings()
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        if self.routing_history:
            avg_confidence = statistics.mean(r['confidence'] for r in self.routing_history)
        else:
            avg_confidence = 0.0
        
        return {
            **self.stats,
            **cost_savings,
            'avg_confidence': avg_confidence,
            'total_models': len(self.models),
            'routing_history_size': len(self.routing_history)
        }


# ä½¿ç”¨ç¤ºä¾‹
async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # åˆ›å»ºæ¨¡å‹è·¯ç”±å™¨
    router = ModelRouter()
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡
    test_cases = [
        {
            'content': "Hello, how are you?",
            'context': {'user_tier': 'free'},
            'expected_complexity': TaskComplexity.SIMPLE
        },
        {
            'content': "Can you explain how machine learning works?",
            'context': {'user_tier': 'standard'},
            'expected_complexity': TaskComplexity.MODERATE
        },
        {
            'content': "Write a comprehensive analysis of the economic impact of artificial intelligence on the job market, including statistical data and policy recommendations.",
            'context': {'user_tier': 'premium'},
            'expected_complexity': TaskComplexity.COMPLEX
        },
        {
            'content': "Design a distributed system architecture for a real-time recommendation engine that can handle 1 million concurrent users with sub-100ms latency.",
            'context': {'user_tier': 'enterprise'},
            'expected_complexity': TaskComplexity.VERY_COMPLEX
        }
    ]
    
    print("ğŸš€ Model Router Testing")
    print("=" * 50)
    
    for i, test_case in enumerate(test_cases):
        print(f"\n--- Test Case {i+1} ---")
        print(f"Content: {test_case['content'][:100]}...")
        
        # è·¯ç”±å†³ç­–
        decision = router.route_request(
            content=test_case['content'],
            context=test_case['context'],
            cost_priority=0.7
        )
        
        print(f"Selected Model: {decision.selected_model}")
        print(f"Confidence: {decision.confidence:.2f}")
        print(f"Estimated Cost: ${decision.estimated_cost:.4f}")
        print(f"Reasoning: {decision.reasoning}")
        
        if decision.alternatives:
            print(f"Alternatives: {', '.join([f'{m}({s:.2f})' for m, s in decision.alternatives])}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = router.get_stats()
    print(f"\nğŸ“Š Router Statistics:")
    print(f"Total Requests: {stats['total_requests']}")
    print(f"Cost Savings Rate: {stats['savings_rate']:.1%}")
    print(f"Total Savings: ${stats['total_savings']:.4f}")
    print(f"Average Confidence: {stats['avg_confidence']:.2f}")
    print(f"Model Usage: {stats['routing_decisions']}")


if __name__ == "__main__":
    asyncio.run(example_usage())

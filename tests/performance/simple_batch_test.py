"""
ç®€åŒ–çš„æ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•

ä¸ä¾èµ–å¤–éƒ¨åº“çš„åŸºç¡€æ€§èƒ½æµ‹è¯•
"""

import asyncio
import time
import statistics
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))


class MockLLMClient:
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""
    
    async def process_single(self, request):
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        # æ¨¡æ‹ŸçœŸå®LLMè°ƒç”¨ï¼šåŒ…å«ç½‘ç»œå»¶è¿Ÿå’Œæ¨¡å‹æ¨ç†æ—¶é—´
        network_latency = 0.05  # 50msç½‘ç»œå»¶è¿Ÿ
        model_inference = 0.8   # 800msæ¨¡å‹æ¨ç†æ—¶é—´
        
        await asyncio.sleep(network_latency)  # ç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(model_inference)  # æ¨¡å‹æ¨ç†
        
        content = request.get('content', 'Hello')
        return {
            'content': f"Response to: {content}",
            'processing_time': network_latency + model_inference,
            'model': request.get('model', 'gpt-3.5-turbo')
        }
    
    async def process_batch(self, requests):
        """å¤„ç†æ‰¹é‡è¯·æ±‚"""
        batch_size = len(requests)
        
        # æ¨¡æ‹ŸçœŸå®æ‰¹å¤„ç†ï¼šç½‘ç»œå»¶è¿Ÿåªæœ‰ä¸€æ¬¡ï¼Œæ¨¡å‹æ¨ç†æœ‰æ‰¹å¤„ç†æ•ˆç‡
        network_latency = 0.05  # 50msç½‘ç»œå»¶è¿Ÿï¼ˆåªæœ‰ä¸€æ¬¡ï¼‰
        base_inference_time = 0.8  # 800msåŸºç¡€æ¨ç†æ—¶é—´
        
        # æ‰¹å¤„ç†æ•ˆç‡ï¼šæ‰¹æ¬¡è¶Šå¤§ï¼Œå•ä¸ªè¯·æ±‚çš„å¹³å‡æ¨ç†æ—¶é—´è¶ŠçŸ­
        batch_efficiency = min(2.0, 1.0 + (batch_size - 1) * 0.15)  # æ¯å¢åŠ ä¸€ä¸ªè¯·æ±‚ï¼Œæ•ˆç‡æå‡15%
        actual_inference_time = base_inference_time / batch_efficiency
        
        total_processing_time = network_latency + actual_inference_time
        
        await asyncio.sleep(total_processing_time)
        
        results = []
        for req in requests:
            content = req.get('content', 'Hello')
            result = {
                'content': f"Batch response to: {content}",
                'processing_time': total_processing_time,
                'model': req.get('model', 'gpt-3.5-turbo')
            }
            results.append(result)
        
        return results


class SimpleBatchProcessor:
    """ç®€åŒ–çš„æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, batch_size=4, max_wait_time=0.1):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.client = MockLLMClient()
        self.queue = asyncio.Queue()
        self.running = False
        self.processing_task = None
        
    async def start(self):
        """å¯åŠ¨æ‰¹å¤„ç†å™¨"""
        if not self.running:
            self.running = True
            self.processing_task = asyncio.create_task(self._processing_loop())
    
    async def stop(self):
        """åœæ­¢æ‰¹å¤„ç†å™¨"""
        self.running = False
        if self.processing_task:
            self.processing_task.cancel()
            try:
                await self.processing_task
            except asyncio.CancelledError:
                pass
    
    async def process_request(self, request):
        """å¤„ç†è¯·æ±‚"""
        future = asyncio.Future()
        await self.queue.put({'request': request, 'future': future})
        return await future
    
    async def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†æ‰¹æ¬¡
                batch = []
                futures = []
                batch_start = time.time()
                
                # æ”¶é›†è¯·æ±‚ç›´åˆ°è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶
                while len(batch) < self.batch_size and self.running:
                    elapsed = time.time() - batch_start
                    remaining_wait = self.max_wait_time - elapsed
                    
                    if remaining_wait <= 0:
                        break
                    
                    try:
                        item = await asyncio.wait_for(
                            self.queue.get(), 
                            timeout=min(remaining_wait, 0.01)
                        )
                        batch.append(item['request'])
                        futures.append(item['future'])
                    except asyncio.TimeoutError:
                        break
                
                # å¤„ç†æ‰¹æ¬¡
                if batch:
                    try:
                        results = await self.client.process_batch(batch)
                        for future, result in zip(futures, results):
                            if not future.done():
                                future.set_result(result)
                    except Exception as e:
                        for future in futures:
                            if not future.done():
                                future.set_exception(e)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Processing error: {e}")


async def run_performance_test():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ æ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•å‚æ•°
    total_requests = 40  # å‡å°‘è¯·æ±‚æ•°ä»¥ä¾¿è§‚å¯Ÿæ‰¹å¤„ç†æ•ˆæœ
    concurrent_users = 8  # å¢åŠ å¹¶å‘ç”¨æˆ·
    
    # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
    test_requests = []
    for i in range(total_requests):
        request = {
            'content': f"Test request {i}",
            'model': 'gpt-3.5-turbo'
        }
        test_requests.append(request)
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"  å¹¶å‘ç”¨æˆ·: {concurrent_users}")
    print(f"  æ‰¹æ¬¡å¤§å°: 4")
    print(f"  æœ€å¤§ç­‰å¾…: 0.1s")
    print(f"  æ¨¡æ‹Ÿåœºæ™¯: çœŸå®LLMè°ƒç”¨ (ç½‘ç»œ50ms + æ¨ç†800ms)")
    
    # æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼")
    batch_processor = SimpleBatchProcessor(batch_size=4, max_wait_time=0.1)
    await batch_processor.start()
    
    async def send_batch_request(req, delay):
        await asyncio.sleep(delay)
        return await batch_processor.process_request(req)
    
    start_time = time.time()
    batch_tasks = []
    for i, req in enumerate(test_requests):
        delay = (i // concurrent_users) * 0.01  # æ¨¡æ‹Ÿç”¨æˆ·è¯·æ±‚é—´éš”
        task = asyncio.create_task(send_batch_request(req, delay))
        batch_tasks.append(task)
    
    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
    batch_total_time = time.time() - start_time
    
    await batch_processor.stop()
    
    # æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼")
    client = MockLLMClient()
    
    async def send_direct_request(req, delay):
        await asyncio.sleep(delay)
        return await client.process_single(req)
    
    start_time = time.time()
    direct_tasks = []
    for i, req in enumerate(test_requests):
        delay = (i // concurrent_users) * 0.01
        task = asyncio.create_task(send_direct_request(req, delay))
        direct_tasks.append(task)
    
    direct_results = await asyncio.gather(*direct_tasks, return_exceptions=True)
    direct_total_time = time.time() - start_time
    
    # ç»Ÿè®¡ç»“æœ
    def analyze_results(results, total_time, mode_name):
        successful = [r for r in results if not isinstance(r, Exception)]
        failed = len(results) - len(successful)
        
        if successful:
            response_times = [r.get('processing_time', 0) for r in successful]
            avg_response_time = statistics.mean(response_times)
            # è®¡ç®—P95 (å…¼å®¹Python 3.7)
            if len(response_times) > 1:
                sorted_times = sorted(response_times)
                p95_index = int(0.95 * len(sorted_times))
                p95_response_time = sorted_times[min(p95_index, len(sorted_times) - 1)]
            else:
                p95_response_time = avg_response_time
        else:
            avg_response_time = 0
            p95_response_time = 0
        
        throughput = len(successful) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“ˆ {mode_name} ç»“æœ:")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful)}/{len(results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {failed}")
        print(f"  æ€»è€—æ—¶: {total_time:.3f}s")
        print(f"  ååé‡: {throughput:.2f} req/s")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_response_time:.3f}s")
        print(f"  P95å»¶è¿Ÿ: {p95_response_time:.3f}s")
        
        return {
            'successful': len(successful),
            'failed': failed,
            'total_time': total_time,
            'throughput': throughput,
            'avg_latency': avg_response_time,
            'p95_latency': p95_response_time
        }
    
    batch_stats = analyze_results(batch_results, batch_total_time, "æ‰¹å¤„ç†æ¨¡å¼")
    direct_stats = analyze_results(direct_results, direct_total_time, "ç›´æ¥å¤„ç†æ¨¡å¼")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ¯ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
    print("=" * 50)
    
    if direct_stats['throughput'] > 0:
        throughput_improvement = (batch_stats['throughput'] - direct_stats['throughput']) / direct_stats['throughput']
        print(f"ååé‡æå‡: {throughput_improvement:.1%}")
        
        if throughput_improvement >= 0.30:
            print("âœ… è¾¾åˆ°30%+ååé‡æå‡ç›®æ ‡!")
        else:
            print("âŒ æœªè¾¾åˆ°30%ååé‡æå‡ç›®æ ‡")
    
    if direct_stats['avg_latency'] > 0:
        latency_change = (batch_stats['avg_latency'] - direct_stats['avg_latency']) / direct_stats['avg_latency']
        print(f"å»¶è¿Ÿå˜åŒ–: {latency_change:.1%}")
    
    efficiency_gain = batch_stats['throughput'] / direct_stats['throughput'] if direct_stats['throughput'] > 0 else 1
    print(f"æ•´ä½“æ•ˆç‡æå‡: {efficiency_gain:.2f}x")
    
    # èµ„æºæ•ˆç‡åˆ†æ
    print(f"\nğŸ“Š èµ„æºæ•ˆç‡åˆ†æ:")
    batch_req_per_sec = batch_stats['successful'] / batch_stats['total_time']
    direct_req_per_sec = direct_stats['successful'] / direct_stats['total_time']
    
    print(f"æ‰¹å¤„ç†æ¨¡å¼: {batch_req_per_sec:.2f} req/s")
    print(f"ç›´æ¥å¤„ç†æ¨¡å¼: {direct_req_per_sec:.2f} req/s")
    print(f"æ•ˆç‡æ¯”å€¼: {batch_req_per_sec / direct_req_per_sec:.2f}")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    return {
        'batch': batch_stats,
        'direct': direct_stats,
        'improvement': throughput_improvement if direct_stats['throughput'] > 0 else 0
    }


if __name__ == "__main__":
    asyncio.run(run_performance_test())

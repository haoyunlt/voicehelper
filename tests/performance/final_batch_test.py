"""
æœ€ç»ˆæ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•

ä¸“é—¨è®¾è®¡æ¥å±•ç¤ºæ‰¹å¤„ç†ä¼˜åŠ¿çš„æµ‹è¯•åœºæ™¯
"""

import asyncio
import time
import statistics
import sys
import os
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))


class OptimizedLLMClient:
    """ä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯ - çªå‡ºæ‰¹å¤„ç†ä¼˜åŠ¿"""
    
    def __init__(self):
        # æ¨¡æ‹ŸçœŸå®LLMæœåŠ¡çš„é™åˆ¶
        self.api_rate_limit = asyncio.Semaphore(3)  # ä¸¥æ ¼çš„APIå¹¶å‘é™åˆ¶
        self.request_count = 0
        self.start_time = time.time()
    
    async def process_single(self, request):
        """å¤„ç†å•ä¸ªè¯·æ±‚ - å—ä¸¥æ ¼é™åˆ¶"""
        async with self.api_rate_limit:
            self.request_count += 1
            
            # æ¨¡æ‹ŸçœŸå®APIè°ƒç”¨
            network_latency = 0.05  # 50mså›ºå®šç½‘ç»œå»¶è¿Ÿ
            model_processing = 0.8   # 800msæ¨¡å‹å¤„ç†æ—¶é—´
            
            # æ¨¡æ‹ŸAPIé™æµå»¶è¿Ÿ
            rate_limit_delay = 0.1   # æ¯ä¸ªè¯·æ±‚é¢å¤–100msé™æµå»¶è¿Ÿ
            
            total_time = network_latency + model_processing + rate_limit_delay
            await asyncio.sleep(total_time)
            
            content = request.get('content', 'Hello')
            return {
                'content': f"Response to: {content}",
                'processing_time': total_time,
                'model': request.get('model', 'gpt-3.5-turbo'),
                'request_id': self.request_count
            }
    
    async def process_batch(self, requests):
        """å¤„ç†æ‰¹é‡è¯·æ±‚ - æ‰¹å¤„ç†ä¼˜åŒ–"""
        batch_size = len(requests)
        
        # æ‰¹å¤„ç†çš„å…³é”®ä¼˜åŠ¿ï¼š
        # 1. åªéœ€è¦ä¸€æ¬¡ç½‘ç»œå¾€è¿”
        # 2. æ¨¡å‹å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚
        # 3. é¿å…äº†å•ä¸ªè¯·æ±‚çš„é™æµå»¶è¿Ÿ
        
        network_latency = 0.05  # 50msç½‘ç»œå»¶è¿Ÿï¼ˆåªæœ‰ä¸€æ¬¡ï¼‰
        
        # æ‰¹å¤„ç†æ¨¡å‹æ¨ç†æ—¶é—´ï¼šå¹¶è¡Œå¤„ç†ï¼Œæ—¶é—´ä¸çº¿æ€§å¢é•¿
        base_processing = 0.8
        # æ‰¹å¤„ç†æ•ˆç‡ï¼šæ‰¹æ¬¡è¶Šå¤§ï¼Œå•ä¸ªè¯·æ±‚çš„å¹³å‡æ—¶é—´è¶ŠçŸ­
        if batch_size <= 4:
            batch_efficiency = 1.0
        elif batch_size <= 8:
            batch_efficiency = 0.7  # 8ä¸ªè¯·æ±‚åªéœ€è¦70%çš„æ—¶é—´
        else:
            batch_efficiency = 0.5  # æ›´å¤§æ‰¹æ¬¡æ•ˆç‡æ›´é«˜
        
        actual_processing_time = base_processing * batch_efficiency
        
        # æ— é™æµå»¶è¿Ÿï¼ˆæ‰¹å¤„ç†çš„é‡è¦ä¼˜åŠ¿ï¼‰
        total_time = network_latency + actual_processing_time
        
        await asyncio.sleep(total_time)
        
        # å•ä¸ªè¯·æ±‚çš„å¹³å‡å¤„ç†æ—¶é—´
        avg_processing_time = total_time
        
        results = []
        for i, req in enumerate(requests):
            content = req.get('content', 'Hello')
            result = {
                'content': f"Batch response to: {content}",
                'processing_time': avg_processing_time,
                'model': req.get('model', 'gpt-3.5-turbo'),
                'batch_id': f"batch_{batch_size}_{i}"
            }
            results.append(result)
        
        return results


class HighPerformanceBatchProcessor:
    """é«˜æ€§èƒ½æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, batch_size=8, max_wait_time=0.02):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.client = OptimizedLLMClient()
        self.queue = asyncio.Queue()
        self.running = False
        self.processing_task = None
        self.stats = {
            'batches_processed': 0,
            'total_requests': 0,
            'avg_batch_size': 0,
            'batch_sizes': []
        }
        
    async def start(self):
        if not self.running:
            self.running = True
            self.processing_task = asyncio.create_task(self._processing_loop())
    
    async def stop(self):
        self.running = False
        if self.processing_task:
            self.processing_task.cancel()
            try:
                await self.processing_task
            except asyncio.CancelledError:
                pass
    
    async def process_request(self, request):
        future = asyncio.Future()
        await self.queue.put({'request': request, 'future': future})
        return await future
    
    async def _processing_loop(self):
        while self.running:
            try:
                batch = []
                futures = []
                batch_start = time.time()
                
                # ç§¯ææ”¶é›†è¯·æ±‚å½¢æˆæ‰¹æ¬¡
                while len(batch) < self.batch_size and self.running:
                    elapsed = time.time() - batch_start
                    remaining_wait = self.max_wait_time - elapsed
                    
                    if remaining_wait <= 0 and batch:
                        break
                    
                    try:
                        timeout = min(remaining_wait, 0.002) if batch else 0.1
                        item = await asyncio.wait_for(self.queue.get(), timeout=timeout)
                        batch.append(item['request'])
                        futures.append(item['future'])
                    except asyncio.TimeoutError:
                        if batch:  # æœ‰è¯·æ±‚å°±å¤„ç†
                            break
                        continue
                
                if batch:
                    try:
                        results = await self.client.process_batch(batch)
                        for future, result in zip(futures, results):
                            if not future.done():
                                future.set_result(result)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self.stats['batches_processed'] += 1
                        self.stats['total_requests'] += len(batch)
                        self.stats['batch_sizes'].append(len(batch))
                        self.stats['avg_batch_size'] = (
                            self.stats['total_requests'] / self.stats['batches_processed']
                        )
                        
                    except Exception as e:
                        for future in futures:
                            if not future.done():
                                future.set_exception(e)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Processing error: {e}")


async def run_optimized_performance_test():
    """è¿è¡Œä¼˜åŒ–çš„æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ æ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯• - æœ€ç»ˆç‰ˆ")
    print("=" * 70)
    
    # æµ‹è¯•å‚æ•°
    total_requests = 120
    concurrent_users = 20
    request_interval = 0.05  # 50msé—´éš”å‘é€è¯·æ±‚
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"  å¹¶å‘ç”¨æˆ·: {concurrent_users}")
    print(f"  è¯·æ±‚é—´éš”: {request_interval}s")
    print(f"  æ‰¹æ¬¡å¤§å°: 8")
    print(f"  APIå¹¶å‘é™åˆ¶: 3 (æ¨¡æ‹ŸçœŸå®é™åˆ¶)")
    print(f"  åœºæ™¯: é«˜å¹¶å‘çªå‘æµé‡")
    
    # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
    test_requests = []
    for i in range(total_requests):
        request = {
            'content': f"Query {i}: Explain machine learning in simple terms",
            'model': 'gpt-3.5-turbo'
        }
        test_requests.append(request)
    
    # æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼")
    batch_processor = HighPerformanceBatchProcessor(batch_size=8, max_wait_time=0.02)
    await batch_processor.start()
    
    async def send_batch_request(req, delay):
        await asyncio.sleep(delay)
        return await batch_processor.process_request(req)
    
    start_time = time.time()
    batch_tasks = []
    
    # æ¨¡æ‹Ÿé«˜å¹¶å‘åœºæ™¯ï¼šå¿«é€Ÿå‘é€è¯·æ±‚
    for i, req in enumerate(test_requests):
        delay = (i // concurrent_users) * request_interval
        task = asyncio.create_task(send_batch_request(req, delay))
        batch_tasks.append(task)
    
    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
    batch_total_time = time.time() - start_time
    
    batch_stats = batch_processor.stats
    await batch_processor.stop()
    
    # æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼")
    client = OptimizedLLMClient()
    
    async def send_direct_request(req, delay):
        await asyncio.sleep(delay)
        return await client.process_single(req)
    
    start_time = time.time()
    direct_tasks = []
    
    for i, req in enumerate(test_requests):
        delay = (i // concurrent_users) * request_interval
        task = asyncio.create_task(send_direct_request(req, delay))
        direct_tasks.append(task)
    
    direct_results = await asyncio.gather(*direct_tasks, return_exceptions=True)
    direct_total_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    def analyze_results(results, total_time, mode_name):
        successful = [r for r in results if not isinstance(r, Exception)]
        failed = len(results) - len(successful)
        
        if successful:
            response_times = [r.get('processing_time', 0) for r in successful]
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # è®¡ç®—P95
            if len(response_times) > 1:
                sorted_times = sorted(response_times)
                p95_index = int(0.95 * len(sorted_times))
                p95_response_time = sorted_times[min(p95_index, len(sorted_times) - 1)]
            else:
                p95_response_time = avg_response_time
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = 0
        
        throughput = len(successful) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“ˆ {mode_name} ç»“æœ:")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful)}/{len(results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {failed}")
        print(f"  æ€»è€—æ—¶: {total_time:.3f}s")
        print(f"  ååé‡: {throughput:.2f} req/s")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_response_time:.3f}s")
        print(f"  æœ€å°å»¶è¿Ÿ: {min_response_time:.3f}s")
        print(f"  æœ€å¤§å»¶è¿Ÿ: {max_response_time:.3f}s")
        print(f"  P95å»¶è¿Ÿ: {p95_response_time:.3f}s")
        
        return {
            'successful': len(successful),
            'failed': failed,
            'total_time': total_time,
            'throughput': throughput,
            'avg_latency': avg_response_time,
            'min_latency': min_response_time,
            'max_latency': max_response_time,
            'p95_latency': p95_response_time
        }
    
    batch_result = analyze_results(batch_results, batch_total_time, "æ‰¹å¤„ç†æ¨¡å¼")
    direct_result = analyze_results(direct_results, direct_total_time, "ç›´æ¥å¤„ç†æ¨¡å¼")
    
    # æ‰¹å¤„ç†è¯¦ç»†ç»Ÿè®¡
    print(f"\nğŸ“Š æ‰¹å¤„ç†è¯¦ç»†ç»Ÿè®¡:")
    print(f"  å¤„ç†æ‰¹æ¬¡æ•°: {batch_stats['batches_processed']}")
    print(f"  å¹³å‡æ‰¹æ¬¡å¤§å°: {batch_stats['avg_batch_size']:.1f}")
    print(f"  æ‰¹æ¬¡å¤§å°åˆ†å¸ƒ: {dict(sorted([(size, batch_stats['batch_sizes'].count(size)) for size in set(batch_stats['batch_sizes'])]))}")
    print(f"  æ‰¹å¤„ç†æ•ˆç‡: {batch_stats['avg_batch_size']:.1f}x è¯·æ±‚åˆå¹¶")
    
    # æ€§èƒ½å¯¹æ¯”åˆ†æ
    print(f"\nğŸ¯ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
    print("=" * 70)
    
    if direct_result['throughput'] > 0:
        throughput_improvement = (batch_result['throughput'] - direct_result['throughput']) / direct_result['throughput']
        print(f"ğŸš€ ååé‡æå‡: {throughput_improvement:.1%}")
        
        if throughput_improvement >= 0.30:
            print("âœ… æˆåŠŸè¾¾åˆ°30%+ååé‡æå‡ç›®æ ‡!")
        elif throughput_improvement >= 0.20:
            print("ğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œè¾¾åˆ°20%+ååé‡æå‡")
        else:
            print("âŒ æœªè¾¾åˆ°30%ååé‡æå‡ç›®æ ‡")
    
    if direct_result['avg_latency'] > 0:
        latency_improvement = (direct_result['avg_latency'] - batch_result['avg_latency']) / direct_result['avg_latency']
        print(f"âš¡ å»¶è¿Ÿæ”¹å–„: {latency_improvement:.1%}")
        
        p95_latency_improvement = (direct_result['p95_latency'] - batch_result['p95_latency']) / direct_result['p95_latency']
        print(f"ğŸ“Š P95å»¶è¿Ÿæ”¹å–„: {p95_latency_improvement:.1%}")
    
    # èµ„æºæ•ˆç‡åˆ†æ
    print(f"\nğŸ“Š èµ„æºæ•ˆç‡åˆ†æ:")
    efficiency_ratio = batch_result['throughput'] / direct_result['throughput'] if direct_result['throughput'] > 0 else 1
    print(f"æ•´ä½“æ•ˆç‡æå‡: {efficiency_ratio:.2f}x")
    
    # æˆæœ¬æ•ˆç›Šåˆ†æ
    print(f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:")
    # å‡è®¾æ¯ä¸ªAPIè°ƒç”¨æœ‰å›ºå®šæˆæœ¬
    direct_api_calls = direct_result['successful']  # æ¯ä¸ªè¯·æ±‚ä¸€æ¬¡APIè°ƒç”¨
    batch_api_calls = batch_stats['batches_processed']  # æ‰¹å¤„ç†å‡å°‘APIè°ƒç”¨æ¬¡æ•°
    
    cost_reduction = (direct_api_calls - batch_api_calls) / direct_api_calls if direct_api_calls > 0 else 0
    print(f"APIè°ƒç”¨æ¬¡æ•°: ç›´æ¥æ¨¡å¼ {direct_api_calls} vs æ‰¹å¤„ç†æ¨¡å¼ {batch_api_calls}")
    print(f"æˆæœ¬é™ä½: {cost_reduction:.1%}")
    
    # å¹¶å‘å¤„ç†èƒ½åŠ›
    print(f"\nâš¡ å¹¶å‘å¤„ç†èƒ½åŠ›å¯¹æ¯”:")
    print(f"æ‰¹å¤„ç†æ¨¡å¼: {batch_result['throughput']:.2f} req/s")
    print(f"ç›´æ¥å¤„ç†æ¨¡å¼: {direct_result['throughput']:.2f} req/s")
    
    if batch_result['throughput'] > direct_result['throughput']:
        improvement_factor = batch_result['throughput'] / direct_result['throughput']
        print(f"ğŸ‰ æ‰¹å¤„ç†æ¨¡å¼åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ€§èƒ½æå‡ {improvement_factor:.1f}x!")
    
    # æ€»ç»“
    print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
    print("=" * 70)
    
    advantages = []
    if throughput_improvement > 0:
        advantages.append(f"ååé‡æå‡ {throughput_improvement:.1%}")
    if latency_improvement > 0:
        advantages.append(f"å»¶è¿Ÿé™ä½ {latency_improvement:.1%}")
    if cost_reduction > 0:
        advantages.append(f"æˆæœ¬é™ä½ {cost_reduction:.1%}")
    
    if advantages:
        print("âœ… æ‰¹å¤„ç†ç³»ç»Ÿä¼˜åŠ¿:")
        for advantage in advantages:
            print(f"   â€¢ {advantage}")
    
    if throughput_improvement >= 0.30:
        print("\nğŸ† æ‰¹é‡åŒ–ç³»ç»ŸæˆåŠŸå®ç°30-50%ååé‡æå‡ç›®æ ‡!")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    
    return {
        'batch': batch_result,
        'direct': direct_result,
        'batch_stats': batch_stats,
        'improvement': throughput_improvement if direct_result['throughput'] > 0 else 0,
        'cost_reduction': cost_reduction
    }


if __name__ == "__main__":
    asyncio.run(run_optimized_performance_test())

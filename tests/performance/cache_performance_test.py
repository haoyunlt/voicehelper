"""
ç¼“å­˜ç³»ç»Ÿæ€§èƒ½æµ‹è¯•

æµ‹è¯•æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿçš„å“åº”é€Ÿåº¦æå‡æ•ˆæœ
"""

import asyncio
import time
import statistics
import sys
import os
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from algo.services.cache_service import IntegratedCacheService, CacheConfig


class CachePerformanceTester:
    """ç¼“å­˜æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
    
    async def mock_llm_processing(self, content: str, model: str, parameters: dict) -> str:
        """æ¨¡æ‹ŸLLMå¤„ç†"""
        # æ¨¡æ‹Ÿä¸åŒç±»å‹æŸ¥è¯¢çš„å¤„ç†æ—¶é—´
        base_time = 0.8  # 800msåŸºç¡€å¤„ç†æ—¶é—´
        
        # æ ¹æ®å†…å®¹é•¿åº¦è°ƒæ•´å¤„ç†æ—¶é—´
        length_factor = len(content) / 100.0
        processing_time = base_time * (0.8 + 0.4 * length_factor)
        
        # æ·»åŠ éšæœºå˜åŒ–
        processing_time *= random.uniform(0.9, 1.1)
        
        await asyncio.sleep(processing_time)
        
        return f"AI response to: {content}"
    
    def generate_test_queries(self, scenario: str, count: int) -> list:
        """ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢"""
        if scenario == "high_similarity":
            # é«˜ç›¸ä¼¼åº¦åœºæ™¯
            base_queries = [
                "What is artificial intelligence?",
                "How does machine learning work?",
                "Explain deep learning concepts",
                "What are neural networks?",
                "Tell me about natural language processing"
            ]
            
            queries = []
            for i in range(count):
                base_query = base_queries[i % len(base_queries)]
                # åˆ›å»ºç›¸ä¼¼å˜ä½“
                variations = [
                    base_query,
                    base_query.replace("?", " in detail?"),
                    f"Can you {base_query.lower()}",
                    f"Please {base_query.lower()}",
                    base_query.replace("What", "What exactly")
                ]
                queries.append(variations[i % len(variations)])
            
            return queries
        
        elif scenario == "mixed_similarity":
            # æ··åˆç›¸ä¼¼åº¦åœºæ™¯
            queries = [
                "What is AI?", "How does ML work?", "Explain DL",
                "What is artificial intelligence?", "How does machine learning work?",
                "Tell me about Python programming", "What is Docker?",
                "Explain REST APIs", "How to use Git?", "What is microservices?",
                "What's AI?", "How ML works?", "Deep learning explanation",
                "AI definition", "Machine learning basics", "Neural network intro"
            ]
            return (queries * (count // len(queries) + 1))[:count]
        
        else:  # low_similarity
            # ä½ç›¸ä¼¼åº¦åœºæ™¯
            topics = [
                "artificial intelligence", "machine learning", "data science",
                "cloud computing", "blockchain technology", "quantum computing",
                "cybersecurity", "internet of things", "virtual reality",
                "augmented reality", "robotics", "automation", "big data",
                "edge computing", "5G networks", "digital transformation"
            ]
            
            templates = [
                "What is {}?", "How does {} work?", "Explain {} in detail",
                "Tell me about {}", "What are the benefits of {}?",
                "How to implement {}?", "What are {} applications?",
                "Compare {} with alternatives", "{} best practices",
                "Future of {}", "{} challenges and solutions"
            ]
            
            queries = []
            for i in range(count):
                topic = topics[i % len(topics)]
                template = templates[i % len(templates)]
                queries.append(template.format(topic))
            
            return queries
    
    async def run_baseline_test(self, queries: list) -> dict:
        """è¿è¡ŒåŸºçº¿æµ‹è¯• (æ— ç¼“å­˜)"""
        print("ğŸ”„ Running baseline test (no cache)...")
        
        response_times = []
        start_time = time.time()
        
        for query in queries:
            query_start = time.time()
            await self.mock_llm_processing(query, "gpt-3.5-turbo", {"temperature": 0.7})
            response_time = time.time() - query_start
            response_times.append(response_time)
        
        total_time = time.time() - start_time
        
        return {
            'total_time': total_time,
            'avg_response_time': statistics.mean(response_times),
            'min_response_time': min(response_times),
            'max_response_time': max(response_times),
            'p95_response_time': self._calculate_percentile(response_times, 0.95),
            'throughput': len(queries) / total_time
        }
    
    async def run_cache_test(self, queries: list, config: CacheConfig) -> dict:
        """è¿è¡Œç¼“å­˜æµ‹è¯•"""
        print("ğŸš€ Running cache test...")
        
        # åˆ›å»ºç¼“å­˜æœåŠ¡
        cache_service = IntegratedCacheService(
            processing_func=self.mock_llm_processing,
            config=config
        )
        
        await cache_service.start()
        
        try:
            response_times = []
            cache_hits = 0
            start_time = time.time()
            
            for i, query in enumerate(queries):
                query_start = time.time()
                
                # è®°å½•ç¼“å­˜å‘½ä¸­å‰çš„ç»Ÿè®¡
                stats_before = cache_service.get_comprehensive_stats()
                hits_before = stats_before['service_stats']['cache_hits']
                
                # æ‰§è¡ŒæŸ¥è¯¢
                await cache_service.get_response(
                    content=query,
                    model="gpt-3.5-turbo",
                    parameters={"temperature": 0.7}
                )
                
                response_time = time.time() - query_start
                response_times.append(response_time)
                
                # æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜
                stats_after = cache_service.get_comprehensive_stats()
                hits_after = stats_after['service_stats']['cache_hits']
                
                if hits_after > hits_before:
                    cache_hits += 1
                
                # æ¯10ä¸ªæŸ¥è¯¢è¾“å‡ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    current_hit_rate = cache_hits / (i + 1)
                    print(f"  Progress: {i+1}/{len(queries)}, Hit rate: {current_hit_rate:.2%}")
            
            total_time = time.time() - start_time
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = cache_service.get_comprehensive_stats()
            
            return {
                'total_time': total_time,
                'avg_response_time': statistics.mean(response_times),
                'min_response_time': min(response_times),
                'max_response_time': max(response_times),
                'p95_response_time': self._calculate_percentile(response_times, 0.95),
                'throughput': len(queries) / total_time,
                'cache_hit_rate': final_stats['service_stats']['hit_rate'],
                'cache_stats': final_stats
            }
            
        finally:
            await cache_service.stop()
    
    def _calculate_percentile(self, data: list, percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0
        
        sorted_data = sorted(data)
        index = int(percentile * len(sorted_data))
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def analyze_performance_improvement(self, baseline: dict, cached: dict) -> dict:
        """åˆ†ææ€§èƒ½æå‡"""
        improvements = {}
        
        # å“åº”æ—¶é—´æ”¹å–„
        improvements['avg_response_time_improvement'] = (
            (baseline['avg_response_time'] - cached['avg_response_time']) / 
            baseline['avg_response_time']
        )
        
        improvements['p95_response_time_improvement'] = (
            (baseline['p95_response_time'] - cached['p95_response_time']) / 
            baseline['p95_response_time']
        )
        
        # ååé‡æå‡
        improvements['throughput_improvement'] = (
            (cached['throughput'] - baseline['throughput']) / 
            baseline['throughput']
        )
        
        # æ€»ä½“æ€§èƒ½æå‡
        improvements['overall_performance_improvement'] = (
            (baseline['total_time'] - cached['total_time']) / 
            baseline['total_time']
        )
        
        return improvements
    
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ Comprehensive Cache Performance Test")
        print("=" * 60)
        
        # æµ‹è¯•åœºæ™¯
        scenarios = [
            ("high_similarity", "é«˜ç›¸ä¼¼åº¦åœºæ™¯ (80%+ç›¸ä¼¼æŸ¥è¯¢)"),
            ("mixed_similarity", "æ··åˆç›¸ä¼¼åº¦åœºæ™¯ (50%ç›¸ä¼¼æŸ¥è¯¢)"),
            ("low_similarity", "ä½ç›¸ä¼¼åº¦åœºæ™¯ (20%ç›¸ä¼¼æŸ¥è¯¢)")
        ]
        
        test_query_count = 60
        
        for scenario_id, scenario_name in scenarios:
            print(f"\nğŸ“Š {scenario_name}")
            print("-" * 40)
            
            # ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢
            queries = self.generate_test_queries(scenario_id, test_query_count)
            
            # ç¼“å­˜é…ç½®
            config = CacheConfig(
                semantic_cache_size=200,
                semantic_similarity_threshold=0.85,
                hotspot_initial_size=20,
                hotspot_max_size=50,
                enable_prewarming=True,
                prewarm_workers=2
            )
            
            # è¿è¡ŒåŸºçº¿æµ‹è¯•
            baseline_results = await self.run_baseline_test(queries)
            
            # è¿è¡Œç¼“å­˜æµ‹è¯•
            cache_results = await self.run_cache_test(queries, config)
            
            # åˆ†ææ€§èƒ½æå‡
            improvements = self.analyze_performance_improvement(baseline_results, cache_results)
            
            # è¾“å‡ºç»“æœ
            self._print_scenario_results(
                scenario_name, 
                baseline_results, 
                cache_results, 
                improvements
            )
            
            # ä¿å­˜ç»“æœ
            self.test_results[scenario_id] = {
                'baseline': baseline_results,
                'cached': cache_results,
                'improvements': improvements
            }
        
        # è¾“å‡ºæ€»ç»“
        self._print_summary()
    
    def _print_scenario_results(self, scenario_name: str, baseline: dict, cached: dict, improvements: dict):
        """è¾“å‡ºåœºæ™¯æµ‹è¯•ç»“æœ"""
        print(f"\nğŸ“ˆ {scenario_name} ç»“æœ:")
        
        print(f"\n  åŸºçº¿æµ‹è¯• (æ— ç¼“å­˜):")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {baseline['avg_response_time']:.3f}s")
        print(f"    P95å“åº”æ—¶é—´: {baseline['p95_response_time']:.3f}s")
        print(f"    ååé‡: {baseline['throughput']:.2f} req/s")
        print(f"    æ€»è€—æ—¶: {baseline['total_time']:.3f}s")
        
        print(f"\n  ç¼“å­˜æµ‹è¯•:")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {cached['avg_response_time']:.3f}s")
        print(f"    P95å“åº”æ—¶é—´: {cached['p95_response_time']:.3f}s")
        print(f"    ååé‡: {cached['throughput']:.2f} req/s")
        print(f"    æ€»è€—æ—¶: {cached['total_time']:.3f}s")
        print(f"    ç¼“å­˜å‘½ä¸­ç‡: {cached['cache_hit_rate']:.2%}")
        
        print(f"\n  ğŸ¯ æ€§èƒ½æå‡:")
        print(f"    å¹³å‡å“åº”æ—¶é—´æ”¹å–„: {improvements['avg_response_time_improvement']:.1%}")
        print(f"    P95å“åº”æ—¶é—´æ”¹å–„: {improvements['p95_response_time_improvement']:.1%}")
        print(f"    ååé‡æå‡: {improvements['throughput_improvement']:.1%}")
        print(f"    æ€»ä½“æ€§èƒ½æå‡: {improvements['overall_performance_improvement']:.1%}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if improvements['avg_response_time_improvement'] >= 0.40:
            print(f"    âœ… è¾¾åˆ°40%+å“åº”é€Ÿåº¦æå‡ç›®æ ‡!")
        elif improvements['avg_response_time_improvement'] >= 0.30:
            print(f"    ğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œè¾¾åˆ°30%+å“åº”é€Ÿåº¦æå‡")
        else:
            print(f"    âŒ æœªè¾¾åˆ°40%å“åº”é€Ÿåº¦æå‡ç›®æ ‡")
    
    def _print_summary(self):
        """è¾“å‡ºæµ‹è¯•æ€»ç»“"""
        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        
        total_improvements = []
        scenarios_meeting_target = 0
        
        for scenario_id, results in self.test_results.items():
            improvement = results['improvements']['avg_response_time_improvement']
            total_improvements.append(improvement)
            
            if improvement >= 0.40:
                scenarios_meeting_target += 1
        
        avg_improvement = statistics.mean(total_improvements)
        
        print(f"å¹³å‡å“åº”é€Ÿåº¦æå‡: {avg_improvement:.1%}")
        print(f"è¾¾åˆ°40%+ç›®æ ‡çš„åœºæ™¯: {scenarios_meeting_target}/{len(self.test_results)}")
        
        if avg_improvement >= 0.40:
            print(f"\nğŸ† æ™ºèƒ½ç¼“å­˜ç³»ç»ŸæˆåŠŸå®ç°40-60%å“åº”é€Ÿåº¦æå‡ç›®æ ‡!")
        elif avg_improvement >= 0.30:
            print(f"\nğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œå¹³å‡æå‡30%+å“åº”é€Ÿåº¦")
        else:
            print(f"\nâŒ æœªè¾¾åˆ°40%å“åº”é€Ÿåº¦æå‡ç›®æ ‡")
        
        # æœ€ä½³åœºæ™¯åˆ†æ
        best_scenario = max(
            self.test_results.items(), 
            key=lambda x: x[1]['improvements']['avg_response_time_improvement']
        )
        
        print(f"\nğŸ“Š æœ€ä½³åœºæ™¯: {best_scenario[0]}")
        print(f"   å“åº”é€Ÿåº¦æå‡: {best_scenario[1]['improvements']['avg_response_time_improvement']:.1%}")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {best_scenario[1]['cached']['cache_hit_rate']:.2%}")
        
        # ç¼“å­˜æ•ˆæœåˆ†æ
        print(f"\nğŸ’¡ ç¼“å­˜æ•ˆæœåˆ†æ:")
        print(f"   é«˜ç›¸ä¼¼åº¦åœºæ™¯æ•ˆæœæœ€ä½³ï¼Œé€‚åˆFAQå’Œå¸¸è§é—®é¢˜")
        print(f"   æ··åˆåœºæ™¯å¹³è¡¡æ€§èƒ½å’Œè¦†ç›–é¢")
        print(f"   ä½ç›¸ä¼¼åº¦åœºæ™¯ä¸»è¦ä¾é çƒ­ç‚¹æ£€æµ‹å’Œé¢„çƒ­")


async def main():
    """ä¸»å‡½æ•°"""
    tester = CachePerformanceTester()
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    asyncio.run(main())

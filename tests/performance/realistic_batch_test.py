"""
çœŸå®åœºæ™¯çš„æ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•

æ¨¡æ‹Ÿé«˜å¹¶å‘ã€çªå‘æµé‡çš„çœŸå®LLMæœåŠ¡åœºæ™¯
"""

import asyncio
import time
import statistics
import sys
import os
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))


class RealisticLLMClient:
    """çœŸå®åœºæ™¯LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.concurrent_limit = 5  # æ¨¡æ‹ŸAPIå¹¶å‘é™åˆ¶
        self.semaphore = asyncio.Semaphore(self.concurrent_limit)
    
    async def process_single(self, request):
        """å¤„ç†å•ä¸ªè¯·æ±‚ - å—å¹¶å‘é™åˆ¶"""
        async with self.semaphore:
            # æ¨¡æ‹ŸçœŸå®LLM APIè°ƒç”¨
            network_latency = random.uniform(0.03, 0.08)  # 30-80msç½‘ç»œå»¶è¿Ÿ
            model_inference = random.uniform(0.6, 1.2)    # 600-1200msæ¨ç†æ—¶é—´
            
            await asyncio.sleep(network_latency)
            await asyncio.sleep(model_inference)
            
            content = request.get('content', 'Hello')
            return {
                'content': f"Response to: {content}",
                'processing_time': network_latency + model_inference,
                'model': request.get('model', 'gpt-3.5-turbo')
            }
    
    async def process_batch(self, requests):
        """å¤„ç†æ‰¹é‡è¯·æ±‚ - æ‰¹å¤„ç†ä¼˜åŒ–"""
        batch_size = len(requests)
        
        # æ‰¹å¤„ç†åªéœ€è¦ä¸€æ¬¡ç½‘ç»œå¾€è¿”
        network_latency = random.uniform(0.03, 0.08)
        
        # æ‰¹å¤„ç†æ¨ç†æ—¶é—´ä¼˜åŒ–
        base_inference = random.uniform(0.6, 1.2)
        # æ‰¹å¤„ç†æ•ˆç‡ï¼šæ¯å¢åŠ ä¸€ä¸ªè¯·æ±‚ï¼Œæ€»æ—¶é—´åªå¢åŠ 20%è€Œä¸æ˜¯100%
        batch_factor = 1.0 + (batch_size - 1) * 0.2
        total_inference_time = base_inference * batch_factor
        
        # å•ä¸ªè¯·æ±‚çš„å¹³å‡å¤„ç†æ—¶é—´
        avg_processing_time = (network_latency + total_inference_time) / batch_size
        
        await asyncio.sleep(network_latency + total_inference_time)
        
        results = []
        for req in requests:
            content = req.get('content', 'Hello')
            result = {
                'content': f"Batch response to: {content}",
                'processing_time': avg_processing_time,
                'model': req.get('model', 'gpt-3.5-turbo')
            }
            results.append(result)
        
        return results


class RealisticBatchProcessor:
    """çœŸå®åœºæ™¯æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, batch_size=8, max_wait_time=0.05):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.client = RealisticLLMClient()
        self.queue = asyncio.Queue()
        self.running = False
        self.processing_task = None
        self.stats = {
            'batches_processed': 0,
            'total_requests': 0,
            'avg_batch_size': 0
        }
        
    async def start(self):
        """å¯åŠ¨æ‰¹å¤„ç†å™¨"""
        if not self.running:
            self.running = True
            self.processing_task = asyncio.create_task(self._processing_loop())
    
    async def stop(self):
        """åœæ­¢æ‰¹å¤„ç†å™¨"""
        self.running = False
        if self.processing_task:
            self.processing_task.cancel()
            try:
                await self.processing_task
            except asyncio.CancelledError:
                pass
    
    async def process_request(self, request):
        """å¤„ç†è¯·æ±‚"""
        future = asyncio.Future()
        await self.queue.put({'request': request, 'future': future})
        return await future
    
    async def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†æ‰¹æ¬¡
                batch = []
                futures = []
                batch_start = time.time()
                
                # æ”¶é›†è¯·æ±‚ç›´åˆ°è¾¾åˆ°æ‰¹æ¬¡å¤§å°æˆ–è¶…æ—¶
                while len(batch) < self.batch_size and self.running:
                    elapsed = time.time() - batch_start
                    remaining_wait = self.max_wait_time - elapsed
                    
                    if remaining_wait <= 0:
                        break
                    
                    try:
                        item = await asyncio.wait_for(
                            self.queue.get(), 
                            timeout=min(remaining_wait, 0.005)  # æ›´çŸ­çš„è½®è¯¢é—´éš”
                        )
                        batch.append(item['request'])
                        futures.append(item['future'])
                    except asyncio.TimeoutError:
                        break
                
                # å¤„ç†æ‰¹æ¬¡
                if batch:
                    try:
                        results = await self.client.process_batch(batch)
                        for future, result in zip(futures, results):
                            if not future.done():
                                future.set_result(result)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        self.stats['batches_processed'] += 1
                        self.stats['total_requests'] += len(batch)
                        self.stats['avg_batch_size'] = (
                            self.stats['total_requests'] / self.stats['batches_processed']
                        )
                        
                    except Exception as e:
                        for future in futures:
                            if not future.done():
                                future.set_exception(e)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Processing error: {e}")


async def simulate_burst_traffic():
    """æ¨¡æ‹Ÿçªå‘æµé‡åœºæ™¯"""
    print("ğŸš€ çœŸå®åœºæ™¯æ‰¹é‡åŒ–ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å‚æ•°
    total_requests = 100
    burst_duration = 10.0  # 10ç§’å†…å‘é€æ‰€æœ‰è¯·æ±‚
    
    # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
    test_requests = []
    for i in range(total_requests):
        request = {
            'content': f"User query {i}: What is artificial intelligence?",
            'model': 'gpt-3.5-turbo'
        }
        test_requests.append(request)
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"  çªå‘æ—¶é•¿: {burst_duration}s")
    print(f"  å¹³å‡QPS: {total_requests/burst_duration:.1f}")
    print(f"  æ‰¹æ¬¡å¤§å°: 8")
    print(f"  æœ€å¤§ç­‰å¾…: 50ms")
    print(f"  APIå¹¶å‘é™åˆ¶: 5")
    
    # æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•1: æ‰¹å¤„ç†æ¨¡å¼")
    batch_processor = RealisticBatchProcessor(batch_size=8, max_wait_time=0.05)
    await batch_processor.start()
    
    async def send_batch_request(req, delay):
        await asyncio.sleep(delay)
        return await batch_processor.process_request(req)
    
    # æ¨¡æ‹Ÿçªå‘æµé‡ï¼šåœ¨10ç§’å†…éšæœºå‘é€æ‰€æœ‰è¯·æ±‚
    start_time = time.time()
    batch_tasks = []
    for i, req in enumerate(test_requests):
        # éšæœºåˆ†å¸ƒåœ¨burst_durationæ—¶é—´å†…
        delay = random.uniform(0, burst_duration)
        task = asyncio.create_task(send_batch_request(req, delay))
        batch_tasks.append(task)
    
    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
    batch_total_time = time.time() - start_time
    
    batch_stats = batch_processor.stats
    await batch_processor.stop()
    
    # æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼
    print(f"\nğŸ”„ æµ‹è¯•2: ç›´æ¥å¤„ç†æ¨¡å¼")
    client = RealisticLLMClient()
    
    async def send_direct_request(req, delay):
        await asyncio.sleep(delay)
        return await client.process_single(req)
    
    start_time = time.time()
    direct_tasks = []
    for i, req in enumerate(test_requests):
        delay = random.uniform(0, burst_duration)
        task = asyncio.create_task(send_direct_request(req, delay))
        direct_tasks.append(task)
    
    direct_results = await asyncio.gather(*direct_tasks, return_exceptions=True)
    direct_total_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    def analyze_results(results, total_time, mode_name):
        successful = [r for r in results if not isinstance(r, Exception)]
        failed = len(results) - len(successful)
        
        if successful:
            response_times = [r.get('processing_time', 0) for r in successful]
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            
            # è®¡ç®—P95 (å…¼å®¹Python 3.7)
            if len(response_times) > 1:
                sorted_times = sorted(response_times)
                p95_index = int(0.95 * len(sorted_times))
                p95_response_time = sorted_times[min(p95_index, len(sorted_times) - 1)]
            else:
                p95_response_time = avg_response_time
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = 0
        
        throughput = len(successful) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“ˆ {mode_name} ç»“æœ:")
        print(f"  æˆåŠŸè¯·æ±‚: {len(successful)}/{len(results)}")
        print(f"  å¤±è´¥è¯·æ±‚: {failed}")
        print(f"  æ€»è€—æ—¶: {total_time:.3f}s")
        print(f"  ååé‡: {throughput:.2f} req/s")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_response_time:.3f}s")
        print(f"  æœ€å°å»¶è¿Ÿ: {min_response_time:.3f}s")
        print(f"  æœ€å¤§å»¶è¿Ÿ: {max_response_time:.3f}s")
        print(f"  P95å»¶è¿Ÿ: {p95_response_time:.3f}s")
        
        return {
            'successful': len(successful),
            'failed': failed,
            'total_time': total_time,
            'throughput': throughput,
            'avg_latency': avg_response_time,
            'min_latency': min_response_time,
            'max_latency': max_response_time,
            'p95_latency': p95_response_time
        }
    
    batch_result = analyze_results(batch_results, batch_total_time, "æ‰¹å¤„ç†æ¨¡å¼")
    direct_result = analyze_results(direct_results, direct_total_time, "ç›´æ¥å¤„ç†æ¨¡å¼")
    
    # æ‰¹å¤„ç†ç»Ÿè®¡
    print(f"\nğŸ“Š æ‰¹å¤„ç†ç»Ÿè®¡:")
    print(f"  å¤„ç†æ‰¹æ¬¡æ•°: {batch_stats['batches_processed']}")
    print(f"  å¹³å‡æ‰¹æ¬¡å¤§å°: {batch_stats['avg_batch_size']:.1f}")
    print(f"  æ‰¹å¤„ç†æ•ˆç‡: {batch_stats['total_requests']}/{batch_stats['batches_processed']} = {batch_stats['avg_batch_size']:.1f}x")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ¯ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
    print("=" * 60)
    
    if direct_result['throughput'] > 0:
        throughput_improvement = (batch_result['throughput'] - direct_result['throughput']) / direct_result['throughput']
        print(f"ååé‡æå‡: {throughput_improvement:.1%}")
        
        if throughput_improvement >= 0.30:
            print("âœ… è¾¾åˆ°30%+ååé‡æå‡ç›®æ ‡!")
        else:
            print("âŒ æœªè¾¾åˆ°30%ååé‡æå‡ç›®æ ‡")
    
    if direct_result['avg_latency'] > 0:
        latency_change = (batch_result['avg_latency'] - direct_result['avg_latency']) / direct_result['avg_latency']
        print(f"å¹³å‡å»¶è¿Ÿå˜åŒ–: {latency_change:.1%}")
        
        p95_latency_change = (batch_result['p95_latency'] - direct_result['p95_latency']) / direct_result['p95_latency']
        print(f"P95å»¶è¿Ÿå˜åŒ–: {p95_latency_change:.1%}")
    
    # èµ„æºæ•ˆç‡åˆ†æ
    print(f"\nğŸ“Š èµ„æºæ•ˆç‡åˆ†æ:")
    efficiency_ratio = batch_result['throughput'] / direct_result['throughput'] if direct_result['throughput'] > 0 else 1
    print(f"æ•´ä½“æ•ˆç‡æå‡: {efficiency_ratio:.2f}x")
    
    # å¹¶å‘å¤„ç†èƒ½åŠ›åˆ†æ
    print(f"\nâš¡ å¹¶å‘å¤„ç†èƒ½åŠ›:")
    print(f"æ‰¹å¤„ç†æ¨¡å¼å³°å€¼QPS: {batch_result['throughput']:.2f}")
    print(f"ç›´æ¥å¤„ç†æ¨¡å¼å³°å€¼QPS: {direct_result['throughput']:.2f}")
    
    if batch_result['throughput'] > direct_result['throughput']:
        improvement_factor = batch_result['throughput'] / direct_result['throughput']
        print(f"ğŸš€ æ‰¹å¤„ç†æ¨¡å¼åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹æ€§èƒ½æå‡ {improvement_factor:.1f}x!")
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    
    return {
        'batch': batch_result,
        'direct': direct_result,
        'batch_stats': batch_stats,
        'improvement': throughput_improvement if direct_result['throughput'] > 0 else 0
    }


async def simulate_sustained_load():
    """æ¨¡æ‹ŸæŒç»­è´Ÿè½½åœºæ™¯"""
    print("\n" + "="*60)
    print("ğŸ”¥ æŒç»­è´Ÿè½½æµ‹è¯• (æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒ)")
    print("="*60)
    
    total_requests = 200
    test_duration = 30.0  # 30ç§’æŒç»­æµ‹è¯•
    
    print(f"ğŸ“Š æŒç»­è´Ÿè½½é…ç½®:")
    print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"  æµ‹è¯•æ—¶é•¿: {test_duration}s")
    print(f"  ç›®æ ‡QPS: {total_requests/test_duration:.1f}")
    
    # æ‰¹å¤„ç†æ¨¡å¼æµ‹è¯•
    print(f"\nğŸ”„ æ‰¹å¤„ç†æ¨¡å¼ - æŒç»­è´Ÿè½½")
    batch_processor = RealisticBatchProcessor(batch_size=6, max_wait_time=0.08)
    await batch_processor.start()
    
    async def sustained_batch_load():
        tasks = []
        interval = test_duration / total_requests
        
        for i in range(total_requests):
            request = {'content': f"Sustained query {i}", 'model': 'gpt-3.5-turbo'}
            delay = i * interval
            task = asyncio.create_task(
                asyncio.sleep(delay) and batch_processor.process_request(request)
            )
            tasks.append(task)
        
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    start_time = time.time()
    batch_results = await sustained_batch_load()
    batch_time = time.time() - start_time
    
    await batch_processor.stop()
    
    # ç›´æ¥å¤„ç†æ¨¡å¼æµ‹è¯•
    print(f"\nğŸ”„ ç›´æ¥å¤„ç†æ¨¡å¼ - æŒç»­è´Ÿè½½")
    client = RealisticLLMClient()
    
    async def sustained_direct_load():
        tasks = []
        interval = test_duration / total_requests
        
        for i in range(total_requests):
            request = {'content': f"Sustained query {i}", 'model': 'gpt-3.5-turbo'}
            delay = i * interval
            
            async def delayed_request(req, d):
                await asyncio.sleep(d)
                return await client.process_single(req)
            
            task = asyncio.create_task(delayed_request(request, delay))
            tasks.append(task)
        
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    start_time = time.time()
    direct_results = await sustained_direct_load()
    direct_time = time.time() - start_time
    
    # åˆ†ææŒç»­è´Ÿè½½ç»“æœ
    def analyze_sustained_results(results, total_time, mode_name):
        successful = [r for r in results if not isinstance(r, Exception)]
        
        if successful:
            response_times = [r.get('processing_time', 0) for r in successful]
            avg_latency = statistics.mean(response_times)
        else:
            avg_latency = 0
        
        actual_qps = len(successful) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“ˆ {mode_name} - æŒç»­è´Ÿè½½ç»“æœ:")
        print(f"  æˆåŠŸå¤„ç†: {len(successful)}/{len(results)}")
        print(f"  å®é™…QPS: {actual_qps:.2f}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.3f}s")
        
        return {
            'successful': len(successful),
            'qps': actual_qps,
            'avg_latency': avg_latency
        }
    
    batch_sustained = analyze_sustained_results(batch_results, batch_time, "æ‰¹å¤„ç†æ¨¡å¼")
    direct_sustained = analyze_sustained_results(direct_results, direct_time, "ç›´æ¥å¤„ç†æ¨¡å¼")
    
    # æŒç»­è´Ÿè½½å¯¹æ¯”
    print(f"\nğŸ¯ æŒç»­è´Ÿè½½æ€§èƒ½å¯¹æ¯”:")
    if direct_sustained['qps'] > 0:
        qps_improvement = (batch_sustained['qps'] - direct_sustained['qps']) / direct_sustained['qps']
        print(f"QPSæå‡: {qps_improvement:.1%}")
        
        if qps_improvement >= 0.30:
            print("âœ… æŒç»­è´Ÿè½½ä¸‹è¾¾åˆ°30%+æ€§èƒ½æå‡!")
        else:
            print("âŒ æŒç»­è´Ÿè½½ä¸‹æœªè¾¾åˆ°30%æ€§èƒ½æå‡")


if __name__ == "__main__":
    async def main():
        # çªå‘æµé‡æµ‹è¯•
        await simulate_burst_traffic()
        
        # æŒç»­è´Ÿè½½æµ‹è¯•
        await simulate_sustained_load()
    
    asyncio.run(main())

"""
VoiceHelper v1.20.0 æ€§èƒ½æµ‹è¯•å¥—ä»¶
æµ‹è¯•è¯­éŸ³ä¼˜åŒ–ã€æƒ…æ„Ÿè¯†åˆ«å’Œæ‰¹å¤„ç†æ€§èƒ½
"""

import asyncio
import time
import random
import math
from typing import List, Dict, Any
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from algo.core.enhanced_voice_optimizer import (
    EnhancedVoiceOptimizer, 
    optimize_voice_request,
    VoiceResponse
)
from algo.core.advanced_emotion_recognition import (
    AdvancedEmotionRecognition,
    analyze_emotion,
    EmotionAnalysisResult
)
from algo.core.adaptive_batch_scheduler import (
    AdaptiveBatchScheduler,
    submit_batch_request,
    RequestType,
    RequestPriority
)

class V120PerformanceTest:
    """v1.20.0 æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.voice_optimizer = EnhancedVoiceOptimizer()
        self.emotion_recognizer = AdvancedEmotionRecognition()
        self.batch_scheduler = AdaptiveBatchScheduler()
        
        # æ€§èƒ½åŸºçº¿
        self.baseline_voice_latency = 300  # ms
        self.baseline_emotion_accuracy = 0.85
        self.baseline_batch_throughput = 10  # requests/second
        
        # ç›®æ ‡æ€§èƒ½
        self.target_voice_latency = 150  # ms
        self.target_emotion_accuracy = 0.95
        self.target_batch_improvement = 2.0  # 200% improvement
    
    def generate_test_audio(self, duration_seconds: float) -> bytes:
        """ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®"""
        # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®ï¼š44.1kHz, 16-bit, mono
        sample_rate = 44100
        samples = int(duration_seconds * sample_rate)
        
        # ç”Ÿæˆç®€å•çš„æ­£å¼¦æ³¢
        frequency = 440  # A4 note
        audio_data = []
        
        for i in range(samples):
            t = i / sample_rate
            sample = math.sin(2 * math.pi * frequency * t)
            # è½¬æ¢ä¸º16-bitæ•´æ•°
            audio_int16 = int(sample * 32767)
            audio_data.extend([audio_int16 & 0xFF, (audio_int16 >> 8) & 0xFF])
        
        return bytes(audio_data)
    
    def create_test_request(self, request_type: RequestType = RequestType.TEXT_GENERATION) -> Dict[str, Any]:
        """åˆ›å»ºæµ‹è¯•è¯·æ±‚"""
        return {
            "type": request_type,
            "data": f"test_data_{int(time.time()*1000)}",
            "user_id": f"test_user_{random.randint(1, 100)}",
            "priority": random.choice(list(RequestPriority))
        }
    
    async def test_voice_latency_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–"""
        print("\n=== è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–æµ‹è¯• ===")
        
        test_cases = [
            {"audio_length": 1, "expected_latency": 100},   # 1ç§’éŸ³é¢‘ï¼ŒæœŸæœ›100mså»¶è¿Ÿ
            {"audio_length": 3, "expected_latency": 150},   # 3ç§’éŸ³é¢‘ï¼ŒæœŸæœ›150mså»¶è¿Ÿ
            {"audio_length": 5, "expected_latency": 200},   # 5ç§’éŸ³é¢‘ï¼ŒæœŸæœ›200mså»¶è¿Ÿ
            {"audio_length": 10, "expected_latency": 300},  # 10ç§’éŸ³é¢‘ï¼ŒæœŸæœ›300mså»¶è¿Ÿ
        ]
        
        results = []
        
        for i, case in enumerate(test_cases):
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {case['audio_length']}ç§’éŸ³é¢‘")
            
            # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
            test_audio = self.generate_test_audio(case["audio_length"])
            
            # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            latencies = []
            for _ in range(5):
                start_time = time.time()
                result = await optimize_voice_request(test_audio, f"test_user_{i}")
                latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                latencies.append(latency)
            
            avg_latency = sum(latencies) / len(latencies)
            sorted_latencies = sorted(latencies)
            p95_index = int(len(sorted_latencies) * 0.95)
            p95_latency = sorted_latencies[p95_index]
            
            # éªŒè¯æ€§èƒ½
            passed = avg_latency <= case["expected_latency"]
            improvement = (self.baseline_voice_latency - avg_latency) / self.baseline_voice_latency * 100
            
            result_data = {
                "audio_length": case["audio_length"],
                "expected_latency": case["expected_latency"],
                "average_latency": avg_latency,
                "p95_latency": p95_latency,
                "improvement_percent": improvement,
                "passed": passed
            }
            
            results.append(result_data)
            
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
            print(f"  æ€§èƒ½æå‡: {improvement:.1f}%")
            print(f"  æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}")
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½
        latency_values = [r["average_latency"] for r in results]
        overall_avg_latency = sum(latency_values) / len(latency_values)
        overall_improvement = (self.baseline_voice_latency - overall_avg_latency) / self.baseline_voice_latency * 100
        overall_passed = overall_avg_latency <= self.target_voice_latency
        
        summary = {
            "test_name": "è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•",
            "results": results,
            "overall_average_latency": overall_avg_latency,
            "overall_improvement": overall_improvement,
            "target_achieved": overall_passed,
            "baseline_latency": self.baseline_voice_latency,
            "target_latency": self.target_voice_latency
        }
        
        print(f"\næ€»ä½“ç»“æœ:")
        print(f"  å¹³å‡å»¶è¿Ÿ: {overall_avg_latency:.2f}ms")
        print(f"  æ€§èƒ½æå‡: {overall_improvement:.1f}%")
        print(f"  ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if overall_passed else 'âŒ å¦'}")
        
        return summary
    
    async def test_emotion_recognition_accuracy(self) -> Dict[str, Any]:
        """æµ‹è¯•æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡"""
        print("\n=== æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡æµ‹è¯• ===")
        
        # æµ‹è¯•æ•°æ®é›†
        test_dataset = [
            {"text": "æˆ‘ä»Šå¤©éå¸¸å¼€å¿ƒï¼Œå·¥ä½œè¿›å±•å¾ˆé¡ºåˆ©ï¼", "expected": "happy"},
            {"text": "æˆ‘å¾ˆæ‹…å¿ƒè¿™ä¸ªé¡¹ç›®èƒ½ä¸èƒ½æŒ‰æ—¶å®Œæˆ", "expected": "sad"},
            {"text": "è¿™ä¸ªç»“æœè®©æˆ‘å¾ˆå¤±æœ›", "expected": "sad"},
            {"text": "å¤ªæ£’äº†ï¼è¿™æ­£æ˜¯æˆ‘æƒ³è¦çš„", "expected": "happy"},
            {"text": "æˆ‘å¯¹æ­¤æ„Ÿåˆ°å¾ˆæ„¤æ€’", "expected": "angry"},
            {"text": "å¥½çš„ï¼Œæˆ‘çŸ¥é“äº†", "expected": "neutral"},
            {"text": "æˆ‘å¯¹è¿™ä¸ªæ–°åŠŸèƒ½æ„Ÿåˆ°å¾ˆå…´å¥‹", "expected": "excited"},
            {"text": "è®©æˆ‘å†·é™åœ°æ€è€ƒä¸€ä¸‹", "expected": "calm"},
            {"text": "è¿™è®©æˆ‘æ„Ÿåˆ°å¾ˆæ²®ä¸§", "expected": "sad"},
            {"text": "æˆ‘å¾ˆæ»¡æ„è¿™ä¸ªç»“æœ", "expected": "happy"},
        ]
        
        correct_predictions = 0
        total_predictions = len(test_dataset)
        processing_times = []
        confidence_scores = []
        
        print(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {total_predictions}")
        
        for i, sample in enumerate(test_dataset):
            # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
            test_audio = self.generate_test_audio(2.0)  # 2ç§’éŸ³é¢‘
            
            start_time = time.time()
            result = await analyze_emotion(
                audio=test_audio,
                text=sample["text"],
                user_id=f"test_user_{i}"
            )
            processing_time = (time.time() - start_time) * 1000
            
            processing_times.append(processing_time)
            confidence_scores.append(result.confidence)
            
            # æ£€æŸ¥é¢„æµ‹å‡†ç¡®æ€§
            predicted_emotion = result.primary_emotion
            expected_emotion = sample["expected"]
            
            is_correct = predicted_emotion == expected_emotion
            if is_correct:
                correct_predictions += 1
            
            print(f"  æ ·æœ¬ {i+1}: '{sample['text'][:30]}...'")
            print(f"    é¢„æœŸ: {expected_emotion}, é¢„æµ‹: {predicted_emotion}")
            print(f"    ç½®ä¿¡åº¦: {result.confidence:.2f}, å¤„ç†æ—¶é—´: {processing_time:.2f}ms")
            print(f"    ç»“æœ: {'âœ…' if is_correct else 'âŒ'}")
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = correct_predictions / total_predictions
        avg_processing_time = sum(processing_times) / len(processing_times)
        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        
        # æ€§èƒ½è¯„ä¼°
        accuracy_improvement = (accuracy - self.baseline_emotion_accuracy) / self.baseline_emotion_accuracy * 100
        target_achieved = accuracy >= self.target_emotion_accuracy
        
        summary = {
            "test_name": "æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡æµ‹è¯•",
            "total_samples": total_predictions,
            "correct_predictions": correct_predictions,
            "accuracy": accuracy,
            "accuracy_improvement": accuracy_improvement,
            "average_processing_time": avg_processing_time,
            "average_confidence": avg_confidence,
            "target_achieved": target_achieved,
            "baseline_accuracy": self.baseline_emotion_accuracy,
            "target_accuracy": self.target_emotion_accuracy
        }
        
        print(f"\næ€»ä½“ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
        print(f"  æ€§èƒ½æå‡: {accuracy_improvement:.1f}%")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f}ms")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
        print(f"  ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_achieved else 'âŒ å¦'}")
        
        return summary
    
    async def test_batch_processing_throughput(self) -> Dict[str, Any]:
        """æµ‹è¯•æ‰¹å¤„ç†ååé‡"""
        print("\n=== æ‰¹å¤„ç†ååé‡æµ‹è¯• ===")
        
        # å¯åŠ¨æ‰¹å¤„ç†è°ƒåº¦å™¨
        await self.batch_scheduler.start()
        
        try:
            # æµ‹è¯•ä¸åŒæ‰¹å¤§å°çš„æ€§èƒ½
            batch_sizes = [10, 25, 50, 100, 200]
            results = []
            
            for batch_size in batch_sizes:
                print(f"\næµ‹è¯•æ‰¹å¤§å°: {batch_size}")
                
                # ç”Ÿæˆæµ‹è¯•è¯·æ±‚
                requests = []
                for i in range(batch_size):
                    request_data = self.create_test_request()
                    requests.append(request_data)
                
                # æäº¤è¯·æ±‚å¹¶æµ‹é‡æ—¶é—´
                start_time = time.time()
                
                tasks = []
                for req_data in requests:
                    task = submit_batch_request(
                        request_type=req_data["type"],
                        data=req_data["data"],
                        user_id=req_data["user_id"],
                        priority=req_data["priority"]
                    )
                    tasks.append(task)
                
                # ç­‰å¾…æ‰€æœ‰è¯·æ±‚æäº¤
                request_ids = await asyncio.gather(*tasks)
                
                # ç­‰å¾…å¤„ç†å®Œæˆ
                await asyncio.sleep(2.0)  # ç»™è¶³å¤Ÿæ—¶é—´å¤„ç†
                
                end_time = time.time()
                total_time = end_time - start_time
                throughput = batch_size / total_time
                
                # è·å–è°ƒåº¦å™¨ç»Ÿè®¡
                stats = self.batch_scheduler.get_statistics()
                
                result_data = {
                    "batch_size": batch_size,
                    "total_time": total_time,
                    "throughput": throughput,
                    "average_batch_size": stats.get("average_batch_size", 0),
                    "average_wait_time": stats.get("average_wait_time_ms", 0),
                    "queue_size": stats.get("queue_size", 0)
                }
                
                results.append(result_data)
                
                print(f"  å¤„ç†æ—¶é—´: {total_time:.2f}s")
                print(f"  ååé‡: {throughput:.2f} req/s")
                print(f"  å¹³å‡æ‰¹å¤§å°: {result_data['average_batch_size']:.1f}")
                print(f"  å¹³å‡ç­‰å¾…æ—¶é—´: {result_data['average_wait_time']:.2f}ms")
            
            # è®¡ç®—æ€§èƒ½æå‡
            max_throughput = max(r["throughput"] for r in results)
            throughput_improvement = (max_throughput - self.baseline_batch_throughput) / self.baseline_batch_throughput
            target_achieved = throughput_improvement >= (self.target_batch_improvement - 1)
            
            summary = {
                "test_name": "æ‰¹å¤„ç†ååé‡æµ‹è¯•",
                "results": results,
                "max_throughput": max_throughput,
                "throughput_improvement": throughput_improvement * 100,
                "target_achieved": target_achieved,
                "baseline_throughput": self.baseline_batch_throughput,
                "target_improvement": self.target_batch_improvement * 100
            }
            
            print(f"\næ€»ä½“ç»“æœ:")
            print(f"  æœ€å¤§ååé‡: {max_throughput:.2f} req/s")
            print(f"  æ€§èƒ½æå‡: {throughput_improvement*100:.1f}%")
            print(f"  ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_achieved else 'âŒ å¦'}")
            
            return summary
            
        finally:
            # åœæ­¢è°ƒåº¦å™¨
            await self.batch_scheduler.stop()
    
    async def test_system_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§"""
        print("\n=== ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯• ===")
        
        # å¯åŠ¨æ‰¹å¤„ç†è°ƒåº¦å™¨
        await self.batch_scheduler.start()
        
        try:
            # é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
            test_duration = 30  # 30ç§’
            request_interval = 0.1  # 100msé—´éš”
            
            start_time = time.time()
            request_count = 0
            error_count = 0
            latencies = []
            
            print(f"è¿è¡Œç¨³å®šæ€§æµ‹è¯• {test_duration} ç§’...")
            
            while time.time() - start_time < test_duration:
                try:
                    # éšæœºé€‰æ‹©æµ‹è¯•ç±»å‹
                    test_type = random.choice(["voice", "emotion", "batch"])
                    
                    if test_type == "voice":
                        # è¯­éŸ³å¤„ç†æµ‹è¯•
                        test_audio = self.generate_test_audio(1.0)
                        req_start = time.time()
                        await optimize_voice_request(test_audio, f"stability_user_{request_count}")
                        latency = (time.time() - req_start) * 1000
                        latencies.append(latency)
                        
                    elif test_type == "emotion":
                        # æƒ…æ„Ÿè¯†åˆ«æµ‹è¯•
                        test_audio = self.generate_test_audio(1.0)
                        req_start = time.time()
                        await analyze_emotion(
                            audio=test_audio,
                            text="ç¨³å®šæ€§æµ‹è¯•æ–‡æœ¬",
                            user_id=f"stability_user_{request_count}"
                        )
                        latency = (time.time() - req_start) * 1000
                        latencies.append(latency)
                        
                    else:
                        # æ‰¹å¤„ç†æµ‹è¯•
                        req_start = time.time()
                        await submit_batch_request(
                            request_type=RequestType.TEXT_GENERATION,
                            data=f"stability_test_{request_count}",
                            user_id=f"stability_user_{request_count}",
                            priority=RequestPriority.NORMAL
                        )
                        latency = (time.time() - req_start) * 1000
                        latencies.append(latency)
                    
                    request_count += 1
                    
                except Exception as e:
                    error_count += 1
                    print(f"  é”™è¯¯ {error_count}: {str(e)[:50]}...")
                
                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                await asyncio.sleep(request_interval)
            
            # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
            total_time = time.time() - start_time
            success_rate = (request_count - error_count) / request_count if request_count > 0 else 0
            avg_latency = sum(latencies) / len(latencies) if latencies else 0
            if latencies:
                sorted_latencies = sorted(latencies)
                p95_index = int(len(sorted_latencies) * 0.95)
                p95_latency = sorted_latencies[p95_index]
            else:
                p95_latency = 0
            throughput = request_count / total_time
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = self.batch_scheduler.get_statistics()
            
            summary = {
                "test_name": "ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•",
                "test_duration": total_time,
                "total_requests": request_count,
                "error_count": error_count,
                "success_rate": success_rate,
                "average_latency": avg_latency,
                "p95_latency": p95_latency,
                "throughput": throughput,
                "scheduler_stats": final_stats
            }
            
            print(f"\nç¨³å®šæ€§æµ‹è¯•ç»“æœ:")
            print(f"  æµ‹è¯•æ—¶é•¿: {total_time:.1f}s")
            print(f"  æ€»è¯·æ±‚æ•°: {request_count}")
            print(f"  é”™è¯¯æ•°é‡: {error_count}")
            print(f"  æˆåŠŸç‡: {success_rate:.2%}")
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
            print(f"  ååé‡: {throughput:.2f} req/s")
            
            return summary
            
        finally:
            await self.batch_scheduler.stop()
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ VoiceHelper v1.20.0 æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        all_results = {}
        
        try:
            # 1. è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–æµ‹è¯•
            voice_results = await self.test_voice_latency_optimization()
            all_results["voice_optimization"] = voice_results
            
            # 2. æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡æµ‹è¯•
            emotion_results = await self.test_emotion_recognition_accuracy()
            all_results["emotion_recognition"] = emotion_results
            
            # 3. æ‰¹å¤„ç†ååé‡æµ‹è¯•
            batch_results = await self.test_batch_processing_throughput()
            all_results["batch_processing"] = batch_results
            
            # 4. ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•
            stability_results = await self.test_system_stability()
            all_results["system_stability"] = stability_results
            
            # ç”Ÿæˆæ€»ä½“è¯„ä¼°
            overall_assessment = self._generate_overall_assessment(all_results)
            all_results["overall_assessment"] = overall_assessment
            
            print("\n" + "=" * 60)
            print("ğŸ“Š v1.20.0 æ€§èƒ½æµ‹è¯•æ€»ç»“")
            print("=" * 60)
            
            for test_name, result in overall_assessment.items():
                if test_name != "overall_score":
                    status = "âœ… é€šè¿‡" if result["passed"] else "âŒ å¤±è´¥"
                    print(f"{result['name']}: {status}")
                    print(f"  ç›®æ ‡: {result['target']}")
                    print(f"  å®é™…: {result['actual']}")
                    print(f"  æå‡: {result['improvement']}")
            
            overall_score = overall_assessment["overall_score"]
            print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {overall_score:.1f}/100")
            
            if overall_score >= 80:
                print("ğŸ‰ v1.20.0 æ€§èƒ½æµ‹è¯•å…¨é¢é€šè¿‡ï¼")
            elif overall_score >= 60:
                print("âš ï¸  v1.20.0 æ€§èƒ½åŸºæœ¬è¾¾æ ‡ï¼Œéƒ¨åˆ†æŒ‡æ ‡éœ€è¦ä¼˜åŒ–")
            else:
                print("âŒ v1.20.0 æ€§èƒ½æµ‹è¯•æœªè¾¾æ ‡ï¼Œéœ€è¦é‡å¤§ä¼˜åŒ–")
            
            return all_results
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return {"error": str(e)}
    
    def _generate_overall_assessment(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        assessment = {}
        
        # è¯­éŸ³ä¼˜åŒ–è¯„ä¼°
        voice_result = results.get("voice_optimization", {})
        voice_passed = voice_result.get("target_achieved", False)
        voice_improvement = voice_result.get("overall_improvement", 0)
        
        assessment["voice_optimization"] = {
            "name": "è¯­éŸ³å»¶è¿Ÿä¼˜åŒ–",
            "passed": voice_passed,
            "target": f"< {self.target_voice_latency}ms",
            "actual": f"{voice_result.get('overall_average_latency', 0):.1f}ms",
            "improvement": f"{voice_improvement:.1f}%",
            "score": 25 if voice_passed else 10
        }
        
        # æƒ…æ„Ÿè¯†åˆ«è¯„ä¼°
        emotion_result = results.get("emotion_recognition", {})
        emotion_passed = emotion_result.get("target_achieved", False)
        emotion_accuracy = emotion_result.get("accuracy", 0)
        
        assessment["emotion_recognition"] = {
            "name": "æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡",
            "passed": emotion_passed,
            "target": f"> {self.target_emotion_accuracy:.0%}",
            "actual": f"{emotion_accuracy:.1%}",
            "improvement": f"{emotion_result.get('accuracy_improvement', 0):.1f}%",
            "score": 25 if emotion_passed else 10
        }
        
        # æ‰¹å¤„ç†è¯„ä¼°
        batch_result = results.get("batch_processing", {})
        batch_passed = batch_result.get("target_achieved", False)
        batch_improvement = batch_result.get("throughput_improvement", 0)
        
        assessment["batch_processing"] = {
            "name": "æ‰¹å¤„ç†ååé‡",
            "passed": batch_passed,
            "target": f"> {self.target_batch_improvement*100:.0f}% æå‡",
            "actual": f"{batch_improvement:.1f}% æå‡",
            "improvement": f"{batch_improvement:.1f}%",
            "score": 25 if batch_passed else 10
        }
        
        # ç¨³å®šæ€§è¯„ä¼°
        stability_result = results.get("system_stability", {})
        stability_success_rate = stability_result.get("success_rate", 0)
        stability_passed = stability_success_rate >= 0.99  # 99%æˆåŠŸç‡
        
        assessment["system_stability"] = {
            "name": "ç³»ç»Ÿç¨³å®šæ€§",
            "passed": stability_passed,
            "target": "> 99% æˆåŠŸç‡",
            "actual": f"{stability_success_rate:.1%}",
            "improvement": f"{(stability_success_rate - 0.95) * 100:.1f}%",
            "score": 25 if stability_passed else 10
        }
        
        # è®¡ç®—æ€»åˆ†
        total_score = sum(item["score"] for item in assessment.values() if isinstance(item, dict) and "score" in item)
        assessment["overall_score"] = total_score
        
        return assessment

# æµ‹è¯•è¿è¡Œå™¨
async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    test_suite = V120PerformanceTest()
    results = await test_suite.run_all_tests()
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    import json
    with open("v1_20_0_performance_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: v1_20_0_performance_results.json")
    
    return results

if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    asyncio.run(main())
